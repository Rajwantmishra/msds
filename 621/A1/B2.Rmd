---
title: "multicolinearity"
author: "Rajwant Mishra"
date: "February 20, 2020"
output: html_document
---

As a datascientis most of the time we have to work on som unknown problems or data. I am cricket fan and what if someone gave me data of baseball to analuyise.It is very imporant that My inability to understand differnt terms in the data due to my lack of knowledge about the sport shouldn't impact the mdoel or nalysis.
How can we make sure that we can avoid some of the common mistakes while busilding or planning for our model. Few things that makes a model a parsomonious. 

>A parsimonious model is a model that accomplishes a desired level of explanation or prediction with as few predictor variables as possible.They explain data with a minimum number of parameters, or predictor variables.

>Multicolinearity : Multicollinearity (also collinearity) is a statistical phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with a non-trivial degree of accuracy.

We know the four asumption of Linear Regression lm(X~y) :

Linearity: The relationship between X and the mean of Y is linear.
Homoscedasticity: The variance of residual is the same for any value of X.
Independence: Observations are independent of each other.
Normality: For any fixed value of X, Y is normally distributed.


Multicollinearity is a problem because it undermines the statistical significance of an independent variable as  those varaibles are correlated, higher degree of correlation between variables can't gurntee that our model can better explain all the varaitions of the data. 


> We will user alias function to detect the collinearity of all the predictor in the model1. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(plyr)
library(bbmle)
library(car)
library(psych)
library(tibble)

```

Here I have data about Kids score depeding upon  moms age and if Mom were working and Mom_iq. Let try to fit the model based on this data.

Response : - Kid_score
Predicotr :- mom_hs , mom_iq , mom_work, mom_age

```{r}

cognitive <- read.csv("http://bit.ly/dasi_cognitive")
head(cognitive)
```
```{r}
cog_full = lm(kid_score~.,data = cognitive)
summary(cog_full)
```

```{r}
cognitive$c_ageiq <- cognitive$mom_age*cognitive$mom_iq
cognitive$c_iq  <- cognitive$mom_iq^2 
head((cognitive))

```

```{r}
cog_full2 = lm(kid_score~.,data = cognitive)
summary(cog_full2)
```
As we can see the two model1(cog_full) and Model2 (cog_full2), 

| Model Name | R-squared| Adjusted R-squared    | Significant | standard error |
|------------|---------|---------|------------------------|----------|
|Model1      |  0.2171 | 0.2098  | mom_hsyes, mom_iq | 18.14 |
|Model2 (c)  |   0.2371| 0.2264  | mom_iq,c_iq| 17.95 |

From the above two model we see the coefficients of mom_iq is showing very high std error and its p-value has also gone down. Why we see this problem, its becasue of exisitng colrealted predictor  `c_ageiq` and `c_iq`.

We can use two command to test theses :

 a.  Using the Correlation among the predictors.  
 b.  VIF (variance inflation factor), which measures how much the variance of a regression coefficient is inflated due to multicollinearity in        the model. The smallest possible value of VIF is one (absence of multicollinearity). Here we will look for VIF value, if that exceeds 5 or       10 indicates a problematic amount of collinearity. 
 "Read More"['http://www.sthda.com/english/articles/39-regression-model-diagnostics/160-multicollinearity-essentials-and-vif-in-r/']


```{r}
library(corrplot)
corrplot(cor(cognitive[,-c(2,4)],use="pairwise.complete.obs"),type = 'upper')

```


```{r}
cor(cognitive[,-c(2,4)])[,c(2,5)]
```
Above plots and correlation matrix, very clearly shows that `Mom_iq` ia colinear with `c_iq` and `c_ageiq`

Now we will see Variance Inflation Factors for each predictors in the model.

```{r}
library(car)
vif(cog_full2)
```

Here again the value from Correlation matrix and Variance Inflation Factors, is giving same result with some correlation that exist between predictors : `mom_iq` , `mom_iq` , `c_ageiq` , `c_iq`

In contrast if we check the inflation factor for the `Model1` we see that none of the predictors are correlated. Which is good sign for further improving this model  .


```{r}
vif(cog_full)
```
 High Variance Inflation Factor (VIF) and Low Tolerance : These two useful statistics are reciprocals of each other. So either a high VIF or a low tolerance is indicative of multicollinearity. VIF is a direct measure of how much the variance of the coefficient (ie. its standard error) is being inflated due to multicollinearity.
 
 

# CALCULATING VIF by hand

```{r}

cog_2 = lm(kid_score~mom_iq+mom_hs,data = cognitive)
summary(cog_2)
vif(cog_2)
```

```{r}
par(mfrow = c(2,2))
plot(cog_full)
vif(cog_full); vif(cog_2)

pt( 2.201,df=429,lower.tail = FALSE)*2
# Favors that mom_hys is significant since t value is less than .05.

```

```{r}
df = 429
# Find 95% confidence interval of slope that mom works

# Sine its a two bound test , we will .025 on both side ,
# lets get crtical t score by qt(.025, ,429)

crT<- qt(.025 ,429)
CI_mom_workyes_1 <- 2.54 + 1.965509*2.35 # Point Estimate of the var +- (crtical T value * Std Error)
CI_mom_workyes_2 <- 2.54 - 1.965509*2.35 # Point Estimate of the var +- (crtical T value * Std Error)

CI_mom_workyes_1; CI_mom_workyes_2
confint(cog_full,level = 0.95)
confint(cog_2,level = 0.95)

```

```{r}
vif(cog_full)
alias(cog_full)
AICctab(cog_full,cog_2,weights = TRUE)
```

