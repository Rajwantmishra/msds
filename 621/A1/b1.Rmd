---
title: "B1"
author: "Rajwant Mishra"
date: "February 13, 2020"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Idenifying Residual Plot and Regression Model

As a datascientist we always have to work on data which may or may not be alwyays in support of the problem we are trying to solve. In regression anlysis while working on multiple models we have to focus on many datapoint and their imacts on the predictions. 
  One of such datapoint is Residual plots and how we can read it and correct or improve our regression model by visually undersatdning the Residual plots.

I am going to user Cars dataset from R, which has follwoing two fields:

+ speed	 numeric	 Speed (mph)
+ dist	 numeric	 Stopping distance (ft)

Summary of the data can be seen as below :

```{r cars}
summary(cars)
```

### Regression 
I am going to use speed of the car in mph to identify the stopping distance covered in ft. 

+ Predictor: Speed
+ response : Stopping Distance 
+ Model1 : Distance = intercept + slope * speed

Below Residuals vs Fitted plots shows how the residuals are seen close to the regression line . Points below the regression line have positive error and points above the regression line have negative error. 
Postive errors indicates points lies below the regression line and have predicted value greater than the actual vlaue. 
Negative Error indicates points lies above the regression line and have predicted value less than the actaul value.

```{r pressure, echo=FALSE}
model1 <- lm(dist~speed,cars)
# par(mfrow = c(2, 2)) 
summary(model1)
plot(model1)

```

Rsidual plots if it doens't show any pattern by looking at the data points (as shown in above plot) we can say that this model may be able to explain some of the varaiats of the data. This doen't suggest that its a best model but it suggest that its not a bad model. Since there is no visible pattern of the data that can be seen can say that model is on the right track.  


```{r}
coef(model1)
```
# Shows at the speed of 0  your Stopping distance -17.57 ft  . 




```{r}
model2 <- lm(dist~I(speed-mean(speed)),cars)
coef(model2)
```

```{r}
mean(cars$speed) 
```

# Shows at the speed of 15.4mph your Stopping distance  42.98  ft . 

Lets see how much it for a 10th of a mile speeds


```{r}
model3 <- lm(dist~I(speed*10),cars)
coef(model3)
```

```{r}
sp <- c(4,7,8)
coef(model1)[1] + coef(model1)[2] * sp
coef(model2)[1] + coef(model2)[2] * sp
coef(model3)[1] + coef(model3)[2] * sp
predict(model1,newdata = data.frame(speed=sp))
summary(model1)
summary(model2)
summary(model3)
```

```{r}
par(mfrow = c(2, 2)) 
plot(model2)
```

```{r}
par(mfrow = c(2, 2))  
plot(model1)
```

```{r}
library(ggplot2)
 g = ggplot(cars,aes(x = speed, y= dist)) 
 g = g + xlab("Speed")
 g = g + ylab("Distance")
 # g = g + geom_point(size = 7, color = "black" , alpha = .6)
 g = g + geom_point(size = 3, color = "blue" , alpha = .3)
 g = g + geom_smooth(method = "lm", color = "gray")
 g
```

properties of REsidual E[e] = 0
```{r}
e <- resid(model1) 

# Residual = Observed value - Predicted value

yhat <- predict(model1) # predicted from Model

y <- cars$dist # Observed value 

calculated_Res <- y - yhat

# check if Calcaulted residual is equial to computed Residual 
# print(paste("Computed Residual-->:",round(e,2),"=",round(calculated_Ress,3),"<--Calculated Residual"))
dt_ress <- data.frame("Computed Residual"=e, "Calculated Residual"=calculated_Res)

dt_ress


```

```{r}
# Sum of residual must be zero 

 sum(dt_ress$Calculated.Residual)

```
10^-12 is close to 0.

```{r}
# Sum of Residual multiplyied by Regression Variable  shoud be zero
sum(dt_ress$Calculated.Residual * cars$speed)
```
10^-11 is close to 0.

```{r}
ggplot(cars, aes(x = speed, y = dist)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = speed, yend = yhat), alpha = .2) +

  # > Color adjustments made here...
  geom_point(aes(color = abs(e))) + # Color mapped to abs(residuals)
  scale_color_continuous(low = "black", high = "red") +  # Colors to use here
  guides(color = FALSE) +  # Color legend removed
  # <

  geom_point(aes(y = yhat), shape = 1) +
  theme_bw()

```
```{r}
# same Plot but moving it in center so that we can clearly see residuals
ggplot(cars, aes(x = speed, y = e)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = speed, yend = 0), color ="red", alpha = .2) + 
  ylab("Residual Disatance in Ft") +

  # > Color adjustments made here...
  geom_point(aes(color = abs(e))) + # Color mapped to abs(residuals)
  scale_color_continuous(low = "black", high = "red") +  # Colors to use here
  guides(color = FALSE) +  # Color legend removed
  # <

  # geom_point(aes(y = yhat), shape = 1) +
  theme_bw()
```

## Moving steps ahead 
Now we will genrate some data points that are randomaly gernated and noramlly distrbbuted. 

X <- Genrate 100 data points between 3 and -3 
y <-  x + sin(x) 
y1 <- x + sin(x) + rnorm(100,sd=.2)
y2 <-  x + rnorm(100,sd=.2)

My objective is to see how residual plots would come when we try to build llinear regression model lm(Y~X). Let's dive into it and see the power of residual plots visaullay. 

 You can see how y,y1,y2 are distributed , by histogram and scatterplot. 
 
```{r}
# Trying to see Residual with different data set that is not linear.

x <- runif(100,-3,3) # Genrate 100 data points between 3 and -3 

y <-  x + sin(x) 
y1 <- x + sin(x) + rnorm(100,sd=.2)
y2 <-  x + rnorm(100,sd=.2)
dt_chk <- data.frame('x'= x,'y'=y ,'y1'=y1, 'y2'=y2)
head(dt_chk )

par(mfrow = c(3, 3)) 
hist(y)
hist(y1)
hist(y2)
plot(y)
plot(y1)
plot(y2)


```

Now if we plot regression line and actual data points , and try to understand how good the model is, most of the time we may feel our model is good since datapoints are close to regression line. In  1st plot , it looks very clearly that our data is making a sine waves along with regression line, whereas in 2nd plot it looks very close that our model might be  good choice for this data. Third plot is also showing the same information. 

```{r}

 g = ggplot(dt_chk,aes(x = x, y= y)) 
 g = g + xlab("Speed") + ggtitle(" Value for Y ")
 g = g + ylab("Distance")
 # g = g + geom_point(size = 7, color = "black" , alpha = .6)
 g = g + geom_point( aes(x = x, y= y),size = 3, color = "blue" , alpha = .3)
 # g = g + geom_point( aes(x = x, y= y1),size = 2, color = "green" , alpha = .3)
 # g = g + geom_point( aes(x = x, y= y2),size = 1, color = "red" , alpha = .3)
 # g = g + geom_point(size = 3, color = "red" , alpha = .3))
 g = g + geom_smooth(method = "lm", color = "gray")
 g
 
 
 g = ggplot(dt_chk,aes(x = x, y= y)) 
 g = g + xlab("Speed") + ggtitle(" Value for Y1 ")
 g = g + ylab("Distance")
 # g = g + geom_point(size = 7, color = "black" , alpha = .6)
 # g = g + geom_point( aes(x = x, y= y),size = 3, color = "blue" , alpha = .3)
 g = g + geom_point( aes(x = x, y= y1),size = 2, color = "green" , alpha = .3)
 # g = g + geom_point( aes(x = x, y= y2),size = 1, color = "red" , alpha = .3)
 # g = g + geom_point(size = 3, color = "red" , alpha = .3))
 g = g + geom_smooth(method = "lm", color = "gray")
 g
 
 g = ggplot(dt_chk,aes(x = x, y= y)) 
 g = g + xlab("Speed") + ggtitle(" Value for Y2 ")
 g = g + ylab("Distance")
 # g = g + geom_point(size = 7, color = "black" , alpha = .6)
 # g = g + geom_point( aes(x = x, y= y),size = 3, color = "blue" , alpha = .3)
 # g = g + geom_point( aes(x = x, y= y1),size = 2, color = "green" , alpha = .3)
 g = g + geom_point( aes(x = x, y= y2),size = 1, color = "red" , alpha = .3)
 # g = g + geom_point(size = 3, color = "red" , alpha = .3))
 g = g + geom_smooth(method = "lm", color = "gray")
 g
```


### How do we validate it now ?
Lets plot residual plot and see how does that looks for each y,y1,y2.

Figure 1: In contrast to last plot here we see that our residuals plots give very clear pattern which seems to follow a clear sine wave path in the residaul plot. Except third plot first two plots seems to follow sine wave and hence it's clear that we are missing some thing the regression model for these equations. We can try explaning these pattern in the model to improve our regression model.

```{r}

# lets build the model
model3 <- lm(y~x,dt_chk)
model4 <- lm(y1~x,dt_chk)
model5 <- lm(y2~x,dt_chk)
par(mfrow = c(2, 2)) 
plot(model3)
plot(model4)
plot(model5)

```

This resach suggest how residual plots are imporant and how they can give us hidden inforaiton about the data and help us bnuild the model that better explain the response varaible .


```{r}
# Sum of residual must be zero 

 sum(resid(model3))
# Sum of Residual multiplyied by Regression Variable  shoud be zero
sum(resid(model3) * x)

# Sum of residual must be zero 

 sum(resid(model5))
# Sum of Residual multiplyied by Regression Variable  shoud be zero
sum(resid(model5) * x)
```

