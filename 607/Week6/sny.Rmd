---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(rjson)
library(XML)
library(xml2)
```

```{r}


# Get  bad word JSON File from GITHUB to do you string compare 
#install.packages("rjson")
badwordURL <- "https://raw.githubusercontent.com/web-mech/badwords/master/lib/lang.json"
#library(rjson)
suppressWarnings(rm(dontSay))

raw <- read_file(badwordURL)
dontSay <- fromJSON(raw)
class(dontSay)
dontSay <- as.data.frame(dontSay$words)
colnames(dontSay) <- c("words")
dontSay <- as.character(dontSay$words)

```

```{r}
# Parsing of HTML/XML files  
library(rvest)    

# String manipulation
library(stringr)   

# Verbose regular expressions
#install.packages("rebus")
library(rebus)     

# Eases DateTime manipulation
library(lubridate)

library(DT)


```


```{r}
 url <-c('https://www.imdb.com/title/tt3203528/reviews?ref_=tt_urv',
          'https://www.imdb.com/title/tt4154756/reviews?ref_=tt_urv',
          'https://www.imdb.com/title/tt2231461/reviews?ref_=tt_urv',
          'https://www.imdb.com/title/tt6772950/reviews?ref_=tt_urv',
         'https://www.imdb.com/title/tt3606756/reviews?ref_=tt_urv',
          'https://www.imdb.com/title/tt1517451/reviews?ref_=tt_urv')
 



suppressWarnings(rm(getReview))
readAllAttribute(first_page)


   
   
   
   url <- "https://www.booksamillion.com/search?id=7523317361322&query=&filter=product_type%3Abooks"
   first_page <- read_html(url)
   
   # Title
   Title <- html_nodes(first_page,'.title') %>%   html_text()  %>%  str_trim() %>%    unlist() 
   # Author
   Author <- html_nodes(first_page,'.list') %>%   html_nodes('.byline') %>% html_text()  %>%  str_trim() %>%    unlist() 
   ## ISBON 
   a[1]
   all <- trimws(unlist(str_split(a,"ISBN")))
   
   isbn <- unlist(str_split(a,"ISBN",n=2))
   
   library(tidyverse)
   finaldf <- data.frame(title = Title,Auth = Author)
   
   
   finaldf <- separate(finaldf,Auth,c("Author","ISBN"),sep = "ISBN")
    
   finaldf <- separate(finaldf,ISBN,c("ISBN","REL_Date"),sep = "\\/")
   finaldf <- separate(finaldf,Author,c("Author1","Author2"),sep = ",")
   finaldf$Author1 <- trimws(str_sub(finaldf$Author1,3,-1L))
    finaldf$Author2 <- trimws(finaldf$Author2)
    
    write.csv(finaldf,"finaldf.csv")
    
   
   
```



```{r}
   
     url2 <- "https://whatsq.com/gst/index.php?name=rautsan&key=Yes"

#tdata <- read_html(url2,options = "HUGE")

# write_html(tdata,"ra.html")



html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist() 

td<- html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist() 
td<- data_frame(data= td)

# Tweeter id:
# tid  <- str_extract(td[1],"@[[a-zA-Z]]+")
 td$DateTime<- str_extract(td$data,"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
   # td$Tweets <- str_split(td$data,"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
 
 
 
 td<- separate(td,data,c("TName","Tweet"),"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}",remove = FALSE)
 
 td$Month <- trimws(str_extract(td$DateTime," [A-Z]{1}[a-z]{1}[a-z]{1}"))
 td$weekDay <- str_extract(td$DateTime,"[A-Z]{1}[a-z]{1}[a-z]{1}")
 td$year <- str_extract(td$DateTime,"[0-9]{4}$")
 td$day <- str_extract(td$DateTime,"(?<= [A-Z]{1}[a-z]{1}[a-z]{1}) [0-9]{2}")
 #td$hash <- str_extract_all(td$Tweet,"#[0-9a-zA-X]+")
 # paste(unlist(str_extract_all(td$Tweet,"#[0-9a-zA-X]+")), collapse = ",")
View(tmp)
# td$hash <- mutate(td, hash =  str_extract_all(td$Tweet,"#[0-9a-zA-X]+") )
 write_csv(td,file.path(getwd(),"tweet.csv"))
write.csv(td,file = "tweet.csv")
 
 # View(mutate(td, hash = paste(str_extract_all(td$Tweet,"#[0-9a-zA-X]+",simplify = TRUE),collapse = " ") ))
#  str_extract_all(td$Tweet[1],"(?<=#)[0-9a-zA-X]+")

# str_split(td$DateTime[1],"[A-Z]{1}[a-z]{1}[a-z]{1}") 
#  [[1]]
# [1] "@rautsan"                " "                       " 12 03:57:09 +0000 2019"
#  
 # td<- separate(td,DateTime,c("TID","DateTime")," ",remove = FALSE)
# str_extract(td$DateTime[1]," [A-Z]{1}[a-z]{1}[a-z]{1} [0-9]{2}") 
  # " Mar 12"

  
  

     tghtml_nodes(first_page,'.list') %>%   html_nodes('.our-price') %>% html_text()  %>%  str_trim() %>%    unlist() 
     
       html_nodes(first_page,'.list') %>%   html_nodes('.title') 
       
        html_nodes(first_page,'.list') %>%   html_nodes('.title') %>% html_text()  %>%  str_trim() %>%    unlist() 
  str_trim() %>%    unlist() 
   
   

 # url of the deatil page : 
          html_nodes(first_page,'.list span.title a') %>%   html_attr("href")
 # Image source 
          img <- '.list li img'
           html_nodes(first_page,'.list li img') %>% html_attr("src")
            html_nodes(first_page,img) %>% html_attr("src")
            
            
            
            
# # Rating            
#  rating <-  '.list  .product-actions'        
#  html_nodes(first_page,'.list .product-actions section.pr-category-snippet__total')
#  
#  html_nodes(first_page,'.list  .product-actions ')
#  
# brt <-  html_nodes(first_page,'.list  .product-actions #search_stars .flt-left')  
#  html_nodes(brt,'#pr-categorysnippet-9781547600380') %>%   "["(1) %>% str()
#  html_nodes(brt,'.pr-category-snippet__item')
#  
#  html_nodes(first_page,'.list #pr-categorysnippet-9781547600380 section.pr-category-snippet__total')
#  
#  pr-category-snippet__total pr-category-snippet__item
# 
#   div#pr-categorysnippet-9781547600380 section.pr-category-snippet__total
#   
  
  
  movie <- read_html("http://www.imdb.com/title/tt1490017/")
cast <- html_nodes(movie, "#titleCast")
html_text(cast,".cast_list td")
html_name(cast)
html_attrs(cast)
html_attr(cast, "class")

  
```





# Read from https://whatsq.com/gst/index.php?name=rautsan&key=Yes

```{r}



```

# Read from  https://www.thinkers360.com/top-20-global-thought-leaders-on-analytics-july-2018/
```{r}
urlA <- read_html("https://www.thinkers360.com/top-20-global-thought-leaders-on-analytics-july-2018/")


tbls_ls <- urlA %>%
        html_nodes("table") %>%
        .[1] %>%
        html_table(fill = TRUE)

tbls_ls[[1]]

dataurlA <- as_tibble(tbls_ls[[1]])
# https://www.thinkers360.com/tl/profiles/view/109


# urlA %>%
#         html_nodes("table td a") %>% html_attrs() %>% unlist()
# 
# urlA %>%
#         html_nodes("table td ") %>% xml_children() %>% html_attr("href")

# Profile URL 
profileUrlA  <- urlA %>%
        html_nodes("table tr td:nth-of-type(3) a") %>% html_attr("href")

dataurlA$profileURL <- profileUrlA

# Tweeturl
urlA %>%
        html_nodes("table tr td:nth-of-type(2) a") %>% html_attr("href")

dataurlA

```



```{r}

 dataurlA$profileIMGURL <- NA
 dataurlA$intro <- NA
 dataurlA$AreasofExpertise<- NA
 dataurlA$location <- NA
dataurlA$tFollower <- NA
dataurlA$IndustryExperience <- NA
dataurlA$company <- NA
dataurlA$Publications <- NA
dataurlA$Opportunities <- NA
dataurlA$company <- NA
dataurlA$socialURL <- NA

for (i in 8:20){
#--------------------------------------------------------------------------
#-------------Read Sub Page -----------------------------------------------
#--------------------------------------------------------------------------

  # urlAprofile <- "https://www.thinkers360.com/tl/profiles/view/100"
  urlAprofile <- dataurlA$profileURL[i]
  Sys.sleep(10)
  urlB <- read_html(urlAprofile)



  dataurlA$intro[i] <- html_nodes(urlB,"div.profile_section p") %>% html_text() 
  dataurlA$profileIMGURL[i] <-  html_nodes(urlB,'div.profile_bg img') %>% html_attr("src")
  
  

      profileIMGURL <-      html_nodes(urlB,'div.profile_bg img') %>% html_attr("src")
      intro <- html_nodes(urlB,"div.profile_section p") %>% html_text() 
      areofExper <- "div.psection:nth-of-type(4) div.skill_section"
      publication <- "div#publications.skill_section div.col-sm-12"
      
      blog <- "div#blog.skill_section div.pitem"
blog <- html_nodes(urlB,blog) %>% html_text()
blogT <- html_nodes(urlB,"div#blog.skill_section div.ppanel div div:nth-of-type(1)") %>% html_text() %>% unlist()

blogTag <-  html_nodes(urlB,"div#blog.skill_section p:nth-of-type(2)") %>% html_text() %>% unlist()

blogData <- 
html_nodes(urlB,"div#blog.skill_section div.ppanel div div:nth-of-type(2)") %>% html_text() %>% unlist()


# book <- "div.pitem:nth-of-type(3)"
# bookTitle <- "div#publication2.ppanel div div:nth-of-type(1)"
# bookTag <- "div#publication2.ppanel p:nth-of-type(2)"
# bookdat <- "div#publication2.ppanel div div:nth-of-type(2)"
# 
# book <- html_nodes(urlB,"div#publications.skill_section div.pitem") %>% html_text()
# bookTitle <- html_nodes(urlB,"div#publication3.ppanel div div:nth-of-type(1)") %>% html_text()
# bookTag <- html_nodes(urlB,bookTag) %>% html_text()
# bookdat <- html_nodes(urlB,bookdat) %>% html_text()

Publications <-  html_nodes(urlB,"div#publications.skill_section div.pitem") %>% html_text()

dataurlA$Publications[i] <- paste(Publications, collapse = " | ")

# span.item

AreasofExpertise <- html_nodes(urlB,"div.psection:nth-of-type(4) div.skill_section span.item") %>% html_text() %>% unlist() 
dataurlA$AreasofExpertise[i] <- paste(str_extract(AreasofExpertise,"[[:alpha:]]+ ([[:alpha:]]+)?"),collapse = "|")

# Followers

tFollower <- 
html_nodes(urlB,"div.profile_section div:nth-of-type(2)") %>% html_text()%>%trimws() %>% unlist() %>% str_extract("[0-9]+")

dataurlA$tFollower[i] <-ifelse(length(tFollower)==0,NA,tFollower)

dataurlA$location[i] <- html_nodes(urlB,"div.profile_section h3") %>% html_text()%>%trimws() %>% unlist()

IndustryExperience <- "div.psection:nth-of-type(5) span.item"
 IndustryExperience <- html_nodes(urlB,"div.psection:nth-of-type(5) span.item") %>% html_text()%>%trimws() %>% unlist()
 dataurlA$IndustryExperience[i]  <- paste(IndustryExperience, collapse = " | ")
 
 
 
 
 
  socialURL<- html_nodes(urlB,"div.pinfo_section:nth-of-type(2) a") %>% html_attr("href")
  dataurlA$socialURL[i] <- paste(socialURL,collapse = " | ")
  
  
  # Opportunities
  
   
   dataurlA$Opportunities[i] <- ifelse(length(html_nodes(urlB,"div#opportunities.skill_section div.ppanel div div:nth-of-type(1)") %>% html_text()%>%trimws() %>% unlist())==0 ,NA,
                                       html_nodes(urlB,"div#opportunities.skill_section div.ppanel div div:nth-of-type(1)") %>% html_text()%>%trimws() %>% unlist())
 
 # COmpany 
   
  company <- html_nodes(urlB,"div.user-info") %>% html_text()%>%trimws() %>% unlist()
   
   dataurlA$company[i] <-   ifelse(length(company)==0,NA,company)

}
#--------------------------------------------------------------------------
#-------------Read Sub Page -----------------------------------------------
#--------------------------------------------------------------------------
  dataurlA$tURL <- trimws(str_extract(dataurlA$socialURL," ?(https://twitter.com/)[:alpha:]+"))

dataurlA$thandle <- str_replace(dataurlA$tURL,"https://twitter.com/","")

  dataurlA$linkURL <- str_extract(dataurlA$socialURL," ?(https://www.linkedin.com/)[:alpha:]+/?[:alpha:]+")
   


```

```{r}

dataurlAc <- dataurlA
urlC <- "https://data606.net/course-overview/links/"

 urlC <- read_html(urlC)



  
    html_nodes(urlC,'div.profile_bg img') %>% html_attr("src") 
    
    Name  <-  html_nodes(urlC,"ul:nth-of-type(5) a.highlight, ul:nth-of-type(5) li:nth-of-type(n+3)") %>% html_text()  %>% unlist()
    
    nameDF <- as.data.frame(Name)
     nameDF <- separate(nameDF,Name,c("Name","thandle"),sep="@")
     
    
 dataurlA <-  tibble::add_row(dataurlAc  ,Name = nameDF$Name,`Twitter Handle`=nameDF$thandle)
 
 dataurlA$`Twitter Handle` <- str_replace_all( dataurlA$`Twitter Handle` ,"@","")
 

 #---------------------------!st data backup
    datatable(dataurlA)



write.csv(dataurlA,"dataurlA.csv")

fsp3 <- gridfs(db = "MSProject3", url = "mongodb+srv://msds_user:msds@cluster0-bqyhe.gcp.mongodb.net/", prefix = "fs",options = ssl_options())
fsp3$drop()
fsp3$upload("dataurlA.csv")
 

```


```{r}
    
length(dataurlA$`Twitter Handle`)
rm(dtf)
dtf <- data.frame(data=character(),
                 TName=character(),
                 Tweet=character(),
                 DateTime=character(),
                 Month=character(),
                 weekDay=character(),
                 year=character(),
                 day =character(),
                 hash = character(),
                 img = character()
                 )




 # url2 <- "https://whatsq.com/gst/index.php?name=rautsan&key=Yes"
for ( i in 1: length(dataurlA$`Twitter Handle`)) {
   url2 <- paste0("https://whatsq.com/gst/index.php?name=",dataurlA$`Twitter Handle`[i],"&key=Yes")
   print(url2)
   Sys.sleep(10)
   tdata <- read_html(url2,options = "HUGE")

# write_html(tdata,"ra.html")



html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist() 

td<- html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist() 
td<- tibble(data= td)

# Tweeter id:
# tid  <- str_extract(td[1],"@[[a-zA-Z]]+")
 td$DateTime<- str_extract(td$data,"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
   # td$Tweets <- str_split(td$data,"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
 
 
 
 td<- separate(td,data,c("TName","Tweet"),"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}",remove = FALSE)
 
 td$Month <- trimws(str_extract(td$DateTime," [A-Z]{1}[a-z]{1}[a-z]{1}"))
 td$weekDay <- str_extract(td$DateTime,"[A-Z]{1}[a-z]{1}[a-z]{1}")
 td$year <- str_extract(td$DateTime,"[0-9]{4}$")
 td$day <- str_extract(td$DateTime,"(?<= [A-Z]{1}[a-z]{1}[a-z]{1}) [0-9]{2}")
 #td$hash <- str_extract_all(td$Tweet,"#[0-9a-zA-X]+")
 # paste(unlist(str_extract_all(td$Tweet,"#[0-9a-zA-X]+")), collapse = ",")
View(tmp)
# td$hash <- mutate(td, hash =  str_extract_all(td$Tweet,"#[0-9a-zA-X]+") )
td <- mutate(td, hash = unlist(lapply(str_extract_all(td$Tweet,"#\\S+"), function(X){paste(unlist(X), collapse = ", ")})))
 
td$img <- html_nodes(tdata,'div.col-lg-4 img.img-thumbnail') %>% html_attr("src")
dtf <- rbind(dtf,td)

}
# write_csv(td,file.path(getwd(),"tweet.csv"))
# write.csv(td,file = "tweet.csv")



write.csv(dtf,"tweet.csv")

fsp3 <- gridfs(db = "MSProject3", url = "mongodb+srv://msds_user:msds@cluster0-bqyhe.gcp.mongodb.net/", prefix = "fs",options = ssl_options())
# fsp3$drop("tweet.csv")
fsp3$upload("tweet.csv")



# 
#   
#   
# 
#      tghtml_nodes(first_page,'.list') %>%   html_nodes('.our-price') %>% html_text()  %>%  str_trim() %>%    unlist() 
#      
#        html_nodes(first_page,'.list') %>%   html_nodes('.title') 
#        
#         html_nodes(first_page,'.list') %>%   html_nodes('.title') %>% html_text()  %>%  str_trim() %>%    unlist() 
#   str_trim() %>%    unlist() 
#    
#    
# 
#  # url of the deatil page : 
#           html_nodes(first_page,'.list span.title a') %>%   html_attr("href")
#  # Image source 
#           img <- '.list li img'
#            
#             html_nodes(first_page,img) %>% html_attr("src")
            
         
            
```


# API TRY
```{r eval=FALSE, include=FALSE}
library(httr)
library(jsonlite)

url <- "https://www.linkedin.com/in/gpiatetsky/"

urla <- "https://www.linkedin.com/in/gpiatetsky/"


# GET https://api.linkedin.com/v2/me?projection=(id,firstName,lastName,profilePicture(displayImage~:playableStreams))

api.linkedin.com

turl <- "https://api.linkedin.com/v2/me"

# Client ID: 78gerglk5nuy0z
# secret  Rydq1tmp9ZbX9lgU
#------------------------------------------------------------

library(httr)

# 1. Find OAuth settings for linkedin:
#    https://developer.linkedin.com/documents/linkedins-oauth-details
endpoints <- oauth_endpoints("linkedin")

# 2. Register an application at https://www.linkedin.com/secure/developer
#    Make sure to register http://localhost:1410/ as an "OAuth 2.0 Redirect URL".
#    (the trailing slash is important!)
#
#    Replace key and secret below.
myapp <- oauth_app("linkedin",
  key = "78gerglk5nuy0z",
  secret = "Rydq1tmp9ZbX9lgU"
)

# 3. Get OAuth credentials
token <- oauth2.0_token(endpoints, myapp)

# tokenl <- "https://www.linkedin.com/oauth/v2/accessToken?grant_type=client_credentials&client_id=78gerglk5nuy0z&client_secret=Rydq1tmp9ZbX9lgU"
# 
# ltoken <- oauth2.0_token(tokenl)
# 4. Use API
req <- GET("https://api.linkedin.com/v1/people/~", config(token = token))
stop_for_status(req)
content(req)

req <- GET("https://api.linkedin.com/v2/people/(id:_WPwkOf3ncX)", config(token = token))
stop_for_status(req)
content(req)




------------------------------------------

install.packages("Rlinkedin")
library(Rlinkedin)
# To use the default API and Secret Key for the Rlinkedin package:
in.auth <- inOAuth()

# To use your own application's API and Secret Key:
in.auth <- inOAuth("msds", "78gerglk5nuy0z", "Rydq1tmp9ZbX9lgU")
my.connections <- getMyConnections(in.auth)


 application_name <- "msds"
    consumer_key <- "78gerglk5nuy0z"
    consumer_secret <- "Rydq1tmp9ZbX9lgU"
    linkedin <- oauth_endpoint("requestToken", "authorize", 
      "accessToken", base_url = "https://www.linkedin.com/oauth/v2/authorization")
    myapp <- oauth_app(appname = application_name, consumer_key, 
      consumer_secret)
    token <- oauth2.0_token(linkedin, myapp)
    
    linkedin <- oauth_endpoint("requestToken", "authorize", 
      "accessToken", base_url = "https://www.linkedin.com/oauth/v2/authorization?")
    myapp <- oauth_app(appname = application_name, consumer_key, 
      consumer_secret)
    token <- oauth2.0_token(linkedin, myapp)

    search.ppl <- searchPeople(token=token, first_name="Michael", last_name="Piccirilli")
class(search.ppl)

getProfile(token)
connections.profiles <- getProfile(in.auth, connections = TRUE)
searchJobs(token = in.auth, keywords = "data scientist")
getCompany(token=in.auth, universal_name="the coca cola company")
getCompany(token=in.auth, email_domain = "columbia.edu")
getCompany(token=in.auth, company_id = company.email$company_id[14])



# id_secret <- RCurl::base64(paste("78gerglk5nuy0z","Rydq1tmp9ZbX9lgU",sep='&'))
id_secret <- RCurl::base64(paste("client_id=78gerglk5nuy0z","client_credentials=Rydq1tmp9ZbX9lgU",sep='&'))
my_headers <- httr::add_headers(c(Authorization=paste(
id_secret,sep='')))
my_body <- list(grant_type='client_credentials')
my_token <- httr::content(httr::POST('https://www.linkedin.com/oauth/v2/accessToken',
my_headers,body=my_body,encode='form'))
GET(call1, authenticate(username,password, type = "basic"))


```



# Linkedi 

```{r eval=FALSE, include=FALSE}

 url <- "https://www.linkedin.com/in/gpiatetsky/"
   first_page <- read_html(url)
   
   read_html(url)
   
   
   # --------------
   library(xml2)
library(rvest)
library(curl)
   

web_content <- read_html(curl('http://benchmarkrealestate.com/', handle = new_handle("useragent" = "Mozilla/5.0")))

web_content <- read_html(curl(url, handle = new_handle("useragent" = "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36")))
# -----------------
#----------------------------------------------------------------------------

install.packages("RSelenium")
library('RSelenium')
checkForServer() # search for and download Selenium Server java binary.  Only need to run once.
startServer() # run Selenium Server binary
remDr <- remoteDriver(browserName="firefox", port=47663) # instantiate remote driver to connect to Selenium Server

remDr <- remoteDriver(
  remoteServerAddr = "localhost",
  port = 47663,
  browserName = "firefox"
)

remDr <- remoteDriver(port = 47663)
remDr$open()

remDr$open(silent=T) # open web browser

library('XML')
master <- c()
n <- 5 # number of pages to scrape.  80 pages in total.  I just scraped 5 pages for this example.
for(i in 1:n) {
  site <- paste0("https://www.fidelity.com/fund-screener/evaluator.shtml#!&ntf=N&ft=BAL_all&msrV=advanced&sortBy=FUND_MST_MSTAR_CTGY_NM&pgNo=",i) # create URL for each page to scrape
  remDr$navigate(site) # navigates to webpage
  
  elem <- remDr$findElement(using="id", value="tbody") # get big table in text string
  elem$highlightElement() # just for interactive use in browser.  not necessary.
  elemtxt <- elem$getElementAttribute("outerHTML")[[1]] # gets us the HTML
  elemxml <- htmlTreeParse(elemtxt, useInternalNodes=T) # parse string into HTML tree to allow for querying with XPath
  fundList <- unlist(xpathApply(elemxml, '//input[@title]', xmlGetAttr, 'title')) # parses out just the fund name and ticker using XPath
  master <- c(master, fundList) # append fund lists from each page together
}

head(master)







   # Title
   Title <- html_nodes(first_page,'.title') %>%   html_text()  %>%  str_trim() %>%    unlist() 

```



#---------------------------------------
```{r}  
     
   first_page %>% 
        # The relevant tag
        html_nodes('.med') %>%      
        html_text() %>% 
        # Trim additional white space
        str_trim() %>%                       
        # Convert the list into a vector
        unlist()  
   
   
   
   library(rvest)
html <- read_html("http://www.imdb.com/title/tt1490017/")
cast <- html_nodes(html, "#titleCast .itemprop")
length(cast)
cast[1:2]


```

```{r}
## COllection of all the Function Needed 

 Final_Review <- NA


readAllAttribute<- function(html){
  getMoviewName <- Mname(html)
  str_split(getMoviewName,"\n") # Got the moview name 
 temp_movieName <- trimws(str_split(getMoviewName,"\n",simplify = TRUE))
  # Read Rating 
  rating <- rating(html)
  
  # Comment Title 
  title<- mtitle(html)
  
  # COntributor Name 
  name<- mrname(html)
  
  #Comment Content
  desc<- mdesc(html)
  # Retrun Final Data 
 suppressWarnings(rm(Final_Review))
 
  Final_Review <- data.frame(rating,desc[1:length(rating)],title[1:length(rating)],name[1:length(rating)])
  
  fm_movie_name <-  temp_movieName[1,1]
 fm_movie_year <-  str_replace_all(temp_movieName[1,2],"\\(|\\)|[A-Z]","")
 
 print ("----------------------------------------------------")
 print(fm_movie_name)
 print ("----------------------------------------------------")
 updateDB (fm_movie_name,fm_movie_year , Final_Review )

}




# Get Movie Name from Headerfir
Mname <- function(html){
      html %>% 
        # The relevant tag
        html_nodes('.parent') %>%      
        html_text() %>% 
        # Trim additional white space
        str_trim() %>%                       
        # Convert the list into a vector
        unlist()   
  
  
  
 
}


# Rating 



rating <- function(html){
      html %>% 
        # The relevant tag
        html_nodes('.rating-other-user-rating') %>%      
        html_text() %>% 
        # Trim additional white space
        str_trim() %>%                       
        # Convert the list into a vector
        unlist()                             
}



# Comment and Title 

mtitle <- function(html){
  
  if (!is.na(html_node(first_page,'.ipl-ratings-bar')))(
      html %>% 
        # The relevant tag
        html_nodes('.lister-item-content .title') %>%      
        html_text() %>% 
        # Trim additional white space
    map(1) %>%
        str_trim() %>%                       
        # Convert the list into a vector
        unlist()  )                           
}

# html_nodes(first_page,'.ipl-ratings-bar')
# COntributor Name 

mrname <- function(html){
   if (!is.na(html_node(first_page,'.ipl-ratings-bar')))(
      html %>% 
        # The relevant tag
        html_nodes('.display-name-link') %>%      
        html_text() %>% 
        # Trim additional white space
   
        str_trim() %>%                       
        # Convert the list into a vector
        unlist()  
   )
}



#display-name-date
#Desc .lister-item-content .content .text show-more__control

mdesc <- function(html){
   if (!is.na(html_node(first_page,'.ipl-ratings-bar')))(
      html %>% 
        # The relevant tag
        html_nodes('.content .text') %>%      
        html_text() %>% 
        # Trim additional white space
   
        str_trim() %>%                       
        # Convert the list into a vector
        unlist() 
   )
}

## End OF COllection Function 


```





