---
title: "Project_606"
author: "Rajwant Mishra"
date: "April 24, 2019"
output:
  html_document:
    code_folding: "hide"
    theme: sandstone
    highlight: tango
    toc: true
    df_print: paged
---

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>

  <!-- {.tabset .tabset-fade .tabset-pills} -->
```{r}
#  Final Project Format 
# 	The final report should be presented in more formal format. Consider your audience to be non data analysts. Fellow data analysts (i.e. students) will be able to access your R Markdown file for details on the analysis. Submit a Zip file with your R Markdown file, the HTML output, and any supplementary files (e.g. data, figures, etc.). You must address the five following sections:
# 
# Introduction: What is your research question? Why do you care? Why should others care?
# 
# Data: Write about the data from your proposal in text form. Address the following points:
# 
# Data collection: Describe how the data were collected.
# Cases: What are the cases? (Remember: case = units of observation or units of experiment)
# Variables: What are the two variables you will be studying? State the type of each variable.
# Type of study: What is the type of study, observational or an experiment? Explain how you've arrived at your conclusion using information on the sampling and/or experimental design.
# Scope of inference - generalizability: Identify the population of interest, and whether the findings from this analysis can be generalized to that population, or, if not, a subsection of that population. Explain why or why not. Also discuss any potential sources of bias that might prevent generalizability.
# Scope of inference - causality: Can these data be used to establish causal links between the variables of interest? Explain why or why not.
# Exploratory data analysis: Perform relevant descriptive statistics, including summary statistics and visualization of the data. Also address what the exploratory data analysis suggests about your research question.
# 
# Inference: If your data fails some conditions and you can't use a theoretical method, then you should use simulation. If you can use both methods, then you should use both methods. It is your responsibility to figure out the appropriate methodology.
# 
# Check conditions
# Theoretical inference (if possible) - hypothesis test and confidence interval
# Simulation based inference - hypothesis test and confidence interval
# Brief description of methodology that reflects your conceptual understanding
# Conclusion: Write a brief summary of your findings without repeating your statements from earlier. Also include a discussion of what you have learned about your research question and the data you collected. You may also want to include ideas for possible future research.
# 
```



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(DT)
library(stringr)
library(lubridate)
library(corrr)
library(psych)
library(readxl)
library(readr)
library(plotly)
library(lme4)
library(lmerTest)


```


---
title: DATA 606 Data Final Proposal
author: Rajwant Mishra
---

### Part 1 - Introduction {.tabset .tabset-fade .tabset-pills}

This is Sales data of from last 2 year, grouped by location and and quarter info.

<div class = "blue">

What is your research question? Why do you care? Why should others care?
 
 A. Does customer who baught more at the Quarter end , how did they perfrom during the followin quarter.
 B. Does customers buy more during the Quarter End
 C. Buid regression Model to predict the Order Quanitity 
 
</div> 



```{r eval=FALSE, include=FALSE}
# load data
workDir <- getwd()

filePath = paste0(workDir,"/Data")
g_max <- 1048576 
ctype = c("text","text","text","text","text","numeric","numeric","text","text","text","text","text","text","text")

 Q1_17 <-read_excel(path=paste0(filePath,"/","2017_Q1.xlsx"),sheet="Data",guess_max =g_max)
 Q2_17 <-read_xlsx(path=paste0(filePath,"/","2017_Q2.xlsx"),sheet="Data",guess_max =g_max)
 Q3_17 <-read_xlsx(path=paste0(filePath,"/","2017_Q3.xlsx"),sheet="DATA",guess_max =g_max)
 Q4_17 <-read_excel(path=paste0(filePath,"/","2017_Q4.xlsx"),sheet="DATA",guess_max =g_max)
 Q1_18 <-read_excel(path=paste0(filePath,"/","2018_Q1.xlsx"),sheet="Data",guess_max =g_max)
 Q2_18 <-read_excel(path=paste0(filePath,"/","2018_Q2.xlsx"),sheet="Data",guess_max =g_max)
 Q3_18 <-read_excel(path=paste0(filePath,"/","2018_Q3.xlsx"),sheet="Data",guess_max =g_max)
 Q4_18 <-read_excel(path=paste0(filePath,"/","2018_Q4.xlsx"),sheet="Data",guess_max =g_max)
 custmap <- read_excel(path=paste0(filePath,"/","custmap.xlsx"),col_types = c("text","text"))

 # Create Subset 
Q1_17 <- Q1_17[,c(2,3,4,5,6,7,8,9,10,14,15,16,17,18,19,20,21,22,25,26,27)]
Q2_17 <- Q2_17[,c(2,3,5,6,7,8,9,10,11,17,18,19,20,21,22,24,25,26,30,31,32)]
Q3_17 <- Q3_17[,c(1,2,4,5,6,7,8,9,10,16,17,18,19,20,22,23,24,25,29,30,31)]
Q4_17 <- Q4_17[,c(2,3,5,6,7,8,9,10,11,17,18,19,20,21,23,24,25,26,29,30,31)]
Q1_18 <- Q1_18[,c(1,2,4,5,6,7,8,9,10,16,17,18,19,20,22,23,24,25,29,30,31)]
Q2_18 <- Q2_18[,c(2,3,5,6,7,8,9,10,11,17,18,19,20,21,23,24,25,26,30,31,32)]
Q3_18 <- Q3_18[,c(2,3,5,6,7,8,9,10,11,18,19,20,21,22,24,25,26,27,31,32,33)]
Q4_18 <- Q4_18[,c(2,3,5,6,7,8,9,10,11,18,19,20,21,22,24,25,26,27,31,32,33)]

# rename Quarter info
   Q1_17$Qt <-   "Q1_17"
   Q2_17$Qt <-   "Q2_17"
   Q3_17$Qt <-   "Q3_17"
   Q4_17$Qt <-   "Q4_17"
   Q1_18$Qt <-   "Q1_18"
   Q2_18$Qt <-   "Q2_18"
   Q3_18$Qt <-   "Q3_18"
   Q4_18$Qt <-   "Q4_18"
   
   #Name COlumn
   names(Q2_17) <- names(Q1_17)
   names(Q3_17) <- names(Q1_17)
   names(Q4_17) <- names(Q1_17)
   names(Q1_18) <- names(Q1_17)
   names(Q2_18) <- names(Q1_17)
   names(Q3_18) <- names(Q1_17)
   names(Q4_18) <- names(Q1_17)
   
   #Combine data 
      
   rm(mkt_Data)
   mkt_Data <- bind_rows(
   Q1_17,
   Q2_17,
   Q3_17,
   Q4_17,
   Q1_18,
   Q2_18,
   Q3_18,
   Q4_18)

  write_csv(mkt_Data,"mkt_data.csv")
```


#### Building Data
```{r message=FALSE}

workDir <- getwd()

filePath = paste0(workDir,"/Data")
g_max <- 1048576 
ctype = c("text","text","text","text","text","numeric","numeric","text","text","text","text","text","text","text")   
 custmap <- read_excel(path=paste0(filePath,"/","custmap.xlsx"),col_types = c("text","text"))

mkt_Data <- read_csv("mkt_Data.csv",guess_max =g_max)
# 
 mkt_Data <- mkt_Data[sample(nrow(mkt_Data),5000),]
 


# upate new Acocunt info 
# custmap$KUNNR_OLD <- as.numeric(custmap$KUNNR_OLD)
# custmap$KUNNR_NEW<- as.numeric(custmap$KUNNR_NEW)
mkt_Data$Customer<- as.character(mkt_Data$Customer)
mkt_Data <- mkt_Data[which(!is.na(mkt_Data$Customer)),]
unique(mkt_Data$Qt)
head(mkt_Data[which(is.na(mkt_Data$Customer)),])
mkt_Data <- left_join(mkt_Data, custmap, by = c("Customer" = "KUNNR_OLD"))
mkt_Data$KUNNR_NEW <- mkt_Data$KUNNR_NEW.x
mkt_Data <- mkt_Data[,-c(23,24)]
# View(mkt_Data[which(is.na(mkt_Data$KUNNR_NEW)),])

mkt_Data$KUNNR_NEW[which(is.na(mkt_Data$KUNNR_NEW))] <- mkt_Data$Customer[which(is.na(mkt_Data$KUNNR_NEW))]
#set all Qty = 0 

# mkt_Data$`Order Quantity`[which(is.na(mkt_Data$`Order Quantity`))]  = 0

#Dropping some Text values from full data 
mkt_Datalean <- mkt_Data[,-c(1,2,3,4,5,6,7,8,7)]

# mkt_s getting data in from of customer and Qt spending

mkt_cust_qt <- mkt_Datalean %>% group_by(KUNNR_NEW,Qt) %>% summarise(order_unit = sum(`Order Quantity`))
mkt_cust_qt <- spread(mkt_cust_qt,Qt,order_unit,fill = 1)


```

### Part 2 - Data {.tabset .tabset-fade .tabset-pills}

  *Data*: (Write about the data from your proposal in text form. Address the following points:)
  *Data collection*: (Describe how the data were collected.)
This data is sample of sales by promotion for last two year. Data was shared by Marketting team to evaluate the sales performance. 

Note : For Confidencilaty names and numbers have been changed in the data.

   *Cases*: What are the cases? (Remember: case = units of observation or units of experiment)
In This sample we have 1000 rows. Each row identify the Order from the given customer.

   *Variables*: What are the two variables you will be studying? State the type of each variable.
  
  Order Quantity is Response variable here . It's Quantitative variable.
  Quarter `Qt`, is Independent varible , it's qualitative variable as we can't add them.
  
  Other qualitative variables: Brands,Promotions, Zipcode etc. 

  
  
   *Type of study*: What is the type of study, observational or an experiment? Explain how you've arrived at your conclusion using information on the sampling and/or experimental design.
  
  We do studies to gather information and draw conclusions. The type of conclusion we draw depends on the study method used: In an observational study, we measure or survey members of a sample without trying to affect them. In a controlled experiment, we assign people or things to groups and apply some treatment to one of the groups, while the other group does not receive the treatment.

This is observational study.

 *Scope of inference - generalizability*: Identify the population of interest, and whether the findings from this analysis can be generalized to that population, or, if not, a subsection of that population. Explain why or why not. Also discuss any potential sources of bias that might prevent generalizability.
  
Since data set is too big and would take too much computing power and machine time. To save time I'll be working on sample data of much smaller size. on computing I am keeping my sample size to 200 * 8 (each quarter from 2 year). These findings may be generlaized for the same customer and identify the posibility of sales .

   *Scope of inference - causality*: Can these data be used to establish causal links between the variables of interest? Explain why or why not.
  Since its a observational study we can't make Casue and Effect inference from here, but it being an observational study it will have some info about customers spending pttern over the years and Quarter.
  

  
```{r message=FALSE}
#---------------------------------------------------------------------
# Create Sample A of 1000 customer  
#---------------------------------------------------------------------

custA <- mkt_cust_qt[sample(nrow(mkt_cust_qt),1000),]
custAG <- gather(custA, key = "Qt",value = "order_unit",-KUNNR_NEW)
custAG$KUNNR_NEW <- as.character(custAG$KUNNR_NEW)
custAG$Qt = as.factor(custAG$Qt)
custAG$order_unit[which(is.na(custAG$order_unit))] <- 0
custAG$seq <- 0
custAG$seq[which(custAG$Qt=="Q1_17")] = 1
custAG$seq[which(custAG$Qt=="Q2_17")] =2
custAG$seq[which(custAG$Qt=="Q3_17")] = 3
custAG$seq[which(custAG$Qt=="Q4_17")] = 4
custAG$seq[which(custAG$Qt=="Q1_18")] = 5
custAG$seq[which(custAG$Qt=="Q2_18")] = 6
custAG$seq[which(custAG$Qt=="Q3_18")] = 7
custAG$seq[which(custAG$Qt=="Q4_18")] = 8
custAG <- custAG[order(custAG$seq),]

#---------------------------------------------------------------------
# Create Sample A of 1000 customer  
#---------------------------------------------------------------------


```


### Part 3 - Exploratory data analysis {.tabset .tabset-fade .tabset-pills}

  Perform relevant descriptive statistics, including summary statistics and visualization of the data. Also address what the exploratory data analysis suggests about your research question.
  

```{r message=FALSE}

  # Data 
 str(head(mkt_Data[,-c(1,2,3,4,5,6,7,8,7)]))
 summary((mkt_Data[,-c(1,2,3,4,5,6,7,8,7)]))
 head(mkt_Data[,-c(1,2,3,4,5,6,7,8,7)])
 
 ggplot(custAG,mapping = aes(x=KUNNR_NEW ,y= order_unit, color=Qt)) + geom_point()

ggplot(custAG,mapping = aes(y=KUNNR_NEW ,x= order_unit, color=Qt)) + geom_point() + facet_wrap(~Qt) 

# Graph by Plotly 
plot_ly(data=custAG,y=custAG$order_unit ,x= custAG$KUNNR_NEW,color=custAG$Qt, type= "scatter" )

# plot_ly(data=custAG,y=custAG$order_unit ,x= custAG$KUNNR_NEW, z = custAG$Qt ,color=custAG$Qt, type= "scatter3d" )


mkt_Data$KUNNR_NEW[which(is.na(mkt_Data$KUNNR_NEW))] 

custAG %>%
  group_by(Qt) %>% summarise(order_unit = sum(order_unit)) %>% 
  ggplot(mapping = aes(x= Qt,y=order_unit,fill = Qt)) + geom_col() +  geom_label(aes(label=order_unit))+
theme_light()

# Line plot with multiple groups
custAG[order(custAG$seq),] %>%
ggplot( aes(x= seq, y=order_unit, group= KUNNR_NEW,color=Qt) )+
  geom_line()+
  geom_point() 
  
# stacked 

# Line plot with multiple groups
custAG[order(custAG$seq),] %>%
ggplot( aes(x= seq, y=order_unit, group=KUNNR_NEW, fill=Qt) )+
  geom_col()


# > Check https://www.youtube.com/watch?v=SvKv375sacA
# > Try getting chi square test using porpatl differen of 1000 custoemr prove is speding in one qt is more or less in other following qt. 

```

### Part 4 - Inference {.tabset .tabset-fade .tabset-pills}

  If your data fails some conditions and you can't use a theoretical method, then you should use simulation. If you can use both methods, then you should use both methods. It is your responsibility to figure out the appropriate methodology.

We will examine sample of data after tidying of the data in the below formt :

|Customer | Q1 | Q2 |
|---------|----|----|
|1|21 | 12|
|2|21 | 12|
|3|21 | 12|
|4|21 | 12|
|.|. | .|
|.|. | .|
|n|n | n|
  
 Note :  # Check conditions
   # Theoretical inference (if possible) - hypothesis test and confidence interval
   # Simulation based inference - hypothesis test and confidence interval
   # Brief description of methodology that reflects your conceptual understanding
  
#### Chi-square Test 

**Test method**. Use the chi-square goodness of fit test to determine whether observed sample frequencies differ significantly from expected frequencies specified in the null hypothesis. 

This approach consists of four steps: 
(1) state the hypotheses :

The hypotheses for the Chi-squared test are as follows.

H0: There is *no* association between qunatity of Q1_17,Q2_17 and so on and so forth.
 pq1 = pq2 = pq3 ...

HA: There is an association between qunatity of Q1_17,Q2_17 and so on and so forth.

**Assumptions***
The chi-squared test, when used with the standard approximation that a chi-squared distribution is applicable, has the following assumptions:

**Simple random sample**
The sample data is a random sampling from a fixed distribution or population where every collection of members of the population of the given sample size has an equal probability of selection. Variants of the test have been developed for complex samples, such as where the data is weighted. Other forms can be used such as purposive sampling.

> n= 50  and 500, selcted randomly from sample . satisfying Simple Random sample 

**Sample size (whole table)**
A sample with a sufficiently large size is assumed. If a chi squared test is conducted on a sample with a smaller size, then the chi squared test will yield an inaccurate inference. The researcher, by using chi squared test on small samples, might end up committing a Type II error.

> sample size is big enough and with multiple itration I have tried to get possible result. 

**Expected cell count**
Adequate expected cell counts. Some require 5 or more, and others require 10 or more. A common rule is 5 or more in all cells of a 2-by-2 table, and 5 or more in 80% of cells in larger tables, but no cells with zero expected count. When this assumption is not met, Yates's correction is applied.

> Cell count is more that 2 X 2 matrix dat value. 

**Independence**
The observations are always assumed to be independent of each other. This means chi-squared cannot be used to test correlated data.

> 
 + Each sample is choosen randonmly from the Population. We can say that they are meeting the condition on Indepenence
 + Then sample of 50 and 500 was created to run the chi-test.
 

(2) formulate an analysis plan, 

We will create Sample of 1000 datapoints where records are spread in column for each quarter.
we would be doing regular chi square test and then we would switch to Multiple simulation

Significance level. significance levels 0.05, or 0.10; but any value between 0 and 1 can be used.



(3) analyze sample data


```{r message=FALSE}

custA$KUNNR_NEW <- as.character(custA$KUNNR_NEW )
summary(custA)
head(custA)
# List test condtion for CHi-Square
# 1.  Are the value independent for each Quarter : 
# 2. 

# Ho : Customer are not baised on speding over Quarter . pq1 = pq2 = pq3 ...
# Ha : Customer are spending less or more compared to last quarter.




# 1. convert the data as a table with only Quater data .

dt <- as.table(as.matrix(custA[,]))


# Test 1 
chi_custA <-  custA[1:50,c(2,4,6,8,3,5,7,9)]  %>% 
            chisq.test(test_custA)
chi_custA
chi_custA$observed
chi_custA$expected
plot(chi_custA$residuals)

chi_custA <-  custA[1:50,c(2,4,6,8,3)]  %>% 
            chisq.test()
chi_custA
chi_custA$observed
chi_custA$expected
plot(chi_custA$residuals)

(gather((as.data.frame(chi_custA$residuals)),"Qt","Val")) %>% ggplot(mapping = aes(x=Qt,y= Val))+
  geom_col()



# Test 2 
chi_custA <- custA[sample(1:nrow(custA),500),c(2,4,6,8,3,5,7,9)]   %>% 
             chisq.test(simulate.p.value = TRUE)
chi_custA
chi_custA$observed
chi_custA$expected
round(chi_custA$residuals, 3)
plot(chi_custA$residuals)
chi_custA$p.value

 # Test 3 
custA[sample(1:nrow(custA),10),c(2,4)]   %>% 
             chisq.test()

custA[sample(1:nrow(custA),10),c(4,6)]   %>% 
             chisq.test()
 

```

> Since p-value = `chi_custA$p.value` , which is less than .05 so we can reject the null hypothesis. So we see there is dependancy in the spending over the quarter. 

#### Using Simulation 

```{r message=FALSE}

#Multiple simulation
#Doing multiple check 
rm(p_chi)
p_chi <- data.frame(Qt12= rep(0, 50),Qt23= rep(0, 50),Qt34= rep(0, 50))
for(i in 1:50){
  
   chi_test <- custA[sample(1:nrow(custA),10),c(2,4)]   %>% 
             chisq.test()
  p_chi$Qt12[i]  <- chi_test$p.value
  
  chi_test <- custA[sample(1:nrow(custA),10),c(4,6)]   %>% 
             chisq.test()
  p_chi$Qt23[i]  <- chi_test$p.value
  
  
  
  chi_test <- custA[sample(1:nrow(custA),10),c(6,8)]   %>% 
             chisq.test()
  
  p_chi$Qt34[i]  <- chi_test$p.value
  # samp <- sample(c("atheist", "non_atheist"), n, replace = TRUE, prob = c(p, 1-p))
  # p_hats[i] <- sum(samp == "atheist")/n
}

# "Chi-squared approximation may be incorrectChi-squared "   The warning message found in the solution above is due to the small cell values in the contingency table. 
#lets check the key
table(gather(p_chi)$key)

#Gather the data so that we can plot on it.
#using geom_jitter to add some noise as most of poits would overlap in normal plot.
gather(p_chi) %>% ggplot(mapping = aes(x= key, y= round(value,3))) + geom_point()+geom_jitter()

gather(p_chi)%>% filter(value > 0.05)
#contigency table 

 rowSums(custA[,-c(1)])
 colSums(custA[,-c(1)])

```


(4) **interpret results**.

<div class = "blue">

Answer to Business Question A: Does customer who baught more at the Quarter end , how did they perfrom during the followin quarter.

</div>

> Since p-value = `chi_custA$p.value` , which is less than .05 so we can reject the null hypothesis. So we see there is dependancy in the spending over the quarter. 
> Multiple simulation also indicatess pvlaue < 0.05 , for Aplha = 0.5 it indicates that we are 95% confident that spending over each quater is depended on spending over last quarter. 

> Limitation : We can't evalaute spending is more or less using Chi-square test , as it is goodness of fittest only.


### Regression {.tabset .tabset-fade .tabset-pills}
#### Build Data and Summary

Lets evaluate Promotion and qty realtion Qt.

```{r message=FALSE}

# Creating subset of data 
lm_cust <- mkt_Datalean[,c(15,4,1,3,5,7,11,12,13,14)]
lm_cust<- rename(lm_cust , promo = `External Description` )
lm_cust$KUNNR_NEW = as.character(lm_cust$KUNNR_NEW)
lm_cust$Brand = as.factor(lm_cust$Brand)
lm_cust$zip = as.factor(lm_cust$zip)
lm_cust$city = as.factor(lm_cust$city)
lm_cust$state = as.factor(lm_cust$state)
lm_cust$Qt = as.factor(lm_cust$Qt)


# Check data 
head(lm_cust)
summary(lm_cust)

lm_cust[which(is.na(lm_cust$state)),]

lm_cust <- lm_cust %>% group_by(KUNNR_NEW,Qt,Brand,`Order Date`,promo,city,state) %>% summarise(Order_Qty = sum(`Order Quantity`),Doll_Val = sum(`Promotion Order Doll`))

lm_cust[which(lm_cust$Brand=="OO"),] %>% ggplot(mapping = aes(x= promo, y= Order_Qty,group= Qt , color = Qt)) + geom_line()+
  geom_point() + geom_jitter()+ facet_grid(year(`Order Date`)~ .)

 ggplot(lm_cust,mapping = aes(x= Qt, y= Order_Qty, group= Brand ,color= Brand )) + geom_line()+
  geom_point() + geom_jitter()
 
 # Brand by Year 
 lm_cust[which(lm_cust$Brand %in% c("RB","RJ")),] %>% group_by(promo,Brand,Qt,year =year(`Order Date`)) %>%summarise(Order_Qty = sum(Order_Qty)) %>% 
   ggplot(mapping = aes(x=Brand, y = Order_Qty,fill = Qt)) + 
   geom_col()+facet_grid(year~ .)+
   theme(axis.text.x = element_text(angle = 70, hjust = 1)) + 
   scale_y_continuous( labels = scales::number)
 
 # promo by year and Brand
 
  lm_cust[which(lm_cust$Brand %in% c("RB")),] %>% group_by(promo,Brand,Qt,year =year(`Order Date`)) %>%summarise(Order_Qty = sum(Order_Qty)) %>% 
   ggplot(mapping = aes(x=promo, y = Order_Qty,fill = Qt)) +
   geom_col( position = "dodge")+facet_grid(Brand+year~ .,scales="free")+
   theme(axis.text.x = element_text(angle = 70, hjust = 1)) + 
   scale_y_continuous( labels = scales::number) 
  
  #Plotly check
  plotData  <-  lm_cust[which(lm_cust$Brand %in% c("RB")),] %>% group_by(promo,Brand,Qt,month =month(`Order Date`)) %>%summarise(Order_Qty = sum(Order_Qty)) 
  
  
  plot_ly(x = plotData$promo, y = plotData$Order_Qty, mode= "marker", type = "bar", data= plotData, color= plotData$Qt)
  
  lm_cust[which(lm_cust$Brand %in% c("RB")),] %>% group_by(promo,Brand,Qt,month =month(`Order Date`)) %>%summarise(Order_Qty = sum(Order_Qty)) 

  
  linPlot <- lm_cust[which(lm_cust$Brand %in% c("RB","OO")),] %>% group_by(Brand,date = month(`Order Date`) )%>%summarise(OrderQty = sum(Order_Qty))
  
  linPlot <- lm_cust[which(lm_cust$Brand %in% c("RB","OO")),] %>% group_by(Brand,date = (format(`Order Date`, "%Y-%m")) )%>%summarise(OrderQty = sum(Order_Qty))
  
  linPlot <- lm_cust[which(lm_cust$Brand %in% c("RB","OO")),] %>% group_by(Brand,date = (month(`Order Date`)) , year =  year(`Order Date`))%>%summarise(OrderQty = sum(Order_Qty))
  
  
    plot_ly(x= linPlot$date,y= linPlot$OrderQty, color = as.factor(linPlot$Brand) , data = linPlot[which(linPlot$year==2017),], linetype = I("Brand"))
  
    linPlot$year <- as.factor(linPlot$year)
    plot_ly(x= linPlot$date,y= linPlot$OrderQty, color = as.factor(linPlot$year) , data = linPlot[which(linPlot$brand=="RB"),], linetype = "solid")
    
     linPlot <- lm_cust[which(lm_cust$Brand %in% c("RB","OO","TY")),] %>% group_by(Brand,dateL = `Order Date`)%>%summarise(OrderQty = sum(Order_Qty))
      plot_ly(data= linPlot,x= month(linPlot$dateL), y = linPlot$OrderQty,color = linPlot$Brand , linetype = 'dot' )
     
     # str_replace(str_extract(linPlot$dateL[1],"\\d+_\\d{2}"),"_","")
     linPlot <- lm_cust[which(lm_cust$Brand %in% c("RB","OO","TY")),] %>% group_by(Brand,
                          dateL = Qt
                        )%>%summarise(OrderQty = sum(Order_Qty))
     linPlot$Qt <-  as.numeric(str_replace_all(str_extract_all(linPlot$dateL,"\\d+_\\d{2}"),"_",""))
     
     plot_ly(data= linPlot,x= linPlot$Qt, y = linPlot$OrderQty,color = linPlot$Brand , linetype = 'dot' )
    
   
  
# check if all promotions are running every year
table(year(lm_cust$`Order Date`),lm_cust$promo)

```


#### Model Creation 
Try Predicting Quantity based on Known informaiton . 
Identifying Best model to predict Quantity 

```{r message=FALSE}

# pairs.panels(lm_cust[,c(2,3)])

# Try Predicting Quantity based on Known informaiton . 
# Identifying Best model to predict Quantity 

head(lm_cust)
result <- data.frame(var = 1:10,pval = 1:10, comment=1:10)

result$var <- "A"
result$pval <- "A"
result$comment <- "A" 

# Working With Sample 
set.seed(42672)
 lm_cust_s1 <- lm_cust[sample(nrow(lm_cust),200),]
 lm_cust_s1$month <- month(lm_cust_s1$`Order Date`)
 lm_cust_s1$month <- as.factor(lm_cust_s1$month )
 
 names(lm_cust_s1)
 


lm1 <- lm(Order_Qty ~ Qt + Brand + promo + month + state,lm_cust_s1)
summary(lm1)
anova(lm1)
result$var[1] <- "lm(formula = Order_Qty ~ Qt + Brand + promo + month + state, 
    data = lm_cust_s1)"
result$pval[1] <-  "Adjusted R-squared:  0.2356 F-statistic: 1.568 on 108 and 91 DF,  p-value: 0.01388"

result$comment[1] <- "(Month , State ) Qt,Promo codes are significant,state is not " 


#Dropping State

lm2 <- lm(Order_Qty ~ Qt + Brand + promo + month,lm_cust_s1)
summary(lm2)
anova(lm2)

result$var[2] <- "lm(formula = Order_Qty ~ Qt + Brand + promo + month, data = lm_cust_s1)"
result$pval[2] <- "Adjusted R-squared:  0.3198 F-statistic: 2.396 on 67 and 132 DF,  p-value: 9.788e-06"
result$comment[2] <- "(Month)Brand is not significant" 

#Promo promoVPFP200 turns out to more significant here. 



#3  Dropping brand info 
lm3 <- lm(Order_Qty ~  Qt + promo + month,lm_cust_s1)
summary(lm3)
anova(lm3)

result$var[3] <- "lm(formula = Order_Qty ~ Qt + promo + month, data = lm_cust_s1)"
result$pval[3] <- "Adjusted R-squared:  0.2799 F-statistic:  3.09 on 37 and 162 DF,  p-value: 4.911e-07"
result$comment[3] <- "(Month) least significant" 

#4  Dropping Month
lm4 <- lm(Order_Qty ~  Qt + promo ,lm_cust_s1)
summary(lm4)
anova(lm4)

result$var[4] <- "lm(formula = Order_Qty ~ Qt + promo, data = lm_cust_s1)"
result$pval[4] <- "Adjusted R-squared:  -0.003147 F-statistic: 0.9799 on 31 and 168 DF,  p-value: 0.5038"
result$comment[4] <- "not much significant"

# 5 Increase Sample Size
rm(lm_cust_s1)
# SInce I see no major significance increasing the sample size to 500
set.seed(12121)
lm_cust_s2 <- lm_cust[sample(nrow(lm_cust),500),]
 lm_cust_s2$month <- month(lm_cust_s2$`Order Date`)
 lm_cust_s2$month <- as.factor(lm_cust_s2$month )
 names(lm_cust_s2)
 
 
 lm5 <- lm(Order_Qty ~ Qt + Brand + promo + month + state,lm_cust_s2)
summary(lm5)
anova(lm5)

result$var[5] <- "500: lm(formula = Order_Qty ~ Qt + Brand + promo + month + state,     data = lm_cust_s2)"
result$pval[5] <-   " Adjusted R-squared:  0.05218 F-statistic: 1.214 on 128 and 370 DF,  p-value: 0.08366"
result$comment[5] <- "(Sig: Promo) " 
 
#6 Dropping Month, I want to keep state for now.

lm6 <- lm(Order_Qty ~ Qt + Brand + promo + state,lm_cust_s2)
summary(lm6)
anova(lm6)

result$var[6] <- "lm(formula = Order_Qty ~ Qt + Brand + promo + state, data = lm_cust_s2)"
result$pval[6] <- "Adjusted R-squared:  0.04593 F-statistic:   1.2 on 120 and 378 DF,  p-value: 0.1016"

result$comment[6] <- "promo is very less significant now, nothing else is significant" 

#7 Drop Brand

set.seed(12123)

lm7 <- lm(Order_Qty ~  Qt + promo + state,lm_cust_s2)
summary(lm7)
anova(lm7)


result$var[7] <- "lm(formula = Order_Qty ~ Qt + promo + state, data = lm_cust_s2)"
result$pval[7] <- "Adjusted R-squared:  0.04285 F-statistic: 1.251 on 89 and 409 DF,  p-value: 0.07818"

result$comment[7] <- "not much. sig " 



#8 Drop state and Brand

set.seed(12126)

lm8 <- lm(Order_Qty ~    Qt + promo + Brand,lm_cust_s2)
summary(lm8)
anova(lm8)

result$var[8] <- "lm(formula = Order_Qty ~ Qt + promo + Brand, data = lm_cust_s2)"
result$pval[8] <- "Adjusted R-squared:  0.04555 F-statistic:  1.34 on 70 and 429 DF,  p-value: 0.04416"
result$comment[8] <- "not much sig." 


lm9 <- lm(formula = Order_Qty ~ Qt + Brand + promo + month, data = lm_cust_s2)	
summary(lm9)
anova(lm9)
result$var[9] <- "lm(formula = Order_Qty ~ Qt + Brand + promo + month, data = lm_cust_s2)"
result$pval[9] <- "Adjusted R-squared:  0.05115 F-statistic: 1.345 on 78 and 421 DF,  p-value: 0.03633"
result$comment[9] <- "Pvalue looks less than  to .05"

lm10 <- lm(formula = Order_Qty ~ Qt + promo + month, data = lm_cust_s2)
summary(lm10)
anova(lm10)
result$var[10] <- "lm(formula = Order_Qty ~ Qt + promo + month, data = lm_cust_s2)"
result$pval[10] <- "Adjusted R-squared:  0.0448 
F-statistic: 1.498 on 47 and 452 DF,  p-value: 0.02147"
result$comment[10] <- "not much sig."


# p-value: 0.02331
lm_pbm <- lm(formula = Order_Qty ~  promo + Brand + month, data = lm_cust_s2)

summary(lm(formula = Order_Qty ~  promo + Brand + month, data = lm_cust_s2))
anova(lm(formula = Order_Qty ~  promo + Brand + month, data = lm_cust_s2))

# p-value: 0.0177
lm_pd <- lm(formula = Order_Qty ~  promo  + `Order Date`, data = lm_cust_s2)
anova(lm(formula = Order_Qty ~  promo  + `Order Date`, data = lm_cust_s2))
summary(lm(formula = Order_Qty ~  promo  + `Order Date`, data = lm_cust_s2))

#p-value: 0.0169
lm_py <- lm(formula = Order_Qty ~  promo  + year(`Order Date`), data = lm_cust_s2)
summary(lm(formula = Order_Qty ~  promo  + year(`Order Date`), data = lm_cust_s2))
anova(lm(formula = Order_Qty ~  promo  + year(`Order Date`), data = lm_cust_s2))

#p-value: 0.4186  Not considering 
summary(lm(formula = Order_Qty ~  month + Brand + Qt , data = lm_cust_s2))

#p-value: 0.0569
lm_pbms <- lm(formula = Order_Qty ~  Brand + promo + month + state,   data = lm_cust_s2)

summary(lm(formula = Order_Qty ~  Brand + promo + month + state,   data = lm_cust_s2))


# p-value: 0.08366 
lmdata<-na.omit(lm_cust_s2) 
lm_qbpms <-(lm(formula = Order_Qty ~  Qt + Brand + promo +  `Order Date`,   data = lm_cust_s2))
summary(lm_qbpms)

```



#### Assess the linear model

From above analyis I would use differnt model to valdiate the result. Lets take model lm_pbm which is model for promotion + brand + month would predict order Quanity.

 To assess whether the linear model is reliable, we need to check for 
 (1) linearity, 
 (2) nearly normal residuals, and 
 (3) constant variability. 
 (4) Residuals are independent
  Residual = Observed value - Predicted value 

```{r message=FALSE}


# From above analyis I would use differnt model to valdiate the result. Lets take model lm_pbm which is model for promotion + brand + month would predict order Quanity .

# To assess whether the linear model is reliable, we need to check for 
#(1) linearity, 
#(2) nearly normal residuals, and 
#(3) constant variability. 
#(4) Residuals are independent
# Residual = Observed value - Predicted value 

library(DATA606)
plot_ss(x = lm_cust_s2$Order_Qty, y = lm_pbm$residuals,showSquares = TRUE)

# # (1) Linear association: The residuals plot shows a random scatter.
#Based on the plot we can clearly say that there is apparent pattern in the distribution as the numbers appear to be group and outlier are close to the regression line, so it can be treated as strong corelation and can be considered as a linear relationship.

# # (2) Nearly normal residuals: To check this condition, we can look at a histogram
hist(lm_pbm$residuals)

# or a normal probability plot of the residuals.
#It seems the plot is slightly skewed left, 
qqnorm(lm_pbm$residuals)
qqline(lm_pbm$residuals)  # adds diagonal line to the normal prob plot
# (3) we can say that its also  Nearly normal residuals even though its right skewed with few outliers . 
# (4)Residuals can be treated as independent as sample is drawn from independent .


# plot on sample
ggplot(data = lm_cust_s2,mapping = aes(y=lm_cust_s2$Order_Qty ,x= lm_cust_s2$promo))+ geom_point() + geom_smooth(method = "lm",se=FALSE) + geom_abline(slope = lm_pbm$coefficients[8], intercept = lm_pbm$coefficients[1], color="red")


# regression Line population data of 2 year.
(ggplot(data = mkt_Data,mapping = aes(y=mkt_Data$`Order Quantity` ,x= mkt_Data$`External Description`))+ geom_point() + geom_smooth(method = "lm",se=FALSE) + geom_abline(slope = lm_pbm$coefficients[8], intercept = lm_pbm$coefficients[1], color="red") )


# Lets plot on by Order Date 
# To assess whether the linear model is reliable, we need to check for 
#(1) linearity, 
#(2) nearly normal residuals, and 
#(3) constant variability. 
#(4) Residuals are independent
# 
plot_ss(x = lm_cust_s2$Order_Qty, y = lm_pd$residuals,showSquares = TRUE) #  Linear association
hist(lm_pd$residuals) # Right skewed 
qqnorm(lm_pd$residuals)
qqline(lm_pd$residuals) # very much on the regression line, Nearly normal residuals even though its right skewed


# On Sample

ggplot(data = lm_cust_s2,mapping = aes(y=lm_cust_s2$Order_Qty ,x= lm_cust_s2$`Order Date`))+ geom_point() + geom_smooth(method = "lm",se=FALSE) +
 geom_abline(slope = lm_pd$coefficients[8], intercept = lm_pd$coefficients[1], color="green")+
  geom_abline(slope = lm_pbm$coefficients[8], intercept = lm_pbm$coefficients[1], color="red")

# On Population 
ggplot(data = mkt_Data,mapping = aes(y = mkt_Data$`Order Quantity`,x= mkt_Data$Brand))+ geom_point() + geom_smooth(method = "lm",se=FALSE) +  geom_abline(slope = lm_pd$coefficients[8], intercept = lm_pd$coefficients[1], color="red") +
  geom_abline(slope = lm_pbm$coefficients[8], intercept = lm_pbm$coefficients[1], color="green")



plot(lm_pbm)
plot(lm_pd)


```


<div class = "row">
  
<div class = "col-md-12">
 Residuals: We can see that the multiple regression model1 has a smaller range for the residuals as compared with Model 2: (Model 1) -59 to 339 vs.(model 2) -62.02 to 356.44.  Secondly the median of the multiple regression model 1 is much closer to 0 than the model 2 regression model.

 Coefficients:
 (Intercept): The intercept is the left over when you average the independent and dependent variable.  In the simple regression Model 1 we see that the intercept is 20.02172 which is close tp ZERO, and Model 2 has much larger intercept ie. 266.9  (format( 2.669e+02, scientific = FALSE))  meaning there's a fair amount left over.  Model 1 looks close fit with nearrest to ZERO intercept. 

 promo: Both multiple regression model shows that when we add promo variable it's multiplying this variable times  the numeric (ordinal) value of the Promotion code.So for every promocode in the year, you add an additional estimated column unit value in sales.  For example : promoRSD will add 130 Unit each MOnth.promoWild Card will add 53 Unit 

 Brand : So far every brand addition would add resepctive value in the sales unit by multiplying the brand intercept with its ordinal vlaue. FOr example  addition of brand AX would add 3 unit each month. 

 Month: When we add in the Month variable it's multiplying this variable times the numeric (ordinal) value of the month.  For example July and August 
</div>
  
<div class = "col-md-6">
```{r, message=FALSE, echo=FALSE}
#Lets take two model and undersatnd its summary
##--------------------------------------------Model 1 
summary(lm_pbm)

lm_pbm$xlevels
# Residuals:
#    Min     1Q Median     3Q    Max 
# -59.94  -8.49  -2.41   4.08 339.09 
# # Equation of Regression line :
# y = b+ mx 
# y = B0 + B1 X Promo  + B2 X Month + B3 X Brand
#  e <-  20.02172 
cc <- lm_pbm$coefficients
(eqn_pbm <- paste("Regression Formula Y =", paste(round(cc[1],2), paste(round(cc[-1],2), names(cc[-1]), sep=" * ", collapse=" + "), sep=" + "), "+ e"))

```
</div>
  


<div class = "col-md-6">
```{r, message=FALSE, echo=FALSE}
##--------------------------------------------Model 2
summary(lm_pd)
anova(lm_pd)
lm_pd$xlevels
#Residuals:
#    Min     1Q Median     3Q    Max 
# -62.02  -9.56  -4.12   2.91 356.44 

```
</div>

<div class = "col-md-12">

Analyis of Variance table 

```{r}
anova(lm_pbm,lm_pd)


```
 Function perfroms an analysis of variance of the two models using an F-test to assess the significanxe of the differences.
 
 We can see Model has decreased the Sum of the Squared error, and the value of 0.1977 says that we can be 80% confidence in saying that they models are different. 



</div>
</div>

#### Evaluation of the Model

Using test and sample data we will see how good our Model is.
We wil luse Absolute Mean error of the model and decide which regression models works well for the sample.

```{r}

# lm_cust_s2 
# lm_cust_s2 <- lm_cust[sample(nrow(lm_cust),500),]
#  lm_cust_s2$month <- month(lm_cust_s2$`Order Date`)
#  lm_cust_s2$month <- as.factor(lm_cust_s2$month )
#  names(lm_cust_s2)
#  

# Uisng same sample to test lm_cust_s2 , creating sample of 500 more to test.

lm_cust_t1  <- lm_cust_s2
lm_cust_t2 <- lm_cust[sample(nrow(lm_cust),500),]
 lm_cust_t2$month <- month(lm_cust_t2$`Order Date`)
 lm_cust_t2$month <- as.factor(lm_cust_t2$month )
 names(lm_cust_t2)
 
 lm_pbm1 <-  update(lm_pbm,lm_cust_s2)
 
 
 # Predict main train data
lm_pred_pbm <- predict(lm_pbm,lm_cust_t1)
lm_pred_pd <- predict(lm_pd,lm_cust_t1)
lm_pred_qbpms <- predict(lm_qbpms,lm_cust_t1)


mean(abs(lm_cust_t1$Order_Qty- lm_cust_t1$Order_Qty))
mean(abs(lm_pred_pbm - lm_cust_t1$Order_Qty))
mean(abs(lm_pred_pd - lm_cust_t1$Order_Qty))
mean(abs(lm_pred_qbpms - lm_cust_t1$Order_Qty))

# Using test data of new set
# lm_pbm <- lm(formula = Order_Qty ~ promo + Brand + month, data = lm_cust_t2)

update
lm_pred_pbm <- predict(lm(formula = Order_Qty ~ promo + Brand + month, data = lm_cust_t2),lm_cust_t2)
lm_pred_pd <- predict(lm(formula = Order_Qty ~ promo + `Order Date` , data = lm_cust_t2),lm_cust_t2)
lm_pred_qbpms <- predict(lm(formula = Order_Qty ~ Qt + Brand+promo +month+ state , data = lm_cust_t2),lm_cust_t2)


mean(abs(lm_cust_t2$Order_Qty- lm_cust_t2$Order_Qty))
mean(abs(lm_pred_pbm - lm_cust_t2$Order_Qty))
mean(abs(lm_pred_pd - lm_cust_t2$Order_Qty))
mean(abs(lm_pred_qbpms - lm_cust_t2$Order_Qty))




```

From above mean errors we can see that model 1 with regession model with formula `lm(formula = Order_Qty ~ promo + Brand + month, data = lm_cust_t2)` is well close to zero in terms of mean absulate error.

</div>


```{r eval=FALSE, include=FALSE}



#### Using Repeated Measures ANOVA in R || lme4
#Started for testing...<Not applicable for analyis now>
# install.packages("lmerTest")

head(lm_cust_s2)
# lm(formula = Order_Qty ~ Qt + promo + month, data = lm_cust_s2)
model1 <- lmer(Order_Qty ~ Qt + (1|KUNNR_NEW),data = lm_cust_s2)
anova(model1)

model2 <- lmer(Order_Qty ~ promo + (1|KUNNR_NEW),data = lm_cust_s2)
anova(model2)
summary(model2)
## 0.02895 *

model3 <- lmer(Order_Qty ~  city + (1|KUNNR_NEW),data = lm_cust_s2)
anova(model3)
summary(model3)



```


```{r eval=FALSE, include=FALSE}
# install.packages("plotly")
# install.packages("Rcpp")
library(plotly)
library(Rcpp)
# install_github('ramnathv/rCharts', force= TRUE)
library(rCharts)

library(rCharts)

nPlot(Order_Qty ~   promo , type = 'multiBarChart',data=lm_cust_s2, type = "point") 

nPlot(Order_Qty ~  Qt , group= 'promo', data=lm_cust_s2, type = "Bar") 



mk <- mkt_Datalean[which(mkt_Data$Brand=="CH"),] %>% group_by(`Order Date`,Qt)%>% summarise(ordQty = sum(`Order Quantity`)) %>% spread(Qt,ordQty)
  
#   
# mPlot(x = day(mk$`Order Date`),y=c("Q1_17","Q2_17"), type = "Line", data= mk)
# library(leaflet)
# mp3 <- Leaflet$new()
# mp3$setView(c(51,505, -0.09), zoom = 12)
# mp3$marker(c(51,505, -0.09), bindPopup = "TEST HELLP")
# mp3



```



```{r eval=FALSE, include=FALSE}

#------------------------------------------SAMPLE ----------------------


#Create sample 

# mkt_s <- sample_n(mkt_Data, size = 5000, replace = FALSE)
 mkt_s <-    bind_rows(
   Q1_17[sample(nrow(Q1_17),200),],
   Q2_17[sample(nrow(Q2_17),200),],
   Q3_17[sample(nrow(Q3_17),200),],
   Q4_17[sample(nrow(Q4_17),200),],
   Q1_18[sample(nrow(Q1_18),200),],
   Q2_18[sample(nrow(Q2_18),200),],
   Q3_18[sample(nrow(Q3_18),200),],
   Q4_18[sample(nrow(Q4_18),200),])

 
#mkt_s <- mkt_Data %>% sample_frac(0.05)

head(mkt_s[which(is.na(mkt_s$Customer)),])
unique(mkt_s$Qt)
mkt_s <- mkt_s[which(!is.na(mkt_s$Customer)),]
unique(mkt_s$Qt)
# mkt_s2 <- dplyr:: left_join(mkt_s, custmap, by = c("Customer" = "KUNNR_OLD"))

unique(mkt_s$Qt)
head(mkt_s[which(is.na(mkt_s$KUNNR_NEW)),])
install.packages("xlsx")
library("xlsx")
write.xlsx(mkt_s,"ap.xlsx",sheetName = "Sheet1", 
  col.names = TRUE, row.names = TRUE, append = FALSE)
write.xlsx(mkt_s2,"ap2.xlsx",sheetName = "Sheet1", 
  col.names = TRUE, row.names = TRUE, append = FALSE)


ggplot(mkt_s, mapping = aes(x= as.character(mkt_s$`Promotion Order Doll`), y= mkt_s$`Order Quantity`, color= mkt_s$`External Description`))+
  geom_point()+theme(axis.text.x = element_text(angle = 70, hjust = 1)) + facet_wrap(~mkt_s$Qt)

 # (nms <- names(read_excel(path=paste0(filePath,"/","2017_Q3.xlsx"), sheet="DATA",n_max = 0)))

# rm(Q1_17)
# rm(Q2_17)
# rm(Q3_17)
# rm(Q4_17)
# rm(Q1_18)
# rm(Q2_18)
# rm(Q3_18)
# rm(Q4_18)


#Dropping some Text values from sample 
mkt_s <- mkt_s[,-c(1,2,4,5,6,7,8,7)]

# mkt_s getting data in from of customer and Qt speding

cust_qt <- mkt_s %>% group_by(Customer,Qt) %>% summarise(order_unit = sum(`Order Quantity`))

spread(cust_qt,Qt,order_unit)
```



### Part 5 - Conclusion
Write a brief summary of your findings without repeating your statements from earlier. Also include a discussion of what you have learned about your research question and the data you collected. You may also want to include ideas for possible future research.
# 

### References
https://stats.stackexchange.com/questions/405243/compare-sales-data-over-time-in-sequence

https://www.machinelearningplus.com/machine-learning/complete-introduction-linear-regression-r/
 
https://university.business-science.io/courses/541056/lectures/9826285
https://www.youtube.com/watch?v=SvKv375sacA

 
### Appendix (optional)

Remove this section if you don't have an appendix









