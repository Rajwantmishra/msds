---
title: "607_project"
author: "Rajwant Mishra"
date: "April 28, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(DT)
library(stringr)
library(lubridate)
library(corrr)
library(psych)
library(readxl)
library(readr)
library(plotly)
library(lme4)
library(lmerTest)
library(tm)
library(wordcloud)
library(e1071)
library(gmodels)
# https://www.dataquest.io/blog/statistical-learning-for-predictive-modeling-r/
```


```{r message=FALSE}

# load data
workDir <- getwd()


filePath = paste0(workDir,"/Data")
g_max <- 1048576

train_Data<- read_tsv("https://raw.githubusercontent.com/Rajwantmishra/msds/master/607/Project5/607Project/data/drugLibTrain_raw.tsv")

test_Data<- read_tsv("https://raw.githubusercontent.com/Rajwantmishra/msds/master/607/Project5/607Project/data/drugLibTest_raw.tsv")

glimpse(train_Data)


```

# Working with One Row 
```{r}
NROW(unique(train_Data$X1)) # indicate X1 is unique 

t(train_Data[1,])

train_Data$condition[1]
train_Data$sideEffects[1]
train_Data$sideEffectsReview[1]
train_Data$commentsReview[1]

#
#https://www.innerbody.com/diseases-conditions
# https://kidspicturedictionary.com/english-through-pictures/people-english-through-pictures/human-body/
```

```{r eval=FALSE, include=FALSE}
# Parsing of HTML/XML files  
library(rvest)    
# String manipulation
library(stringr)   
# Verbose regular expressions
#install.packages("rebus")
library(rebus)     
# Eases DateTime manipulation
library(lubridate)

dcURL <- "https://www.innerbody.com/diseases-conditions"

dcList <- read_html(dcURL)

html_nodes(urlB,"div.profile_section p") %>% html_text() 


# conditonURL <- dcList %>%
#         html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href") %>% 
#   
  conditionURL <- dcList %>%
        html_nodes(".flex-list-2 li a") %>%  html_attr("href") %>% paste0("https://www.innerbody.com",.) %>% unlist()

  conditionBuffer <- data.frame(Dt.url= rep(NA,100),
                                Dt.overview = rep(NA,100),
                                Dt.risknCause = rep(NA,100),
                                Dt.symptoms = rep(NA,100),
                                Dt.dnt = rep(NA,100),
                                Dt.prevention = rep(NA,100),
                                Dt.sources = rep(NA,100))
  
  
  
  # Read sub page
  for ( i in 23: length(conditionURL)) {
   url2 <- paste0(conditionURL[i])
   # url2 <- "https://www.innerbody.com/diseases-conditions/scabies"
      print(url2)
      Sys.sleep(20)
      tdata <- read_html(url2,options = "HUGE")
    
     # Overview
     write_html(tdata,paste0("condition/",str_extract(url2,"[[:alpha:]-?_?[:alpha:]]+$"),".html"))
     
     # Causes and Risk Factors
     # h2#causes-and-risk-factors, p:nth-of-type(5), ul:nth-of-type(2) li
     
    
     
     html_nodes(tdata,"h2#symptoms, p:nth-of-type(6), ul:nth-of-type(3) li")  %>%  html_text()  %>%  str_trim() %>%    unlist() %>% print()
     
     readh2 <- html_nodes(tdata,"section h2") %>%  html_text() %>%unlist() %>% trimws()
     html_nodes(tdata,"section h2") %>%  html_attrs()
     buffer<- html_nodes(tdata,"section h2, p,ul,picture") %>%  html_text() %>% trimws() %>% unlist()
     
     #Build Serach Query 
     # [1] "Overview"                       
     #  [2] "Types and Causes of Stroke"     
     #  [3] "Risk Factors for Stroke"        
     #  [4] "Symptoms"                       
     #  [5] "Diagnosis"                      
     #  [6] "Treatment of Ischemic Stroke"   
     #  [7] "Treatment of Hemorrhagic Stroke"
     #  [8] "Rehabilitation and Prevention"  
     #  [9] "Sources"   
     findoverview <- readh2[str_which(readh2,"Overview")]
     findCauses <- readh2[str_which(readh2,"Causes")]
     findSymptoms <- readh2[str_which(readh2,"Symptoms")]
     findDiagnosis <- readh2[str_which(readh2,"Diagnosis")]
     findPrevention <- readh2[str_which(readh2,"Prevention")]
     findSources <- readh2[str_which(readh2,"Sources")]
     
     ovStart   <- str_which(buffer,paste0('^',findoverview,'$'))
     riskStart <- str_which(buffer,paste0('^',findCauses,'$'))
     sympStart <- str_which(buffer,paste0('^',findSymptoms,'$'))
     dntStart  <- str_which(buffer,paste0('^',findDiagnosis,'$'))
     prevStart <- str_which(buffer,paste0('^',findPrevention,'$'))
     srcStart  <- str_which(buffer,paste0('^',findSources,'$'))
     srcEnd    <- srcStart + 1
     # loop at buffer to read the belwo info
     #set URl 
      conditionBuffer$Dt.url[i] <- url2
               # Working Code repalced with more generic 
               # ovStart <- str_which(buffer,"^Overview$")
               # riskStart <- str_which(buffer,"^(Causes and Risk Factors)$|^Causes$|^(Types and Causes of Stroke)$")
               # riskStart <-  str_which(buffer,"^Symptoms$|^(Hemophilia Symptoms)$|^(Symptoms and Complications)$")
               # sympStart <- ifelse(is_empty(sympStart), str_which(buffer,"^(Symptoms of SCC)$") ,sympStart)
               # 
               # dntStart <- str_which(buffer,"^(Diagnosis and Treatment)$") 
               # dntStart <- ifelse(is_empty(dntStart), str_which(buffer,"^(Diagnosis)$") ,dntStart)
               # 
               # prevStart <-  str_which(buffer,"^Prevention$|^(Treatment and Prevention)$|^(Rehabilitation and Prevention)$")
               # srcStart <-  str_which(buffer,"^Sources$")
               # srcEnd <-  srcStart + 1
      #Overview 
     startFrom <- ovStart+1
     stopOn <- riskStart-1
     conditionBuffer$Dt.overview[i] <- buffer[startFrom:stopOn] %>% trimws() %>% unlist() %>% paste( collapse = " | ")
     
     
     # Causes and Risk Factors
     startFrom <- riskStart+1
     stopOn <- sympStart-1
     conditionBuffer$Dt.risknCause[i] <- buffer[startFrom:stopOn] %>% trimws() %>% unlist() %>% paste( collapse = " | ")
     
     
      # Symptoms
     startFrom <- sympStart+1
     stopOn <- dntStart-1
     conditionBuffer$Dt.symptoms[i] <- buffer[startFrom:stopOn] %>% trimws() %>% unlist() %>% paste( collapse = " | ")
     
     # Diagnosis and Treatment
     # h2#diagnosis-and-treatment, p:nth-of-type(n+7), ul:nth-of-type(4) li
     startFrom <- dntStart+1
     stopOn <- prevStart-1
     conditionBuffer$Dt.dnt[i] <- buffer[startFrom:stopOn] %>% trimws() %>% unlist() %>% paste( collapse = " | ")
     
     #Prevention
     startFrom <- prevStart+1
     stopOn <- srcStart-1
     conditionBuffer$Dt.prevention[i] <- buffer[startFrom:stopOn] %>% trimws() %>% unlist() %>% paste( collapse = " | ")
     
     # Sources
     startFrom <- srcStart+1
     stopOn <- srcStart+1
     conditionBuffer$Dt.sources[i] <- buffer[startFrom:stopOn] %>% trimws() %>% unlist() %>% paste( collapse = " | ")
    
     
     
  # 
  # td<- html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist() 
  # td<- tibble(data= td)
  # 
  # # Tweeter id:
  # # tid  <- str_extract(td[1],"@[[a-zA-Z]]+")
  #  td$DateTime<- str_extract(td$data,"@[\\w]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
  #    # td$Tweets <- str_split(td$data,"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
  #  
     
 
  }
  
  View(conditionBuffer)
  
  
  write_tsv(conditionBuffer,paste0("data/condition.tsv"))
 
  
```

# GEt Body Parts

```{r eval=FALSE, include=FALSE}

bodypartUrl  <- "https://kidspicturedictionary.com/english-through-pictures/people-english-through-pictures/human-body/"


bodypartweb <- read_html(bodypartUrl)

#save Local COpy 
write_html(bodypartweb,paste0("condition/body_part_web.html"))
# Main Section
body1 <- html_nodes(bodypartweb,"#ez-toc-container nav ul.ez-toc-list li a") %>% html_text()

# Sub section 
html_nodes(bodypartweb,"#ez-toc-container nav ul.ez-toc-list li a") %>% html_text()

# Part 
bodyData  <- html_nodes(bodypartweb,".entry-content p,h1,h2,h3,h4") %>% html_text()

# bodyData <- html_nodes(bodypartweb,".entry-content p") %>% html_text() %>% trimws()

masterBody <- c()

masterBody <- data.frame(Dt.group= rep(NA,100),
                                Dt.bparts = rep(NA,100))
                            
groupName <- ""
for (i in 1: 98){
  # %>% str_extract("[[:alpha:]+ ?[:alpha:]+]+")
  groupBody <- body1[str_detect(body1,paste0('^',bodyData[i],'$'))]
  groupName <- ifelse(!is_empty(groupBody),groupBody,groupName) 
  masterBody$Dt.group[i] <- ifelse(is_empty(groupBody),groupName,groupBody)
  # c(paste(bodyData[i] , collapse = ","))
  masterBody <- c(masterBody,grouprBody)
  # masterBody <- c(masterBody,c(paste(bodyData[i] , collapse = ",")))
  masterBody$Dt.bparts[i] <- unlist(bodyData[i])
}

masterBody <- as.data.frame(masterBody)




# conditonURL <- dcList %>%
#         html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href") %>% 
#   
 # conditionURL <- dcList %>%
 #       html_nodes(".flex-list-2 li a") %>%  html_attr("href") %>% paste0("https://www.innerbody.com",.) %>% unlist()


```

```{r eval=FALSE, include=FALSE}


masterBodyB <- data.frame(group = masterBody$Dt.group,part = masterBody$Dt.bparts)
masterBodyB$part <- str_replace_all(masterBodyB$part,"\n"," ")
masterBodyB$part <- str_replace_all(masterBodyB$part,"/ ","/")



masterBodyB$part[6]
patternBody <- "[[:alpha:]-?//? ?[:alpha:]]+"
masterBodyB$part[6]
str_extract_all(masterBodyB$part[6],patternBody,simplify = TRUE)
str_extract_all(masterBodyB$part[9],patternBody,simplify = TRUE)
masterBodyB$part[16]
str_extract_all(masterBodyB$part[9],patternBody, simplify = TRUE)
str_extract_all(masterBodyB$part[9],patternBody)%>% unlist()

data.frame("group", str_extract_all(masterBodyB$part[9],patternBody)%>% unlist())

bodyPartMaster <- data.frame(bodyGroup = c(NA,NA) , bodyPart = c(NA,NA))

for (i in 1: length(masterBodyB$part)){
  bodyPartTemp <- str_extract_all(masterBodyB$part[i],patternBody)
  bodyGroupTemp <- masterBodyB$group[i]
  if(!is_empty(bodyPartTemp[[1]])){
    tempdata <- data.frame(bodyGroupTemp,bodyPartTemp) 
    names(tempdata) = c("bodyGroup","bodyPart")
   
    bodyPartMaster <- bind_rows(bodyPartMaster,tempdata)
  }
  
}

bodyPartMasterb <- bodyPartMaster

remData <- which(is.na(bodyPartMaster))
bodyPartMaster <- bodyPartMaster[-c(remData),]
remData <- which((bodyPartMaster$bodyGroup==""|bodyPartMaster$bodyPart==" "))
bodyPartMaster <- bodyPartMaster[-c(remData),]
bodyPartMaster <- bodyPartMaster[-c(1:3),]

# write.csv(bodyPartMaster,"data/masterbody.csv")
writdata/condition.tsv"))

```


# Using SVM suport vector meothd . 

Since data points were too high it was not possible to run the model with full data on the working machine. Result of this Methods were not accurate . All the valuses were calssifed as one.

```{r}

library(e1071)
t(train_Data[2,])
str_detect(train_Data$condition,"hirschsprungs")
t(conditionBuffer[1,])

str_detect(conditionBuffer$Dt.risknCause,"birth control")
t(conditionBuffer[21,])

#---------------------------------------------------SVm Method 
train_Data$urlDrugName <- as.factor(train_Data$urlDrugName)
train_Data$effectiveness <- as.factor(train_Data$effectiveness)
summary(train_Data)
unique(train_Data$X1)
fitdefaultRadial <- e1071::svm(urlDrugName~condition + effectiveness  ,train_Data[which(train_Data$urlDrugName %in% c("biaxin","lamictal","depakene","sarafem")),])
fitdefaultRadial
summary(fitdefaultRadial)

fitdefaultpolynomial <- e1071::svm(urlDrugName~condition + effectiveness  ,data= train_Data[1:5,] , kernel= "polynomial")
fitdefaultpolynomial
summary(fitdefaultpolynomial)


fitdefaultsigmoid <- e1071::svm(urlDrugName~condition + effectiveness  ,data= train_Data[1:5,] , kernel= "sigmoid")
fitdefaultsigmoid
summary(fitdefaultsigmoid)



#----------------------------------------------------------------
# Predict 
  predictdefaultRad <- predict(fitdefaultRadial,train_Data[which(train_Data$urlDrugName %in% c("biaxin","lamictal","depakene","sarafem")),])
  predictdefaultPloy <- predict(fitdefaultpolynomial,train_Data[1:5,])
  predictdefaultSig <- predict(fitdefaultsigmoid,train_Data[1:5,])
  
 CrossTable(predictdefaultRad,train_Data[which(train_Data$urlDrugName %in% c("biaxin","lamictal","depakene","sarafem")),]$urlDrugName,prop.chisq = TRUE, prop.t = FALSE, dnn = c("Predicted","Actual"))
  

```



## TM Function 
```{r}

(funReplace <- content_transformer(function(x, pattern, rep="") str_replace(x,pattern,rep) ))
# (getPattern <- content_transformer(  function(x,pattern)( str_extract_all(x, '<(.+?)>.+?</\\1>'))))
(getPattern <- content_transformer(  function(x,pattern)( str_extract_all(x, pattern))))
# EMail
 regEmail <-"(?<=(From|Delivered-To:|To:){1} )(<?[[:alnum:]]+@[[:alnum:].?-?]+>?)"
 (getEmail<- content_transformer(function(x,pattern)(str_extract_all(x,regEmail)))) 
  # IP Address
  regIP <- "\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b"
 (getIP<- content_transformer(function(x,pattern)(str_extract_all(x,regIP)))) 
   # Email Type
  # regEmailType <- "(?<=(Content-Type:){1} )([[:graph:]]+)"
  regEmailType <- "(?<=(Content-Type:){1} )([[:alpha:]]+/[[:alpha:]]+)"
(getEmailType <- content_transformer(function(x,pattern)(str_extract_all(x,regEmailType)))) 
   #Subject
  # Subject 
regSub <- "(?<=(Subject:){1} )([[:graph:] [:graph:]]+)"

(getSubject <- content_transformer(function(x,pattern)(str_extract_all(x,regSub))))
(getBody<- content_transformer(function(x)(x[(str_which(x,regSub)):length(x)])))
(getBodyLen <- content_transformer(function(x)( str_count(x))))

# Remove HTML TAG from
(cleanHTML <- content_transformer (function(x) {
  return(gsub("<.*?>", "", x))
}))

```


```{r}
install.packages("quanteda")
library(quanteda)


masterBodyC <- data.frame(doc_id = masterBody$Dt.group,text = masterBody$Dt.bparts)
corpus_body <- Corpus(DataframeSource(masterBodyC[,]))


content(corpus_body[[1]])
head(summary(corpus_body))

tm_map(corpus_body,funReplace,"@")
inspect(corpus_body[[2]])

content(tm_map(corpus_body, getPattern,"[[:alpha:]]+"))
content(corpus_body)


```


```{r}
head(test_Data)
tm_dt_comment <- data.frame(doc_id = test_Data$urlDrugName,text = test_Data$commentsReview)
tm_dt_condition <- data.frame(doc_id = test_Data$urlDrugName,text = test_Data$condition)
corpus_commentsReview <- VCorpus(DataframeSource(tm_dt_comment))
  # PCorpus(DataframeSource(tm_dt_comment),dbControl=list(useDb = TRUE,dbName = "texts.db",dbType = "DB1"))


corpus_condition <-PCorpus(DataframeSource(tm_dt_condition))

print(corpus_commentsReview)
print(corpus_condition)

summary(corpus_commentsReview)
summary(corpus_condition)
inspect(corpus_commentsReview[2][[1]])
inspect(corpus_condition[1][[1]])

corpus_condition[1:5]

tm::getMeta(corpus_condition)
tm::getMeta(corpus_condition,Name) <= "Conditon"

corpus_condition
inspect(corpus_condition[1][[1]])
inspect(corpus_condition[2][[1]])
meta(corpus_condition[[1]])
meta(corpus_condition[[2]])
# meta(corpus_condition[[1]], "comment", type = c("indexed", "corpus", "local")) <- "SPAM"

# Clean Data
corpus_condition_clean <- tm_map(corpus_condition,tolower)
corpus_condition_clean <- tm_map(corpus_condition,removePunctuation)
corpus_condition_clean <- tm_map(corpus_condition,removeNumbers)
corpus_condition_clean <- tm_map(corpus_condition,stripWhitespace)
corpus_condition_clean <- tm_map(corpus_condition,removeWords,stopwords())
inspect(corpus_condition[1:4])[[1]]

corpus_condition_clean_dtm <- DocumentTermMatrix(corpus_condition_clean)
                            # DocumentTermMatrix(corpus_condition_clean, control = list(removePunctuation = TRUE,
                            #                                          stopwords = TRUE))

inspect(corpus_condition_clean_dtm[11:15,])

# Organize terms by their frequency:

corpus_condition_clean_dtm_freq <- colSums(as.matrix(corpus_condition_clean_dtm))  
length(corpus_condition_clean_dtm_freq) 

# write.csv(as.matrix(corpus_condition_clean_dtm) , file="corpus_condition_clean_dtm.csv") 

library(wordcloud)
wordcloud(corpus_condition_clean,min.freq = 10, color= TRUE)

```


#### Create Test 
```{r}

# Creat Test and Train data 
train_back <- train_Data[which(train_Data$urlDrugName %in% c("biaxin","lamictal","depakene","sarafem")),]
test_back <- test_Data[which(test_Data$urlDrugName %in% c("biaxin","lamictal","depakene","sarafem")),]

masterBodyC
conditionBuffer

conditionBuffer[str_which(conditionBuffer$Dt.overview,"depression"),]


test_Data[str_which(test$condition,"obesity"),]


```

### Loading symtoms Diseases Condition 
```{r}
# bodypart <-(bodyPartMaster,"data/masterbody.csv")
Diseases_condition <-  read_tsv("data/condition.tsv")

# Creating TM document for Diseases 
Diseases_condition$Diseases <- str_extract(Diseases_condition$Dt.url,"[[:alpha:]-?_?[:alpha:]]+$")  %>% str_replace_all("-"," ")
Diseases_condition$Diseases <- factor(Diseases_condition$Diseases)
DT::datatable(Diseases_condition[,c(8,2)])
dt_corpus_condtion <- data.frame(doc_id = Diseases_condition$Diseases,text = Diseases_condition$Dt.overview)

corpus_Diseases_condition <- VCorpus(DataframeSource(dt_corpus_condtion))

corpus_Diseases_condition_dtm <- DocumentTermMatrix(corpus_Diseases_condition, control = 
                                                    list(removePunctuation = TRUE,
                                                         tolower = TRUE,
                                                         stripWhitespace= TRUE,
                                                         removeNumbers = TRUE,
                                                         stopwords = TRUE))


wordcloud(findFreqTerms(corpus_Diseases_condition_dtm,25),scale = c(1:5,.2))
fct_count(findFreqTerms(corpus_Diseases_condition_dtm,1,10),sort=TRUE)

tm::findFreqTerms(corpus_Diseases_condition_dtm,1,10)
removeSparseTerms(corpus_Diseases_condition_dtm,.90)

#----------
freq_condition <- findFreqTerms(corpus_Diseases_condition_dtm,1)
head(freq_condition)
corpus_Diseases_condition_dtm_freqcond <- DocumentTermMatrix(corpus_condition, control=list(dictionary = freq_condition,
                                                                        removePunctuation = TRUE,
                                                         tolower = TRUE,
                                                         stripWhitespace= TRUE,
                                                         removeNumbers = TRUE,
                                                         stopwords = TRUE))

dim(corpus_Diseases_condition_dtm_freqcond)
dim(corpus_Diseases_condition_dtm)
# 
# dtm.test.nb <- DocumentTermMatrix(corpus_condition_test_user, control=list(dictionary = freq_words,
#                                                                         removePunctuation = TRUE,
#                                                          tolower = TRUE,
#                                                          stripWhitespace= TRUE,
#                                                          removeNumbers = TRUE,
#                                                          stopwords = TRUE))
# dim(dtm.test.nb)

# Function to convert the word frequencies to yes (presence) and no (absence) labels
convert_count <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}

# Apply the convert_count function to get final training and testing DTMs
train_Condition <- apply(corpus_Diseases_condition_dtm, 2, convert_count)


nav_model_condition <- naiveBayes(train_Condition, Diseases_condition$Diseases, laplace = 1) 
nav_model_condition_test <- predict(nav_model_condition,train_Condition[1:5,])
# table(nav_model_condition_test,Diseases_condition[1:5,]$Diseases)

CrossTable(nav_model_condition_test,Diseases_condition[1:5,]$Diseases,prop.chisq = FALSE, prop.t = FALSE, dnn = c("Predicted","Actual"))

```


### Using Term matrix and naiveBayes algorithm 

```{r}

#-------------------------------------------
#--------------------------------TEST With 4 drug 
#-------------------------------------------
# Creat Test and Train data 
train <- train_Data[which(train_Data$urlDrugName %in% c("biaxin","lamictal","depakene","sarafem")),]
test <- test_Data[which(test_Data$urlDrugName %in% c("biaxin","lamictal","depakene","sarafem")),]



# Chack Data 
head(train)
head(test)
train$urlDrugName <- train$urlDrugName
test$urlDrugName <- test$urlDrugName
# prop.table(table(train$urlDrugName))
forcats::fct_infreq(train$urlDrugName, ordered = TRUE)
head(forcats::fct_count(train$urlDrugName, sort = TRUE))
(forcats::fct_count(test$urlDrugName, sort = TRUE))

# reate Datframe good for TERM MATRIX 
## TRAIN...........................................
tm_dt_comment <- data.frame(doc_id = train$urlDrugName,text = train$commentsReview)
tm_dt_condition <- data.frame(doc_id = train$urlDrugName,text = train$condition)
## Test...........................................
tm_dte_comment <- data.frame(doc_id = test$urlDrugName,text = test$commentsReview)
tm_dte_condition <- data.frame(doc_id = test$urlDrugName,text = test$condition)
## ..........................................
#Create VCorpus 
## TRAIN...........................................
corpus_commentsReview <- VCorpus(DataframeSource(tm_dt_comment))
  # PCorpus(DataframeSource(tm_dt_comment),dbControl=list(useDb = TRUE,dbName = "texts.db",dbType = "DB1"))
corpus_condition <-VCorpus(DataframeSource(tm_dt_condition))
## TRAIN...........................................
corpus_commentsReview_test <- VCorpus(DataframeSource(tm_dte_comment))
corpus_condition_test <-VCorpus(DataframeSource(tm_dte_condition))
## ..........................................

#Cheack Corpus 
print(corpus_commentsReview)
summary(corpus_commentsReview)
print(corpus_condition)
summary(corpus_condition)
meta(corpus_condition[[1]])
# Inspect cosrpus
inspect(corpus_commentsReview[2][[1]])
inspect(corpus_condition[1][[1]])



inspect(corpus_condition[1][[1]])
inspect(corpus_condition[2][[1]])
meta(corpus_condition[[1]])
meta(corpus_condition[[2]])
# meta(corpus_condition[[1]], "comment", type = c("indexed", "corpus", "local")) <- "TEST "


# Clean Data
# corpus_condition_clean <- tm_map(corpus_condition,tolower)
# corpus_condition_clean <- tm_map(corpus_condition,removePunctuation)
# corpus_condition_clean <- tm_map(corpus_condition,removeNumbers)
# corpus_condition_clean <- tm_map(corpus_condition,stripWhitespace)
# corpus_condition_clean <- tm_map(corpus_condition,removeWords,stopwords())

# Create Document Term Matrix 
## TRAIN...........................................
corpus_condition_clean_dtm <-  DocumentTermMatrix(corpus_condition, control = 
                                                    list(removePunctuation = TRUE,
                                                         tolower = TRUE,
                                                         stripWhitespace= TRUE,
                                                         removeNumbers = TRUE,
                                                         stopwords = TRUE))

corpus_commentsReview_clean_dtm <-  DocumentTermMatrix(corpus_commentsReview, control = 
                                                    list(removePunctuation = TRUE,
                                                         tolower = TRUE,
                                                         stripWhitespace= TRUE,
                                                         removeNumbers = TRUE,
                                                         stopwords = TRUE))

## Test...........................................

corpus_condition_test_dtm <-  DocumentTermMatrix(corpus_condition_test, control = 
                                                    list(removePunctuation = TRUE,
                                                         tolower = TRUE,
                                                         stripWhitespace= TRUE,
                                                         removeNumbers = TRUE,
                                                         stopwords = TRUE))

corpus_commentsReview_test_dtm <-  DocumentTermMatrix(corpus_commentsReview_test, control = 
                                                    list(removePunctuation = TRUE,
                                                         tolower = TRUE,
                                                         stripWhitespace= TRUE,
                                                         removeNumbers = TRUE,
                                                         stopwords = TRUE))


## .....................................................................................

#condition Coprpus DTM
corpus_condition_clean_dtm
#Comments Corpus DTM
corpus_commentsReview_clean_dtm
inspect(corpus_condition_clean_dtm[1:2,])
library(wordcloud)
# wordcloud 
suppressWarnings(wordcloud(corpus_condition,  min.freq=20, scale=c(5, .1), colors=brewer.pal(6, "Dark2")) ) 
# suppressWarnings(wordcloud(corpus_condition_test,  min.freq=2, scale=c(3, .5), colors=brewer.pal(6, "Dark2")) )
suppressWarnings(wordcloud(corpus_condition_test,  min.freq=10,scale=c(5, .1), colors=brewer.pal(6, "Dark2")) ) 

# Organize terms by their frequency:

corpus_condition_clean_dtm_freq <- colSums(as.matrix(corpus_condition_clean_dtm))  
length(corpus_condition_clean_dtm_freq) 

# Find Frequent term 
dim(corpus_condition_clean_dtm[])
inspect(corpus_condition_clean_dtm[1,])


# We could create datapoint with most frequest terms used and then apply to the model for taining.. 
# In this case Iwould be usingi n my data as it is . as its not a big data set .
# but to avoid missing vlaue in test dat aset wwe would create a vector of most frequest words and drop every thing that is not in the vacotr. # from the test data for testin purpose to avoaid error at test time.
 corpus_condition_clean_dtm_freq_term  <- findMostFreqTerms(corpus_condition_clean_dtm,20)
 length(corpus_condition_clean_dtm_freq_term[])
 corpus_condition_clean_dtm_freq_term[1:3]
 
 # Organize terms by their frequency:

corpus_condition_clean_dtm_freq <- colSums(as.matrix(corpus_condition_clean_dtm))  
length(corpus_condition_clean_dtm_freq) 

corpus_condition_test_dtm_freq <- colSums(as.matrix(corpus_condition_test_dtm))  
length(corpus_condition_test_dtm_freq) 
 
# $ Matrix to data frame
reshape2::melt(sort(corpus_condition_clean_dtm_freq,decreasing = TRUE)[1:20])
data.frame(word=names(corpus_condition_clean_dtm_freq), freq=corpus_condition_clean_dtm_freq)  

dim(corpus_condition_clean_dtm)
# Find terms with frequecy at least 5
freq_words <- findFreqTerms(corpus_condition_clean_dtm,5)
freq_words


 # corpus_condition_test_dtm_freqword <- 
# https://www.youtube.com/watch?v=sujx3MjEH_0
    corpus_condition_clean_dtm_freqword <-  corpus_condition_clean_dtm[,freq_words]
    corpus_condition_test_dtm_freqword  <-   corpus_condition_test_dtm[,freq_words]
  
     
    options(error=recover)
    
      
 # corpus_condition_test_dtm_Testready <- corpus_condition_test[[1]]
    
    freq_y_n <- function(x){
      y <- ifelse(x>0,1,0)
      y <- factor(y, level=c(0,1),labels = c("No","Yes"))
      y
    }

    new_train <- apply(corpus_condition_clean_dtm_freqword,2,freq_y_n)
    new_test <- apply(corpus_condition_test_dtm_freqword,2,freq_y_n)
    
    new_test2 <- apply(corpus_condition_test_dtm_freqword,2,freq_y_n) 
    corpus_condition_test_dtm_freqword[1:4,]
    
  #-- Start usng librarat e1071
    
    train_classifier <- naiveBayes(new_train,train$urlDrugName)
    
    class(train_classifier)
  # Test 
    test_predict <- predict(train_classifier, new_test)
    
    table (test_predict,test$urlDrugName)
    
 forcats::fct_count(test$urlDrugName)
 forcats::fct_count(test_predict,sort = TRUE)
 
 
 library(gmodels)
 CrossTable(test_predict,test$urlDrugName,prop.chisq = TRUE, prop.t = FALSE, dnn = c("Predicted","Actual"))

 #--------------------------------------------------------------------------------------
 #----------------------------------User Input test ------------------------------------
 #--------------------------------------------------------------------------------------
 
 user_question <- data.frame(doc_id = c("NoDRUG", "NoNEWDRUG",""),text = 
                                       c("I am in depression",
                                         "I have sinus",
                                         "I have throt infection"))
 
 usr_ask(user_question)
 
 #----------Test REgualr 
 user_question2 <- test[,c(1,6)]
 names(user_question2) <- c("doc_id","text")
res <- usr_ask(user_question2)

CrossTable(res,test$urlDrugName,prop.chisq = TRUE, prop.t = FALSE, dnn = c("Predicted","Actual")) 
 
  #--------------------------------------------------------------------------------------
 #----------------------------------User Input Functio ---------------------------------
 #--------------------------------------------------------------------------------------
 usr_ask <- name <- function(dtquestion ) {
   

 
 # tm_dte_condition <- data.frame(doc_id = test$urlDrugName,text = test$condition)
 # tm_dte_condition_User <- data.frame(doc_id = c("NoDRUG", "NoNEWDRUG",""),text = 
 #                                       c("I am in depression",
 #                                         "I have sinus",
 #                                         "I have throt infection"))
   
 tm_dte_condition_User <- data.frame(doc_id = c(rep("noDrug",1,length(dtquestion[,1]))),
                                     text = dtquestion$text )
 
corpus_condition_test_user <-VCorpus(DataframeSource(tm_dte_condition_User))
 
 # Use only 5 most frequent words (fivefreq) to build the DTM
# Find terms with frequecy at least 5
freq_words_user <- findFreqTerms(corpus_condition_clean_dtm,1)
freq_words_user
dtm.train.nb <- DocumentTermMatrix(corpus_condition, control=list(dictionary = freq_words,
                                                                        removePunctuation = TRUE,
                                                         tolower = TRUE,
                                                         stripWhitespace= TRUE,
                                                         removeNumbers = TRUE,
                                                         stopwords = TRUE))

dim(dtm.train.nb)

dtm.test.nb <- DocumentTermMatrix(corpus_condition_test_user, control=list(dictionary = freq_words,
                                                                        removePunctuation = TRUE,
                                                         tolower = TRUE,
                                                         stripWhitespace= TRUE,
                                                         removeNumbers = TRUE,
                                                         stopwords = TRUE))
dim(dtm.test.nb)

# Function to convert the word frequencies to yes (presence) and no (absence) labels
convert_count <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}

# Apply the convert_count function to get final training and testing DTMs
trainNB <- apply(dtm.train.nb, 2, convert_count)
testNB <- apply(dtm.test.nb, 2, convert_count)

nav_lap <- naiveBayes(trainNB, train$urlDrugName, laplace = 1) 
red_lap <- predict(nav_lap,testNB)



fct_count(red_lap,sort = TRUE)
 CrossTable(tm_dte_condition_User$text,red_lap,prop.chisq = FALSE, prop.t = FALSE, dnn = c("Problem","Drug"))
 red_lap
 }
 
 usr_ask(user_question2)
table(red_lap,tra)
 # CrossTable(red_lap,test$urlDrugName,prop.chisq = TRUE, prop.t = FALSE, dnn = c("Predicted","Actual")) 
new_test
```



```{r}
#--------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------

 

 # ------------- Using laplace 
 train_classifier_lp <- naiveBayes(new_train,train$urlDrugName,laplace = 1)
    
    class(train_classifier)
  # Test 
    test_predict_lp <- predict(train_classifier_lp, new_test)
    
    table (test_predict,test$urlDrugName)
    
 forcats::fct_count(test$urlDrugName)
 forcats::fct_count(test_predict,sort = TRUE)
 
 
 CrossTable(test_predict,test$urlDrugName,prop.chisq = TRUE, prop.t = FALSE, dnn = c("Predicted","Actual"))
 CrossTable(test_predict_lp,test$urlDrugName,prop.chisq = TRUE, prop.t = FALSE, dnn = c("Predicted","Actual"))
 CrossTable(red_lap,test$urlDrugName,prop.chisq = TRUE, prop.t = FALSE, dnn = c("Predicted","Actual")) 
 red_lap
    
```

#### Cosine distance 
```{r}
# install.packages("lsa")
library(lsa)
cos_Train <- cbind(drug=train$urlDrugName,as.data.frame(as.matrix(corpus_condition_clean_dtm)))
 
cos_Test  <- cbind(drug=test$urlDrugName,as.data.frame(as.matrix(corpus_condition_test_dtm)))
t(cos_Train)[,1]
# cos_train.similarites <- cosine( as.matrix(t(cos_Train)[,1:2]))


dim(new_train)

# cosine( new_train[,])


    freq_y_n0 <- function(x){
      y <- ifelse(x=="0",0,1)
      y
    }

# apply(corpus_condition_clean_dtm_freqword,2,freq_y_n0)
    #Cosine distance by conditon
    cosine_train <- apply(as.matrix((cos_Train)),2,freq_y_n0)
vec1 = as.matrix(t(cos_Train)[,])
vec2 = as.matrix(t(cos_Train)[,2])
cosine_train_cond_result <- cosine(cosine_train[,-1]) 
hist(cosine_train_cond_result)
cosine_train_cond_result[,1:2]


dim(cosine_train)
dim(cos_Train)

t(cos_Train)

#Cosine distance by Drug 


 cosine_train_drug <- apply(as.matrix(t(cos_Train)),2,freq_y_n0)
cosine_train_drug_result <- cosine(t(cosine_train_drug[-1,])) 

View(cosine_train_drug_result)

# Cos 0 = 1 and cos 90 = 0, so if we have too many value at 0 it means that they are not close to each other .. but if we have too many value close to 1 it means those vecotra are near . 
hist(cosine_train_drug_result)
hist(cosine_train_cond_result)

cos_train 
print("freq_words_user,----------------------------------------------------")
freq_words_user
print("cos_Traindrug sarafem,----------------------------------------------------")

names(cos_Train[which(cos_Train$drug=="sarafem"),])
print("cos_Testlamictal----------------------------------------------------")
names(cos_Test[which(cos_Test$drug=="lamictal"),])
print("which(cos_Test$drug==sarafem----------------------------------------------------")
names(cos_Test[which(cos_Test$drug=="sarafem"),])



CrossTable(test_predict_lp,test$urlDrugName,prop.chisq = TRUE, prop.t = FALSE, dnn = c("Predicted","Actual"))

```


#### SOME PLOT 

```{r}
#------------------------------------------Some Plot
wordcloud(corpus_condition)
# construct the feature co-occurrence matrix
examplefcm <-
tokens(data_corpus_irishbudget2010, remove_punct = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english"), padding = FALSE) %>%
fcm(context = "window", window = 5, tri = FALSE)
# choose 30 most frequency features
topfeats <- names(topfeatures(examplefcm, 30))
# select the top 30 features only, plot the network
set.seed(100)
textplot_network(fcm_select(examplefcm, topfeats), min_freq = 0.8)







```


### USER INPUT TEST 
 
```{r} 
#-----------------------------------------------------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------------------------------------------------
#-----------------------------------------------------------------------------------------------------------------------------
train$condition

predict(train_classifier, new_test[])
 predict(train_classifier,"bipolar disorder and depression")
 
 
 #Function to parese input text for testing 
 process_test <- function(fdata,freqwords){
   # test <- data.frame(urlDrugName =c("NOT","not") , condition = c( "I am in depression"," have sinus infection"))
   
    # fdata <- c( "I am in depression"," have sinus infection")
    # freqwords <-  freq_words
#    test <- test_Data[which(test_Data$urlDrugName %in% c("biaxin","lamictal","depakene","sarafem")),]
# test$urlDrugName <- test$urlDrugName

# Create Datframe good for TERM MATRIX 

## Test...........................................
# tm_dte_comment <- data.frame(doc_id = test$urlDrugName,text = test$commentsReview)
# ftm_dte_condition <- data.frame(doc_id = test$urlDrugName,text = test$condition)

ftm_dte_condition <- data.frame(doc_id = "biaxin",text = data.frame(fdata)$fdata)

# Cretest Corpus VCorpus(DataframeSource(ftm_dte_comment))
fcorpus_condition_test <-VCorpus(DataframeSource(ftm_dte_condition))

# Create Document Term Matrix 

fcorpus_condition_test_dtm <-  DocumentTermMatrix(fcorpus_condition_test, control = 
                                                    list(removePunctuation = TRUE,
                                                         tolower = TRUE,
                                                         stripWhitespace= TRUE,
                                                         removeNumbers = TRUE,
                                                         stopwords = TRUE))


# Organize terms by their frequency:

 fcorpus_condition_test_dtm
  fright<- gather(as.data.frame(as.matrix(fcorpus_condition_test_dtm)))
 
fleft <- as.data.frame(freqwords)
names(fleft) <- "key"
fleft$key = as.character(fleft$key)
an.error.occured <- FALSE
tryCatch( { fresult <- dplyr::left_join(fleft,fright, by= c("key" = "key")) }
          , error = function(e) {an.error.occured <<- TRUE})


print(an.error.occured)

fresult <- fresult[which(!is.na(fresult$value)),]

# if not matching word found as per model , inform user
 an.error.occured <- ifelse(length(fresult$key)==0 , TRUE , FALSE)

if(an.error.occured == FALSE){
newFreqWord<- levels(as.factor(fresult$key))

# as.matrix( fcorpus_condition_test_dtm[,"depresion"])
 fcorpus_condition_test_dtm_freqword  <-   as.matrix(fcorpus_condition_test_dtm[,newFreqWord])
fnew_test = ''

fnew_test <- as.data.frame(apply(fcorpus_condition_test_dtm_freqword,2,freq_y_n))
# mt<- as.matrix(t(fnew_test))
# names(mt) <- c("d","t")
# fnew_test <- t(fnew_test)
# names() <- c("doc","terms")
# 
# t(fnew_test) %>% c("NotKnown",.$1)

}

if(an.error.occured == FALSE){
   fnew_test
}else{
     fnew_test <- "Sorry No Prediction possible at this time."
}
 fnew_test
 }
 
 input_text_predict = NA
 input_text_predict <- process_test(fdata = c( "I am in depression"," have sinus infection"),freq_words)
 input_text_predict <- as.matrix(input_text_predict )
 
 
 
 freq_y_n1 <- function(x){
      y <- ifelse(x=="0",0,1)
      y
 }
 
 as.matrix(input_text_predict)
 new_test[,1]
 lapply(input_text_predict,2, freq_y_n1)input_text_predict
 

 
  # fcorpus_condition_test_dtm_freqword  <-   fcorpus_condition_test_dtm[,newFreqWord]
  #    new_test <- apply(fcorpus_condition_test_dtm_freqword,2,freq_y_n)

predict(train_classifier,as.matrix(input_text_predict ))
 test_predict <- predict(train_classifier, new_test)
 predict(train_classifier,new_test)
    
    table (test_predict,test$urlDrugName)
    
 forcats::fct_count(test$urlDrugName)
 forcats::fct_count(test_predict,sort = TRUE)
 
 
 length(new_test)
 length(fnew_test)
 
  dim(new_test)
 dim(fnew_test)
 
 fcorpus_condition_test_dtm_freqword  <-   fcorpus_condition_test_dtm[,newFreqWord]
     fnew_test2<- apply(fcorpus_condition_test_dtm_freqword,2,freq_y_n)
     

```



```{r}
library(reshape2)
library(caret)
# test <- test_Data
 as.data.frame(melt(colSums(as.matrix(corpus_condition_clean_dtm))))
 train

caret_Train <- cbind(drug=train$urlDrugName,as.data.frame(as.matrix(corpus_condition_clean_dtm)))
 
caret_Test  <- cbind(drug=test$urlDrugName,as.data.frame(as.matrix(corpus_condition_test_dtm)))

caret_Train$drug <- factor(caret_Train$drug)
caret_Test$drug <- factor(caret_Test$drug)


caret_Train$drug
# the more text features and observations in the document-term matrix, the longer it takes to train the model. See time taken by N= 20 and n= 200 
system.time({
  estModel <- train(drug ~ .,
                     data = caret_Train,
                     method = "rf",
                     ntree = 200,
                     trControl = trainControl(method = "oob"),stringsasfactor = FALSE )
})

#
train[which(train$urlDrugName=="depakene"),]
# TEst 
## Reading Model variable 
estModel$finalModel
estModel


# TEST 

## TEsting with TEST DAT  
names(caret_Train)
names(caret_Test[,-1])
plsClasses <- predict((estModel))
str(plsClasses)
table(plsClasses)

#check
# 
# predEW <- with(new_test,
#   expand.grid(type=levels(type),word = as.character(terms))) 

train$condition
plsClasses <- predict(estModel,newdata = new_test)
estModel$levels
names(caret_Test)
names(caret_Train)



```
```{r}
# install.packages("gmodels")
# library(gmodels)
```

# Random Forest
```{r}
# install.packages("randomForest")
library(randomForest)


forest_Train <- cbind(drug=train$urlDrugName,as.data.frame(as.matrix(corpus_condition_clean_dtm)))
 forest_Train$drug <- factor(forest_Train$drug)
forest_Test  <- cbind(drug=test$urlDrugName,as.data.frame(as.matrix(corpus_condition_test_dtm)))
caret_Train$drug
fit <- randomForest(drug~.,data= caret_Train  )

 randomForest(drug~., data= forest_Train  )  
 
 
```

