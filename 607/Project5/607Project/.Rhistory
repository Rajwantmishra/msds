knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(DT)
library(stringr)
library(lubridate)
library(corrr)
library(psych)
library(readxl)
library(readr)
library(plotly)
library(lme4)
library(lmerTest)
# load data
workDir <- getwd()
filePath = paste0(workDir,"/Data")
g_max <- 1048576
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(DT)
library(stringr)
library(lubridate)
library(corrr)
library(psych)
library(readxl)
library(readr)
library(plotly)
library(lme4)
library(lmerTest)
# https://www.dataquest.io/blog/statistical-learning-for-predictive-modeling-r/
read_tsv("https://raw.githubusercontent.com/Rajwantmishra/msds/master/607/Project5/607Project/data/drugLibTrain_raw.tsv")
train_Data<- read_tsv("https://raw.githubusercontent.com/Rajwantmishra/msds/master/607/Project5/607Project/data/drugLibTrain_raw.tsv")
train_Data
describe(train_Data)
glimpse(train_Data)
library(tm)
train_Data[1]
train_Data[1,]
t(train_Data[1,])
unique(train_Data$X1)
count(unique(train_Data$X1))
ncol(unique(train_Data$X1))
nrow(unique(train_Data$X1))
NROW(unique(train_Data$X1))
train_Data$condition[1]
train_Data$condition[1]
train_Data$sideEffects[1]
train_Data$sideEffectsReview[1]
train_Data$commentsReview[1]
train_Data$condition[1]
train_Data$sideEffects[1]
train_Data$sideEffectsReview[1]
train_Data$commentsReview[1]
dcURL <- "https://www.innerbody.com/diseases-conditions"
dcList <- read_html("https://www.thinkers360.com/top-20-global-thought-leaders-on-analytics-july-2018/")
# Parsing of HTML/XML files
library(rvest)
# String manipulation
library(stringr)
# Verbose regular expressions
#install.packages("rebus")
library(rebus)
# Eases DateTime manipulation
library(lubridate)
dcURL <- "https://www.innerbody.com/diseases-conditions"
dcList <- read_html(dcURL)
tbls_ls <- urlA %>%
html_nodes("h2#cancer") %>%
.[1] %>%
html_table(fill = TRUE)
tbls_ls <- dcList %>%
html_nodes("h2#cancer") %>%
.[1] %>%
html_table(fill = TRUE)
tbls_ls
dcList %>%
html_nodes("h2#cancer") %>%
.[1]
dcList %>%
html_nodes("section")
dcList %>%
html_nodes("div.clearfix:nth-of-type(n+2)")
dcList %>%
html_nodes("#cancer p, .flex-list-2 li") %>%
.[1] %>%
html_table(fill = TRUE)
dcList %>%
html_nodes("#cancer p, .flex-list-2 li")
dcList %>%
html_nodes(".flex-list-2 li") %>%  html_attr("href")
dcList %>%
html_nodes("#cancer, .flex-list-2 li") %>%  html_attr("href")
dcList %>%
html_nodes("#cancer p, .flex-list-2 li") %>%  html_attr("href")
dcList %>%
html_nodes("#cancer p, .flex-list-2 li")
dcList %>%
html_nodes("#cancer p, .flex-list-2 li a")
dcList %>%
html_nodes("section, #cancer p, .flex-list-2 li a") %>%  html_attr("href")
dcList %>%
html_nodes("section, #cancer p, .flex-list-2 li a")
dcList %>%
html_nodes("section,  p, .flex-list-2 li a")
dcList %>%
html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href")
dcList %>%
html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href") %>%
html_table(fill = TRUE)
conditonURL <- dcList %>%
html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href") %>%
html_table(fill = TRUE)
conditonURL <- dcList %>%
html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href")
conditonURL
# conditonURL <- dcList %>%
#         html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href") %>%
#
conditonURL <- dcList %>%
html_nodes("section,.flex-list-2 li a") %>%  html_attr("href") %>%
)
# conditonURL <- dcList %>%
#         html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href") %>%
#
conditonURL <- dcList %>%
html_nodes("section,.flex-list-2 li a") %>%  html_attr("href")
conditonURL
# conditonURL <- dcList %>%
#         html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href") %>%
#
conditonURL <- dcList %>%
html_nodes(".flex-list-2 li a") %>%  html_attr("href")
conditonURL
# conditonURL <- dcList %>%
#         html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href") %>%
#
conditonURL <- dcList %>%
html_nodes(".flex-list-2 li a") %>%  html_attr("href") %>% paste0("hre",.)
conditonURL
# conditonURL <- dcList %>%
#         html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href") %>%
#
conditonURL <- dcList %>%
html_nodes(".flex-list-2 li a") %>%  html_attr("href") %>% paste0("https://www.innerbody.com/",.)
conditonURL
# conditonURL <- dcList %>%
#         html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href") %>%
#
conditonURL <- dcList %>%
html_nodes(".flex-list-2 li a") %>%  html_attr("href") %>% paste0("https://www.innerbody.com",.)
conditonURL
# conditonURL <- dcList %>%
#         html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href") %>%
#
conditionURL <- dcList %>%
html_nodes(".flex-list-2 li a") %>%  html_attr("href") %>% paste0("https://www.innerbody.com",.)
conditionURL
# conditonURL <- dcList %>%
#         html_nodes("section,  p, .flex-list-2 li a") %>%  html_attr("href") %>%
#
conditionURL <- dcList %>%
html_nodes(".flex-list-2 li a") %>%  html_attr("href") %>% paste0("https://www.innerbody.com",.) %>% unlist()
conditionURL
length(conditionURL)
conditionURL
# Read sub page
for ( i in 1: length(conditionURL)) {
url2 <- paste0(conditionURL[i])
print(url2)
#    Sys.sleep(10)
#    tdata <- read_html(url2,options = "HUGE")
#    html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist()
#
# td<- html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist()
# td<- tibble(data= td)
#
# # Tweeter id:
# # tid  <- str_extract(td[1],"@[[a-zA-Z]]+")
#  td$DateTime<- str_extract(td$data,"@[\\w]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#    # td$Tweets <- str_split(td$data,"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#
}
# Read sub page
for ( i in 1: length(conditionURL)-98) {
url2 <- paste0(conditionURL[i])
print(url2)
#    Sys.sleep(10)
#    tdata <- read_html(url2,options = "HUGE")
#    html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist()
#
# td<- html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist()
# td<- tibble(data= td)
#
# # Tweeter id:
# # tid  <- str_extract(td[1],"@[[a-zA-Z]]+")
#  td$DateTime<- str_extract(td$data,"@[\\w]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#    # td$Tweets <- str_split(td$data,"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#
}
# Read sub page
for ( i in 98: length(conditionURL)) {
url2 <- paste0(conditionURL[i])
print(url2)
#    Sys.sleep(10)
#    tdata <- read_html(url2,options = "HUGE")
#    html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist()
#
# td<- html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist()
# td<- tibble(data= td)
#
# # Tweeter id:
# # tid  <- str_extract(td[1],"@[[a-zA-Z]]+")
#  td$DateTime<- str_extract(td$data,"@[\\w]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#    # td$Tweets <- str_split(td$data,"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#
}
url2 <- paste0(conditionURL[i])
url2
#    Sys.sleep(10)
tdata <- read_html(url2,options = "HUGE")
html_nodes(tdata,"h2#symptoms")  %>%  html_text()  %>%  str_trim() %>%    unlist()
html_nodes(tdata,".symptoms")  %>%  html_text()  %>%  str_trim() %>%    unlist()
html_nodes(tdata,".symptoms p")  %>%  html_text()  %>%  str_trim() %>%    unlist()
html_nodes(tdata,"h2#symptoms, p:nth-of-type(6), ul:nth-of-type(3) li")  %>%  html_text()  %>%  str_trim() %>%    unlist()
html_nodes(tdata,"h2#symptoms, p:nth-of-type(6), ul:nth-of-type(3) li")  %>%  html_text()  %>%  str_trim() %>%    unlist()
# Read sub page
for ( i in 98: length(conditionURL)) {
url2 <- paste0(conditionURL[i])
print(url2)
#    Sys.sleep(10)
tdata <- read_html(url2,options = "HUGE")
# Overview
# Causes and Risk Factors
# h2#causes-and-risk-factors, p:nth-of-type(5), ul:nth-of-type(2) li
# Symptoms
html_nodes(tdata,"h2#symptoms, p:nth-of-type(6), ul:nth-of-type(3) li")  %>%  html_text()  %>%  str_trim() %>%    unlist()
# Diagnosis and Treatment
# h2#diagnosis-and-treatment, p:nth-of-type(n+7), ul:nth-of-type(4) li
#Prevention
# Sources
#
# td<- html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist()
# td<- tibble(data= td)
#
# # Tweeter id:
# # tid  <- str_extract(td[1],"@[[a-zA-Z]]+")
#  td$DateTime<- str_extract(td$data,"@[\\w]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#    # td$Tweets <- str_split(td$data,"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#
}
html_nodes(tdata,"h2#symptoms, p:nth-of-type(6), ul:nth-of-type(3) li")  %>%  html_text()  %>%  str_trim() %>%    unlist() %>% print()
# Read sub page
for ( i in 98: length(conditionURL)) {
url2 <- paste0(conditionURL[i])
print(url2)
#    Sys.sleep(10)
tdata <- read_html(url2,options = "HUGE")
# Overview
# Causes and Risk Factors
# h2#causes-and-risk-factors, p:nth-of-type(5), ul:nth-of-type(2) li
# Symptoms
html_nodes(tdata,"h2#symptoms, p:nth-of-type(6), ul:nth-of-type(3) li")  %>%  html_text()  %>%  str_trim() %>%    unlist() %>% print()
# Diagnosis and Treatment
# h2#diagnosis-and-treatment, p:nth-of-type(n+7), ul:nth-of-type(4) li
#Prevention
# Sources
#
# td<- html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist()
# td<- tibble(data= td)
#
# # Tweeter id:
# # tid  <- str_extract(td[1],"@[[a-zA-Z]]+")
#  td$DateTime<- str_extract(td$data,"@[\\w]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#    # td$Tweets <- str_split(td$data,"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#
}
html_nodes(tdata,"h2#symptoms, p:nth-of-type(6), ul:nth-of-type(3) li")  %>%  html_text()  %>%  str_trim() %>%    unlist() %>% print()
url2 <- "https://www.innerbody.com/diseases-conditions/scabies"
print(url2)
#    Sys.sleep(10)
tdata <- read_html(url2,options = "HUGE")
html_nodes(tdata,"h2#symptoms, p:nth-of-type(6), ul:nth-of-type(3) li")  %>%  html_text()  %>%  str_trim() %>%    unlist() %>% print()
html_nodes(tdata,"h2#symptoms") %>% html_text()
html_nodes(tdata,"h2#symptoms") %>% html_attrs()
html_nodes(tdata,"h2#symptoms") %>% html_node()
html_nodes(tdata,"page-author-top-container h2") %>% html_node()
html_nodes(tdata,"page-author-top-container h2") %>% html_text()
html_nodes(tdata,"#page-author-top-container h2") %>% html_text()
html_nodes(tdata,"#page-author-top-container") %>% html_text()
html_nodes(tdata,".page-author-top-container") %>% html_text()
html_nodes(tdata,".page-author-top-container h2") %>% html_text()
html_nodes(tdata,".page-author-top-container .h2") %>% html_text()
html_nodes(tdata,".page-author-top-container h2") %>% html_text()
html_nodes(tdata,".page-author-top-container h2#overview") %>% html_text()
html_nodes(tdata,".page-author-top-container h2#overview") %>% html_attr()
html_nodes(tdata,".page-author-top-container h2") %>% html_attr()
html_nodes(tdata,".page-author-top-container, h2") %>% html_attr()
html_nodes(tdata,".page-author-top-container h2") %>% html_attr()
html_nodes(tdata,".page-author-top-container") %>% html_attr()
html_nodes(tdata,".page-author-top-container") %>% html_attrs()
html_nodes(tdata,".page-author-top-container h2") %>% html_attrs()
html_nodes(tdata,".page-author-top-container") %>% html_node()
html_nodes(tdata,".page-author-top-container") %>% html_nodes()
html_nodes(tdata,".page-author-top-container") %>% html_nodes("h2")
html_nodes(tdata,".page-author-top-container") %>% html_nodes("h2#overview")
html_nodes(tdata,".page-author-top-container") %>% html_nodes("h2#overview") %>%  html_text()
html_nodes(tdata,".page-author-top-container") %>% html_nodes("h2#overview") %>%  html_text()
html_nodes(tdata,".page-author-top-container") %>%  html_text()
html_nodes(tdata,".page-author-top-container") %>%  html_text() %>% unlist()
html_nodes(tdata,"section") %>%  html_text() %>% unlist()
html_nodes(tdata,"section h2") %>%  html_text() %>% unlist()
html_nodes(tdata,"section h2") %>%  html_attr()
html_nodes(tdata,"section h2") %>%  html_attrs()
html_nodes(tdata,"section h2") %>%  html_text() %>% unlist()
html_nodes(tdata,"section") %>%  html_text() %>% unlist()
html_nodes(tdata,"section h2, p,ul,picture") %>%  html_text() %>% unlist()
html_nodes(tdata,"section h2, p,ul,picture") %>%  html_text() %>% trimws() %>% unlist()
buffer <- tibble(raw , value,.rows=10000)
rep(1,2)
rep(1,10)
buffer <- tibble(raw = rep(NA,100000),
value= rep(NA,100000))
buffer
buffer
buffer$raw[1] <- tdata
buffer
tdata$node
buffer$raw[1] <- html_nodes(tdata)
buffer$raw[1] <- read_html(tdata)
buffer <- tibble(raw = rep(NA,100000),
value= rep(NA,100000))
buffer
buffer$raw[1] <- htmltools::html_print(tdata)
buffer$raw[1] <- htmltools::HTML(tdata)
buffer <- tibble(raw = rep(NA,100000),
value= rep(NA,100000))
buffer$value <- html_nodes(tdata,"section h2, p,ul,picture") %>%  html_text() %>% trimws() %>% unlist()
buffer<- html_nodes(tdata,"section h2, p,ul,picture") %>%  html_text() %>% trimws() %>% unlist()
buffer
unlist(buffer)
head(unlist(buffer))
class(unlist(buffer))
condtionBuffer <- tibble(raw = rep(NA,1000),
value= rep(NA,1000))
add_row(condtionBuffer, url2,unlist(buffer))
url2
add_row(condtionBuffer, raw = url2,unlist(buffer))
add_row(condtionBuffer, raw = url2,value = unlist(buffer))
condtionBuffer
rbind(condtionBuffer, raw = url2,value = unlist(buffer))
as.data.frame(raw = url2,value = unlist(buffer))
as.data.frame(value = unlist(buffer))
buffer<- html_nodes(tdata,"section h2, p,ul,picture") %>%  html_text() %>% trimws() %>% unlist()
as.data.frame(unlist(buffer))
as.data.frame(value = unlist(buffer))
as.data.frame("value" = unlist(buffer))
as.tibble(value = unlist(buffer))
as.data.frame(url2,unlist(buffer))
# Sources
#
# td<- html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist()
# td<- tibble(data= td)
#
# # Tweeter id:
# # tid  <- str_extract(td[1],"@[[a-zA-Z]]+")
#  td$DateTime<- str_extract(td$data,"@[\\w]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#    # td$Tweets <- str_split(td$data,"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#
as.data.frame(url2,unlist(buffer))
# Sources
#
# td<- html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist()
# td<- tibble(data= td)
#
# # Tweeter id:
# # tid  <- str_extract(td[1],"@[[a-zA-Z]]+")
#  td$DateTime<- str_extract(td$data,"@[\\w]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#    # td$Tweets <- str_split(td$data,"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#
c(url2,buffer)
master <- c()
class(master)
# Sources
#
# td<- html_nodes(tdata,".tweet-text")  %>%  html_text()  %>%  str_trim() %>%    unlist()
# td<- tibble(data= td)
#
# # Tweeter id:
# # tid  <- str_extract(td[1],"@[[a-zA-Z]]+")
#  td$DateTime<- str_extract(td$data,"@[\\w]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#    # td$Tweets <- str_split(td$data,"@[[a-zA-Z]]+ [A-Z]{1}[a-z]{1}[a-z]{1} \\d{2} [0-9]{2}:[0-9]{2}:[0-9]{2} [+][0]{4} [0-9]{4}")
#
c(master,buffer)
