---
title: "Project_606"
author: "Rajwant Mishra"
date: "April 24, 2019"
output:
  html_document:
    code_folding: "hide"
    theme: sandstone
    highlight: tango
    toc: true
    df_print: paged
---

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>

  <!-- {.tabset .tabset-fade .tabset-pills} -->
```{r}
#  Final Project Format 
# 	The final report should be presented in more formal format. Consider your audience to be non data analysts. Fellow data analysts (i.e. students) will be able to access your R Markdown file for details on the analysis. Submit a Zip file with your R Markdown file, the HTML output, and any supplementary files (e.g. data, figures, etc.). You must address the five following sections:
# 
# Introduction: What is your research question? Why do you care? Why should others care?
# 
# Data: Write about the data from your proposal in text form. Address the following points:
# 
# Data collection: Describe how the data were collected.
# Cases: What are the cases? (Remember: case = units of observation or units of experiment)
# Variables: What are the two variables you will be studying? State the type of each variable.
# Type of study: What is the type of study, observational or an experiment? Explain how you've arrived at your conclusion using information on the sampling and/or experimental design.
# Scope of inference - generalizability: Identify the population of interest, and whether the findings from this analysis can be generalized to that population, or, if not, a subsection of that population. Explain why or why not. Also discuss any potential sources of bias that might prevent generalizability.
# Scope of inference - causality: Can these data be used to establish causal links between the variables of interest? Explain why or why not.
# Exploratory data analysis: Perform relevant descriptive statistics, including summary statistics and visualization of the data. Also address what the exploratory data analysis suggests about your research question.
# 
# Inference: If your data fails some conditions and you can't use a theoretical method, then you should use simulation. If you can use both methods, then you should use both methods. It is your responsibility to figure out the appropriate methodology.
# 
# Check conditions
# Theoretical inference (if possible) - hypothesis test and confidence interval
# Simulation based inference - hypothesis test and confidence interval
# Brief description of methodology that reflects your conceptual understanding
# Conclusion: Write a brief summary of your findings without repeating your statements from earlier. Also include a discussion of what you have learned about your research question and the data you collected. You may also want to include ideas for possible future research.
# 
```



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(DT)
library(stringr)
library(lubridate)
library(corrr)
library(psych)
library(readxl)
library(readr)
library(plotly)
library(lme4)
library(lmerTest)


```


---
title: DATA 606 Data Final Proposal
author: Rajwant Mishra
---

### Part 1 - Introduction {.tabset .tabset-fade .tabset-pills}

This is Sales data of from last 2 year, grouped by location and and quarter info.

<div class = "blue">

What is your research question? Why do you care? Why should others care?<br>
 
<li> A. Does customer who baught more at the Quarter end , how did they perfrom during the followin quarter. </li>
<li>B. Does customers buy more during the Quarter End </li>
<li>C. Buid regression Model to predict the Order Quanitity  </li>
 
</div> 

***

<br>
<b> Approach :</b> <br>

+ Step 1 : Normalize Data in the format which can be analysed 
+ Step 2 : Perfrom the Chi-square test 
+ Step 3 : Perfrom ANOVA and Paired T test 
+ Step 4 : Doing regression Analysis on the Order Quantity  

```{r eval=FALSE, include=FALSE}
# load data
workDir <- getwd()

filePath = paste0(workDir,"/Data")
g_max <- 1048576 
ctype = c("text","text","text","text","text","numeric","numeric","text","text","text","text","text","text","text")

 Q1_17 <-read_excel(path=paste0(filePath,"/","2017_Q1.xlsx"),sheet="Data",guess_max =g_max)
 Q2_17 <-read_xlsx(path=paste0(filePath,"/","2017_Q2.xlsx"),sheet="Data",guess_max =g_max)
 Q3_17 <-read_xlsx(path=paste0(filePath,"/","2017_Q3.xlsx"),sheet="DATA",guess_max =g_max)
 Q4_17 <-read_excel(path=paste0(filePath,"/","2017_Q4.xlsx"),sheet="DATA",guess_max =g_max)
 Q1_18 <-read_excel(path=paste0(filePath,"/","2018_Q1.xlsx"),sheet="Data",guess_max =g_max)
 Q2_18 <-read_excel(path=paste0(filePath,"/","2018_Q2.xlsx"),sheet="Data",guess_max =g_max)
 Q3_18 <-read_excel(path=paste0(filePath,"/","2018_Q3.xlsx"),sheet="Data",guess_max =g_max)
 Q4_18 <-read_excel(path=paste0(filePath,"/","2018_Q4.xlsx"),sheet="Data",guess_max =g_max)
 custmap <- read_excel(path=paste0(filePath,"/","custmap.xlsx"),col_types = c("text","text"))

 # Create Subset 
Q1_17 <- Q1_17[,c(2,3,4,5,6,7,8,9,10,14,15,16,17,18,19,20,21,22,25,26,27)]
Q2_17 <- Q2_17[,c(2,3,5,6,7,8,9,10,11,17,18,19,20,21,22,24,25,26,30,31,32)]
Q3_17 <- Q3_17[,c(1,2,4,5,6,7,8,9,10,16,17,18,19,20,22,23,24,25,29,30,31)]
Q4_17 <- Q4_17[,c(2,3,5,6,7,8,9,10,11,17,18,19,20,21,23,24,25,26,29,30,31)]
Q1_18 <- Q1_18[,c(1,2,4,5,6,7,8,9,10,16,17,18,19,20,22,23,24,25,29,30,31)]
Q2_18 <- Q2_18[,c(2,3,5,6,7,8,9,10,11,17,18,19,20,21,23,24,25,26,30,31,32)]
Q3_18 <- Q3_18[,c(2,3,5,6,7,8,9,10,11,18,19,20,21,22,24,25,26,27,31,32,33)]
Q4_18 <- Q4_18[,c(2,3,5,6,7,8,9,10,11,18,19,20,21,22,24,25,26,27,31,32,33)]

# rename Quarter info
   Q1_17$Qt <-   "Q1_17"
   Q2_17$Qt <-   "Q2_17"
   Q3_17$Qt <-   "Q3_17"
   Q4_17$Qt <-   "Q4_17"
   Q1_18$Qt <-   "Q1_18"
   Q2_18$Qt <-   "Q2_18"
   Q3_18$Qt <-   "Q3_18"
   Q4_18$Qt <-   "Q4_18"
   
   #Name COlumn
   names(Q2_17) <- names(Q1_17)
   names(Q3_17) <- names(Q1_17)
   names(Q4_17) <- names(Q1_17)
   names(Q1_18) <- names(Q1_17)
   names(Q2_18) <- names(Q1_17)
   names(Q3_18) <- names(Q1_17)
   names(Q4_18) <- names(Q1_17)
   
   #Combine data 
      
   rm(mkt_Data)
   mkt_Data <- bind_rows(
   Q1_17,
   Q2_17,
   Q3_17,
   Q4_17,
   Q1_18,
   Q2_18,
   Q3_18,
   Q4_18)

  write_csv(mkt_Data,"mkt_data.csv")
```



```{r message=FALSE, include=FALSE}

workDir <- getwd()

filePath = paste0(workDir,"/Data")
g_max <- 1048576 
ctype = c("text","text","text","text","text","numeric","numeric","text","text","text","text","text","text","text")   
 custmap <- read_excel(path=paste0(filePath,"/","custmap.xlsx"),col_types = c("text","text"))

mkt_Data <- read_csv("mkt_Data.csv",guess_max =g_max)
# 
 mkt_Data <- mkt_Data[sample(nrow(mkt_Data),5000),]
 


# upate new Acocunt info 
# custmap$KUNNR_OLD <- as.numeric(custmap$KUNNR_OLD)
# custmap$KUNNR_NEW<- as.numeric(custmap$KUNNR_NEW)
mkt_Data$Customer<- as.character(mkt_Data$Customer)
mkt_Data <- mkt_Data[which(!is.na(mkt_Data$Customer)),]
unique(mkt_Data$Qt)
head(mkt_Data[which(is.na(mkt_Data$Customer)),])
mkt_Data <- left_join(mkt_Data, custmap, by = c("Customer" = "KUNNR_OLD"))
mkt_Data$KUNNR_NEW <- mkt_Data$KUNNR_NEW.x
mkt_Data <- mkt_Data[,-c(23,24)]
# View(mkt_Data[which(is.na(mkt_Data$KUNNR_NEW)),])

mkt_Data$KUNNR_NEW[which(is.na(mkt_Data$KUNNR_NEW))] <- mkt_Data$Customer[which(is.na(mkt_Data$KUNNR_NEW))]
#set all Qty = 0 

# mkt_Data$`Order Quantity`[which(is.na(mkt_Data$`Order Quantity`))]  = 0

#Dropping some Text values from full data 
mkt_Datalean <- mkt_Data[,-c(1,2,3,4,5,6,7,8,7)]

# mkt_s getting data in from of customer and Qt spending

mkt_cust_qt <- mkt_Datalean %>% group_by(KUNNR_NEW,Qt) %>% summarise(order_unit = sum(`Order Quantity`))
mkt_cust_qt <- spread(mkt_cust_qt,Qt,order_unit,fill = 1)


```

### Part 2 - Data {.tabset .tabset-fade .tabset-pills}

  <b>Data :</b> (Write about the data from your proposal in text form. Address the following points:)
  <b>Data collection</b>: (Describe how the data were collected.)
This data is sample of sales by promotion for last two year. Data was shared by Marketting team to evaluate the sales performance. 

Note : For Confidencilaty names and numbers have been changed in the data.

   <b>Cases :</b> What are the cases? (Remember: case = units of observation or units of experiment)
In This sample we have 1000 rows. Each row identify the Order from the given customer.

   <b>Variables :</b> What are the two variables you will be studying? State the type of each variable.
  
  Order Quantity is Response variable here . It's Quantitative variable.
  Quarter `Qt`, is Independent varible , it's qualitative variable as we can't add them.
  Other qualitative variables: Brands,Promotions, Zipcode etc. 

  
   <b>Type of study :</b> What is the type of study, observational or an experiment? Explain how you've arrived at your conclusion using information on the sampling and/or experimental design.
  
>  We do studies to gather information and draw conclusions. The type of conclusion we draw depends on the study method used: In an observational study, we measure or survey members of a sample without trying to affect them. In a controlled experiment, we assign people or things to groups and apply some treatment to one of the groups, while the other group does not receive the treatment.

This is an observational study.

 <b>Scope of inference - generalizability</b>: Identify the population of interest, and whether the findings from this analysis can be generalized to that population, or, if not, a subsection of that population. Explain why or why not. Also discuss any potential sources of bias that might prevent generalizability.
  
Since data set is too big and would take too much computing power and machine time. To save time I'll be working on sample data of much smaller size. on computing I am keeping my sample size to 200 * 8 (each quarter from 2 year). These findings may be generlaized for the same customer and identify the posibility of sales .

   <b>Scope of inference - causality</b>: Can these data be used to establish causal links between the variables of interest? Explain why or why not.
  Since its a observational study we can't make Casue and Effect inference from here, but it being an observational study it will have some info about customers spending pttern over the years and Quarter.
  

  
```{r echo=TRUE, message=FALSE}
#---------------------------------------------------------------------
# Create Sample A of 1000 customer  
#---------------------------------------------------------------------

custA <- mkt_cust_qt[sample(nrow(mkt_cust_qt),1000),]
custAG <- gather(custA, key = "Qt",value = "order_unit",-KUNNR_NEW)
custAG$KUNNR_NEW <- as.character(custAG$KUNNR_NEW)
custAG$Qt = as.factor(custAG$Qt)
custAG$order_unit[which(is.na(custAG$order_unit))] <- 0
custAG$seq <- 0
custAG$seq[which(custAG$Qt=="Q1_17")] = 1
custAG$seq[which(custAG$Qt=="Q2_17")] = 2
custAG$seq[which(custAG$Qt=="Q3_17")] = 3
custAG$seq[which(custAG$Qt=="Q4_17")] = 4
custAG$seq[which(custAG$Qt=="Q1_18")] = 5
custAG$seq[which(custAG$Qt=="Q2_18")] = 6
custAG$seq[which(custAG$Qt=="Q3_18")] = 7
custAG$seq[which(custAG$Qt=="Q4_18")] = 8
custAG <- custAG[order(custAG$seq),]

#---------------------------------------------------------------------
# Create Sample A of 1000 customer  
#---------------------------------------------------------------------
 
 
# Wide Data set
head(custA)

# Long Data set
head(custAG)

```


### Part 3 - Exploratory data analysis {.tabset .tabset-fade .tabset-pills}

  Perform relevant descriptive statistics, including summary statistics and visualization of the data. Also address what the exploratory data analysis suggests about your research question.
  
#### summary statistics
```{r message=FALSE}

  # Data 
 str(head(mkt_Data[,-c(1,2,3,4,5,6,7,8,7)]))
 summary((mkt_Data[,-c(1,2,3,4,5,6,7,8,7)]))
 head(mkt_Data[,-c(1,2,3,4,5,6,7,8,7)])
 describe.by(custA)

```
 
 
#### visualization 
```{r} 

ggplot(custAG,mapping = aes(x=KUNNR_NEW ,y= order_unit, color=Qt)) + geom_point()

ggplot(custAG[which(custAG$order_unit<= 100),],mapping = aes(y=KUNNR_NEW ,x= order_unit, color=Qt)) + geom_point() + facet_wrap(~Qt,ncol = 2) + ggtitle("Ordered Unit < 150 by Customer ") + ylab("Customer")

ggplot(custAG[which(custAG$order_unit> 100),],mapping = aes(y=KUNNR_NEW ,x= order_unit, color=Qt)) + geom_point() + facet_wrap(~Qt,ncol = 2) + ggtitle("Ordered Unit >150 by Customer ") + ylab("Customer") 

# Graph by Plotly 
plot_ly(data=custAG,y=custAG$order_unit ,x= custAG$KUNNR_NEW,color=custAG$Qt, type= "scatter") %>% layout(   title = "scatter plot: Customer Order Qty by Quarter")

# plot_ly(data=custAG,y=custAG$order_unit ,x= custAG$KUNNR_NEW, z = custAG$Qt ,color=custAG$Qt, type= "scatter3d" )



custAG %>%
  group_by(Qt) %>% summarise(order_unit = sum(order_unit)) %>% 
  ggplot(mapping = aes(x= Qt,y=order_unit,fill = Qt)) + geom_col() +  geom_label(aes(label=order_unit))+
theme_light() +  ggtitle("Order Quantity Over Each Quarter") +ylab("Ordered Unit") + xlab("Quarter")

# Line plot with multiple groups
custAG[order(custAG$seq),] %>%
ggplot( aes(x= seq, y=order_unit, group= KUNNR_NEW,color=Qt) )+
  geom_line()+
  geom_point() + ggtitle("Customer Order Qty Movement over Quarter")

```

```{r eval=FALSE, include=FALSE}
# stacked 

# Line plot with multiple groups
# custAG[,] %>% .[order(.$seq),] %>% 
# ggplot( aes(x= seq, y=order_unit, group=KUNNR_NEW, fill=Qt) )+
#   geom_col()


# > Check https://www.youtube.com/watch?v=SvKv375sacA
# > Try getting chi square test using porpatl differen of 1000 custoemr prove is speding in one qt is more or less in other following qt. 

```

### Part 4 - Inference {.tabset .tabset-fade .tabset-pills}

  If your data fails some conditions and you can't use a theoretical method, then you should use simulation. If you can use both methods, then you should use both methods. It is your responsibility to figure out the appropriate methodology.

We will examine sample of data after tidying of the data in the below formt :

|Customer | Q1 | Q2 |
|---------|----|----|
|1|21 | 12|
|2|21 | 12|
|3|21 | 12|
|4|21 | 12|
|.|. | .|
|.|. | .|
|n|n | n|
  
 Note :  # Check conditions
   # Theoretical inference (if possible) - hypothesis test and confidence interval
   # Simulation based inference - hypothesis test and confidence interval
   # Brief description of methodology that reflects your conceptual understanding
  

#### Analysis Variance (ANOVA)  {.tabset .tabset-fade .tabset-pills}

<b> Oneway ANOVA Test & Results </b>
There are several ways to do so but let’s start with the simplest from the base R first 'aov'. While it’s possible to wrap the command in a summary or print statement I   saved the results out to an R object in this case 'AOV_RESULT'.

The dependent variable goes to the left of the tilde and our independent or predictor variable to the right.  aov is not limited to Oneway ANOVA so adding additional factors is possible.
<b>Steps: </b> <br>
1. Set the Hypothesis  
2. Run the AOV test 
3. Peforming eta squared test
3. Interpred the result and Check condition 
4. Check with Paired t test.
##### Prepare 
H0 = Quarter doesn’t matter in predicting Order Quantity  -all Quarters are the same
H1 = At least one of the Quarter populations is different than the others. 
Our null is basically 

If Pvalue is less than that of Alpha .05 we will rejct the null Hypothesis.

Rewording: 
H0: There is *no* difference between qunatity of Q1_17,Q2_17 and so on and so forth.
 pq1 = pq2 = pq3 ...

HA: There is a difference between qunatity of Q1_17,Q2_17 and so on and so forth.

<!-- # https://www.youtube.com/watch?v=-yQb_ZJnFXw -->
<!-- # http://rstudio-pubs-static.s3.amazonaws.com/308410_2ece93ee71a847af9cd12fa750ed8e51.html -->
<br>


```{r}

# # custAG[order(custAG$seq),] %>%spread(key = Qt,value = order_unit, fill=0)
#  custAG[order(custAG$seq),] %>%spread(key = Qt,value = order_unit, fill=0)
# Creating QUarter data over each column
# Q1 Q2 Q3 ...
# 1  2  3  ...
# 1  2  3  ...
# 1  2  3  ...

#Conbined Data
Dt_ANOVA <- custAG[,c(3,2)]
 Dt_ANOVA2017 <- Dt_ANOVA[which(Dt_ANOVA$Qt %in% c("Q1_17", "Q2_17", "Q3_17" ,"Q4_17")),] 
  boxplot(Dt_ANOVA2017$order_unit~Dt_ANOVA2017$Qt, 
        main="Boxplot comparing Qty of Quarter", 
        col= rainbow(4), 
        horizontal = TRUE)
  
library(Hmisc)  
  ggplot(Dt_ANOVA2017, aes(reorder(Qt,order_unit),order_unit,fill=Qt))+
# ggplot(tyre, aes(Brands,Mileage,fill=Brands))+ # if you want to leave them alphabetic
  geom_jitter(colour = "dark gray",width=.1) +
  stat_boxplot(geom ='errorbar',width = 0.4) +
  geom_boxplot()+
  labs(title="Boxplot, dotplot and SEM plot of mileage for four Quarters of Sales", 
       x = "Quarter (sorted)",
       y = "Sales",
       subtitle ="Gray dots=sample data points, Black dot=outlier, Blue dot=mean, Red=99% confidence interval",
caption = "No Major obvious difference in mean is noted") + 
    guides(fill=FALSE) +
  stat_summary(fun.data = "mean_cl_normal", colour = "red", size = 1.5, fun.args = list(conf.int=.99)) +
  stat_summary(geom="point", fun.y=mean, color="blue") +
  theme_bw()

  
  
  
```


#####  Statistical data analysis
```{r}  

All_Qt_row <- custA[,-c(1)]
head(All_Qt_row)
summary(All_Qt_row)
# Creating stacked quarter data.
# Q1
# Q2
# Q3 
All_Qt_stack <- custAG[,c(3,2)]
head(All_Qt_stack,n=10)
print(t(describe.by(All_Qt_stack)))

```
##### AOV test

```{r}
head(All_Qt_stack)
AOV_RESULT <- aov(order_unit~Qt,All_Qt_stack) # Qt is predictor 

class(AOV_RESULT)

 # The names command will give you some sense of all the information contained in the list object.
names(AOV_RESULT)

# The summary command gives us the key ANOVA data we need and produces a classic ANOVA table
summary(AOV_RESULT)
# One-way ANOVA showed a no significant effect of Quarter on Order Quantity gain (F(7, 7992) = 1.504 , , p ă .001).


print( AOV_RESULT )

```

>
# F-test is always one sided
# And that we only reject the null hypothesis for very large F-values
# That means we’re only interested in the upper tail of the F-distribution


```{r}

pf( 1.504, 7, 7992,lower.tail=FALSE)


# R doesn’t use the names “between-group”
# and “within-group”. Instead, it tries to assign more meaningful names: in our case # "between groups" variance corresponds to the effect that the "Qt" has on the outcome variable; "within groups" variance is corresponds to the “leftover” variability, so it calls that the residuals
# There’s a few different ways you could measure the effect size in an ANOVA, but the most commonly
# used measures are η2 (eta squared) and partial η2. For a one way analysis of variance they’re identical
# to each other, so for the moment I’ll just explain η2. The definition of η2 is actually really simple

SSb <- 3473
SSt <-   (3473+2636684)  # total sums of squares

# eta squared
# The interpretation of η2 (eta squared)  is equally straightforward: it refers to the proportion of the variability in the
# outcome variable (Order_Quantity) that can be explained in terms of the predictor (Quarter). 
# A value of η2 ((eta squared) = 0 
# means that there is no relationship at all between the two, 
# whereas a value of η2 (eta squared) = 1 means that the relationship is perfect.
# eta_Sqrd <- sqrt(SSb / SSt) 
eta_Sqrd <- sqrt(SSb / SSt)  # eta-squared value
eta_Sqrd 
### 
```

>Here we can conclude that there is no strong relation between the order Qty over Quarter.

##### Check Condition 

 We have assumed 3 things; independence, homogeneity of variance (homoscedasticity) and normality for considering  ANOVA result for our probelm. Lets see if our result meets these conditions.  <br>
 <b>Independence :</b> Eacu customer is indendent and buying pattern is too. <br>
 <b>our errors or residuals are normally distributed :</b> Our Residual is Right Skewed as its just the promotion sales data. <br>
<b>The final is homogeneity of variance also known as (homoscedasticity). :</b> Due to some big outlier the sample is not showing the plot , but we can asumet this based on small sample size . <br>

```{r}

# Plot each one by one
par(mfrow=c(2,2))
plot(AOV_RESULT)

#PLotting Residual only 
hist(AOV_RESULT$residuals)

# homoscedasticity
plot(AOV_RESULT$residuals)

print(t(psych::describe(AOV_RESULT$residuals) ))# skew kurtosis are very high , which shows that data is not fully normalised , given the sample size we will 

#using sample of size 500 to see data.



par(mfrow=c(2,2))
for (i in 1:10)
{
  AOV_RESULT_Sam <- aov(order_unit~Qt,(Dt_ANOVA[sample(nrow(Dt_ANOVA),500),]))
 plot(AOV_RESULT_Sam$residuals)
}




```


##### Running “pairwise” t-tests

 It’s tempting to conclude that Q1 is better than the other Quarters. Afetr running multiple T test , it was clear that the pvalue 0.0178 < 0.05 , so we can reject null hypothesis. Suport atlernate hypothesis that there is evidence of a difference in sleas of Quarter 1 and Quarter 2. 

Please Note : I have done pairewise t test using function but not sure how to iterpret the result . Including it for future study.

```{r}
# How we can say which quarter is more or less in terms of data 

library(gplots)
plotmeans( formula = order_unit~Qt, # plot Order Qty by Quarter
 data = All_Qt_stack, # the data frame
 xlab = "Quarter", # x-axis label
 ylab = "Order Qty", # y-axis label
 n.label = FALSE # don’t display sample size
 )


plotmeans( formula = order_unit~seq, # plot Order Qty by Quarter
 data = custAG[,c(3,4,2)], # the data frame
 xlab = "Quarter", # x-axis label
 ylab = "Order Qty", # y-axis label
 n.label = FALSE # don’t display sample size,
 
 )

# it’s tempting to conclude that Q1 is better than the other Quarters excpet Q2 and better than Anxifree,
# but there’s no real difference between each quarter. However, if we want to get a clearer
# answer about this, it might help to run some tests.


#Running “pairwise” t-tests
# set p.adjust.method = "none" since we’re not doing any adjustments. 
pairwise.t.test(x = Dt_ANOVA$order_unit,
                g = Dt_ANOVA$Qt,
                p.adjust.method = "none")


library(DATA606)
Dt_ANOVA2017_q12 <- Dt_ANOVA2017[which(Dt_ANOVA2017$Qt %in% c("Q1_17","Q2_17")),] 
Dt_ANOVA2017_q12$Qt <- factor(Dt_ANOVA2017_q12$Qt)
inference(y = as.numeric(Dt_ANOVA2017_q12$order_unit), 
          x = Dt_ANOVA2017_q12$Qt, est = "mean", type = "ht", null = 0, 
          alternative = "greater", method = "theoretical")


Dt_ANOVA2017_q23 <- Dt_ANOVA2017[which(Dt_ANOVA2017$Qt %in% c("Q2_17","Q3_17")),] 
Dt_ANOVA2017_q23$Qt <- factor(Dt_ANOVA2017_q23$Qt)
inference(y = as.numeric(Dt_ANOVA2017_q23$order_unit), 
          x = Dt_ANOVA2017_q23$Qt, est = "mean", type = "ht", null = 0, 
          alternative = "greater", method = "theoretical")


Dt_ANOVA2017_q34 <- Dt_ANOVA2017[which(Dt_ANOVA2017$Qt %in% c("Q3_17","Q4_17")),] 
Dt_ANOVA2017_q34$Qt <- factor(Dt_ANOVA2017_q34$Qt)
inference(y = as.numeric(Dt_ANOVA2017_q34$order_unit), 
          x = Dt_ANOVA2017_q34$Qt, est = "mean", type = "ht", null = 0, 
          alternative = "greater", method = "theoretical")

# (t.test( formula = Dt_ANOVA2017_q12$order_unit ~ Dt_ANOVA2017_q12$Qt,
#  data = Dt_ANOVA2017,
#  subset = Qt %in% c("Q1_17","Q2_17"),
#  var.equal = TRUE
#  ))

# Suppose that my post hoc analysis consists of m separate tests, and I want to ensure
# that the total probability of making any Type I errors at all is at most α.9 If so, then the Bonferroni
# correction just says “multiply all your raw p-values by m”. If we let p denote the original p-value, and
# let p1
# j be the corrected value, then the Bonferroni correction tells that:
# p1 “ m ˆ p
# And therefore, if you’re using the Bonferroni correction, you would reject the null hypothesis if p1 ă α.
# The logic behind this correction is very straightforward. We’re doing m different tests; so if we arrange
# it so that each test has a Type I error rate of at most α{m, then the total Type I error rate across these
# tests cannot be larger than α.
# here we have 8 Quaters data so comb(8,2) = 28 , with this method all the results were multiplied by 28 

pairwise.t.test(x = Dt_ANOVA$order_unit,
                g = Dt_ANOVA$Qt,
                p.adjust.method = "bonferroni")

```


#### Chi-square Test {.tabset .tabset-fade .tabset-pills}

**Test method**. Use the chi-square goodness of fit test to determine whether observed sample frequencies differ significantly from expected frequencies specified in the null hypothesis. 

This approach consists of four steps: 
(1) state the hypotheses :

The hypotheses for the Chi-squared test are as follows.

H0: There is *no* association between qunatity of Q1_17,Q2_17 and so on and so forth.
 pq1 = pq2 = pq3 ...

HA: There is an association between qunatity of Q1_17,Q2_17 and so on and so forth.

**Assumptions***
The chi-squared test, when used with the standard approximation that a chi-squared distribution is applicable, has the following assumptions:

**Simple random sample**
The sample data is a random sampling from a fixed distribution or population where every collection of members of the population of the given sample size has an equal probability of selection. Variants of the test have been developed for complex samples, such as where the data is weighted. Other forms can be used such as purposive sampling.

> n= 50  and 500, selcted randomly from sample . satisfying Simple Random sample 

**Sample size (whole table)**
A sample with a sufficiently large size is assumed. If a chi squared test is conducted on a sample with a smaller size, then the chi squared test will yield an inaccurate inference. The researcher, by using chi squared test on small samples, might end up committing a Type II error.

> sample size is big enough and with multiple itration I have tried to get possible result. 

**Expected cell count**
Adequate expected cell counts. Some require 5 or more, and others require 10 or more. A common rule is 5 or more in all cells of a 2-by-2 table, and 5 or more in 80% of cells in larger tables, but no cells with zero expected count. When this assumption is not met, Yates's correction is applied.

> Cell count is more that 2 X 2 matrix dat value. 

**Independence**
The observations are always assumed to be independent of each other. This means chi-squared cannot be used to test correlated data.

> 
 + Each sample is choosen randonmly from the Population. We can say that they are meeting the condition on Indepenence
 + Then sample of 50 and 500 was created to run the chi-test.
 

(2) formulate an analysis plan, 

We will create Sample of 1000 datapoints where records are spread in column for each quarter.
we would be doing regular chi square test and then we would switch to Multiple simulation

Significance level. significance levels 0.05, or 0.10; but any value between 0 and 1 can be used.



(3) analyze sample data


```{r message=FALSE}

custA$KUNNR_NEW <- as.character(custA$KUNNR_NEW )
summary(custA)
head(custA)
# List test condtion for CHi-Square
# 1.  Are the value independent for each Quarter : 
# 2. 

# Ho : Customer are not baised on speding over Quarter . pq1 = pq2 = pq3 ...
# Ha : Customer are spending less or more compared to last quarter.




# 1. convert the data as a table with only Quater data .

dt <- as.table(as.matrix(custA[,]))


# Test 1 
chi_custA <-  custA[1:50,c(2,4,6,8,3,5,7,9)]  %>% 
            chisq.test(test_custA)
chi_custA
head(chi_custA$observed)
head(chi_custA$expected)
plot(chi_custA$residuals)

chi_custA <-  custA[1:50,c(2,4,6,8,3)]  %>% 
            chisq.test()
chi_custA
head(chi_custA$observed)
head(chi_custA$expected)
plot(chi_custA$residuals)

(gather((as.data.frame(chi_custA$residuals)),"Qt","Val")) %>% ggplot(mapping = aes(x=Qt,y= Val))+
  geom_col()



# Test 2 
chi_custA <- custA[sample(1:nrow(custA),500),c(2,4,6,8,3,5,7,9)]   %>% 
             chisq.test(simulate.p.value = TRUE)
chi_custA
head(chi_custA$observed)
head(chi_custA$expected)
round(chi_custA$residuals, 3)
plot(chi_custA$residuals)
chi_custA$p.value

 # Test 3 
custA[sample(1:nrow(custA),10),c(2,4)]   %>% 
             chisq.test()

custA[sample(1:nrow(custA),10),c(4,6)]   %>% 
             chisq.test()
 

```

> Since p-value = `chi_custA$p.value` , which is less than .05 so we can reject the null hypothesis. So we see there is dependancy in the spending over the quarter. 

#### Using Simulation 

```{r message=FALSE}

#Multiple simulation
#Doing multiple check 
rm(p_chi)
p_chi <- data.frame(Qt12= rep(0, 50),Qt23= rep(0, 50),Qt34= rep(0, 50))
for(i in 1:50){
  
   chi_test <- custA[sample(1:nrow(custA),10),c(2,4)]   %>% 
             chisq.test()
  p_chi$Qt12[i]  <- chi_test$p.value
  
  chi_test <- custA[sample(1:nrow(custA),10),c(4,6)]   %>% 
             chisq.test()
  p_chi$Qt23[i]  <- chi_test$p.value
  
  
  
  chi_test <- custA[sample(1:nrow(custA),10),c(6,8)]   %>% 
             chisq.test()
  
  p_chi$Qt34[i]  <- chi_test$p.value
  # samp <- sample(c("atheist", "non_atheist"), n, replace = TRUE, prob = c(p, 1-p))
  # p_hats[i] <- sum(samp == "atheist")/n
}

# "Chi-squared approximation may be incorrectChi-squared "   The warning message found in the solution above is due to the small cell values in the contingency table. 
#lets check the key
table(gather(p_chi)$key)

#Gather the data so that we can plot on it.
#using geom_jitter to add some noise as most of poits would overlap in normal plot.
gather(p_chi) %>% ggplot(mapping = aes(x= key, y= round(value,3))) + geom_point()+geom_jitter()

gather(p_chi)%>% filter(value > 0.05)
#contigency table 

 rowSums(custA[,-c(1)])
 colSums(custA[,-c(1)])

```


(4) **interpret results**.

<div class = "blue">

Answer to Business Question A: Does customer who baught more at the Quarter end , how did they perfrom during the followin quarter.

</div>

> Since p-value = `chi_custA$p.value` , which is less than .05 so we can reject the null hypothesis. So we see there is dependancy in the spending over the quarter. 
> Multiple simulation also indicatess pvlaue < 0.05 , for Aplha = 0.5 it indicates that we are 95% confident that spending over each quater is depended on spending over last quarter. 

> Limitation : We can't evalaute spending is more or less using Chi-square test , as it is goodness of fittest only.


### Regression {.tabset .tabset-fade .tabset-pills}
#### Build Data and Summary

Lets evaluate Promotion and qty realtion Qt.

```{r message=FALSE}

# Creating subset of data 
lm_cust <- mkt_Datalean[,c(15,4,1,3,5,7,11,12,13,14)]
lm_cust<- rename(lm_cust , promo = `External Description` )
lm_cust$KUNNR_NEW = as.character(lm_cust$KUNNR_NEW)
lm_cust$Brand = as.factor(lm_cust$Brand)
lm_cust$zip = as.factor(lm_cust$zip)
lm_cust$city = as.factor(lm_cust$city)
lm_cust$state = as.factor(lm_cust$state)
lm_cust$Qt = as.factor(lm_cust$Qt)


# Check data 
head(lm_cust)
summary(lm_cust)

# lm_cust[which(is.na(lm_cust$state)),]

lm_cust <- lm_cust %>% group_by(KUNNR_NEW,Qt,Brand,`Order Date`,promo,city,state) %>% summarise(Order_Qty = sum(`Order Quantity`),Doll_Val = sum(`Promotion Order Doll`))
# 
# lm_cust[which(lm_cust$Brand=="OO"),] %>% ggplot(mapping = aes(x= promo, y= Order_Qty,group= Qt , color = Qt)) + geom_line()+
#   geom_point() + geom_jitter()+ facet_grid(year(`Order Date`)~ .)
# 
#  ggplot(lm_cust,mapping = aes(x= Qt, y= Order_Qty, group= Brand ,color= Brand )) + geom_line()+
#   geom_point() + geom_jitter()
 
 # Brand by Year 
 lm_cust[which(lm_cust$Brand %in% c("RB","RJ")),] %>% group_by(promo,Brand,Qt,year =year(`Order Date`)) %>%summarise(Order_Qty = sum(Order_Qty)) %>% 
   ggplot(mapping = aes(x=Brand, y = Order_Qty,fill = Qt)) + 
   geom_col()+facet_grid(year~ .)+
   theme(axis.text.x = element_text(angle = 70, hjust = 1)) + 
   scale_y_continuous( labels = scales::number)+
   ggtitle("Brand by Year ") +ylab("Ordered Unit")
 
 
 # promo by year and Brand
 
 plotly::ggplotly( lm_cust[which(lm_cust$Brand %in% c("RB")),] %>% group_by(promo,Brand,Qt,year =year(`Order Date`)) %>%summarise(Order_Qty = sum(Order_Qty)) %>% 
   ggplot(mapping = aes(x=promo, y = Order_Qty,fill = Qt)) +
   geom_col( position = "dodge")+facet_grid(Brand+year~ .,scales="free")+
   theme(axis.text.x = element_text(angle = 70, hjust = 1)) + 
   scale_y_continuous( labels = scales::number)+
     ggtitle("Promo by year and Brand ") +ylab("Ordered Unit"))
  
  #Plotly check
  plotData  <-  lm_cust[which(lm_cust$Brand %in% c("RB")),] %>% group_by(promo,Brand,Qt,month =month(`Order Date`)) %>%summarise(Order_Qty = sum(Order_Qty)) 
  
  
  plot_ly(x = plotData$promo, y = plotData$Order_Qty, mode= "marker", type = "bar", data= plotData, color= plotData$Qt) %>% layout(title="RB Brand Order ")
  
  lm_cust[which(lm_cust$Brand %in% c("RB")),] %>% group_by(promo,Brand,Qt,month =month(`Order Date`)) %>%summarise(Order_Qty = sum(Order_Qty)) 

  
  linPlot <- lm_cust[which(lm_cust$Brand %in% c("RB","OO")),] %>% group_by(Brand,date = month(`Order Date`) )%>%summarise(OrderQty = sum(Order_Qty))
  
  linPlot <- lm_cust[which(lm_cust$Brand %in% c("RB","OO")),] %>% group_by(Brand,date = (format(`Order Date`, "%Y-%m")) )%>%summarise(OrderQty = sum(Order_Qty))
  
  linPlot <- lm_cust[which(lm_cust$Brand %in% c("RB","OO")),] %>% group_by(Brand,date = (month(`Order Date`)) , year =  year(`Order Date`))%>%summarise(OrderQty = sum(Order_Qty))
  
  
    plot_ly(x= linPlot$date,y= linPlot$OrderQty, color = as.factor(linPlot$Brand) , data = linPlot[which(linPlot$year==2017),], linetype = I("Brand"))
  
    linPlot$year <- as.factor(linPlot$year)
    plot_ly(x= linPlot$date,y= linPlot$OrderQty, color = as.factor(linPlot$year) , data = linPlot[which(linPlot$brand=="RB"),], linetype = "solid")
    
     # linPlot <- lm_cust[which(lm_cust$Brand %in% c("RB","OO","TY")),] %>% group_by(Brand,dateL = `Order Date`)%>%summarise(OrderQty = sum(Order_Qty))
     #  plot_ly(data= linPlot,x= month(linPlot$dateL), y = linPlot$OrderQty,color = linPlot$Brand , linetype = 'dot' )
     
     # str_replace(str_extract(linPlot$dateL[1],"\\d+_\\d{2}"),"_","")
     linPlot <- lm_cust[which(lm_cust$Brand %in% c("RB","OO","TY")),] %>% group_by(Brand,
                          dateL = Qt
                        )%>%summarise(OrderQty = sum(Order_Qty))
     linPlot$Qt <-  as.numeric(str_replace_all(str_extract_all(linPlot$dateL,"\\d+_\\d{2}"),"_",""))
     
     plot_ly(data= linPlot,x= linPlot$Qt, y = linPlot$OrderQty,color = linPlot$Brand , linetype = 'dot' ) %>% layout(title = " Sales by Brand by Quarter")
    
   
  
# check if all promotions are running every year
head(table(year(lm_cust$`Order Date`),lm_cust$promo))

```


#### Model Creation 
Try Predicting Quantity based on Known informaiton . 
Identifying Best model to predict Quantity 

```{r message=FALSE}

# pairs.panels(lm_cust[,c(2,3)])

# Try Predicting Quantity based on Known informaiton . 
# Identifying Best model to predict Quantity 

head(lm_cust)
result <- data.frame(var = 1:10,pval = 1:10, comment=1:10)

result$var <- "A"
result$pval <- "A"
result$comment <- "A" 

# Working With Sample 
set.seed(42672)
 lm_cust_s1 <- lm_cust[sample(nrow(lm_cust),200),]
 lm_cust_s1$month <- month(lm_cust_s1$`Order Date`)
 lm_cust_s1$month <- as.factor(lm_cust_s1$month )
 
 names(lm_cust_s1)
 


lm1 <- lm(Order_Qty ~ Qt + Brand + promo + month + state,lm_cust_s1)
summary(lm1)
anova(lm1)
result$var[1] <- "lm(formula = Order_Qty ~ Qt + Brand + promo + month + state, 
    data = lm_cust_s1)"
result$pval[1] <-  "Adjusted R-squared:  0.2356 F-statistic: 1.568 on 108 and 91 DF,  p-value: 0.01388"

result$comment[1] <- "(Month , State ) Qt,Promo codes are significant,state is not " 


#Dropping State

lm2 <- lm(Order_Qty ~ Qt + Brand + promo + month,lm_cust_s1)
summary(lm2)
anova(lm2)

result$var[2] <- "lm(formula = Order_Qty ~ Qt + Brand + promo + month, data = lm_cust_s1)"
result$pval[2] <- "Adjusted R-squared:  0.3198 F-statistic: 2.396 on 67 and 132 DF,  p-value: 9.788e-06"
result$comment[2] <- "(Month)Brand is not significant" 

#Promo promoVPFP200 turns out to more significant here. 



#3  Dropping brand info 
lm3 <- lm(Order_Qty ~  Qt + promo + month,lm_cust_s1)
summary(lm3)
anova(lm3)

result$var[3] <- "lm(formula = Order_Qty ~ Qt + promo + month, data = lm_cust_s1)"
result$pval[3] <- "Adjusted R-squared:  0.2799 F-statistic:  3.09 on 37 and 162 DF,  p-value: 4.911e-07"
result$comment[3] <- "(Month) least significant" 

#4  Dropping Month
lm4 <- lm(Order_Qty ~  Qt + promo ,lm_cust_s1)
summary(lm4)
anova(lm4)

result$var[4] <- "lm(formula = Order_Qty ~ Qt + promo, data = lm_cust_s1)"
result$pval[4] <- "Adjusted R-squared:  -0.003147 F-statistic: 0.9799 on 31 and 168 DF,  p-value: 0.5038"
result$comment[4] <- "not much significant"

# 5 Increase Sample Size
rm(lm_cust_s1)
# SInce I see no major significance increasing the sample size to 500
set.seed(12121)
lm_cust_s2 <- lm_cust[sample(nrow(lm_cust),500),]
 lm_cust_s2$month <- month(lm_cust_s2$`Order Date`)
 lm_cust_s2$month <- as.factor(lm_cust_s2$month )
 names(lm_cust_s2)
 
 
 lm5 <- lm(Order_Qty ~ Qt + Brand + promo + month + state,lm_cust_s2)
summary(lm5)
anova(lm5)

result$var[5] <- "500: lm(formula = Order_Qty ~ Qt + Brand + promo + month + state,     data = lm_cust_s2)"
result$pval[5] <-   " Adjusted R-squared:  0.05218 F-statistic: 1.214 on 128 and 370 DF,  p-value: 0.08366"
result$comment[5] <- "(Sig: Promo) " 
 
#6 Dropping Month, I want to keep state for now.

lm6 <- lm(Order_Qty ~ Qt + Brand + promo + state,lm_cust_s2)
summary(lm6)
anova(lm6)

result$var[6] <- "lm(formula = Order_Qty ~ Qt + Brand + promo + state, data = lm_cust_s2)"
result$pval[6] <- "Adjusted R-squared:  0.04593 F-statistic:   1.2 on 120 and 378 DF,  p-value: 0.1016"

result$comment[6] <- "promo is very less significant now, nothing else is significant" 

#7 Drop Brand

set.seed(12123)

lm7 <- lm(Order_Qty ~  Qt + promo + state,lm_cust_s2)
summary(lm7)
anova(lm7)


result$var[7] <- "lm(formula = Order_Qty ~ Qt + promo + state, data = lm_cust_s2)"
result$pval[7] <- "Adjusted R-squared:  0.04285 F-statistic: 1.251 on 89 and 409 DF,  p-value: 0.07818"

result$comment[7] <- "not much. sig " 



#8 Drop state and Brand

set.seed(12126)

lm8 <- lm(Order_Qty ~    Qt + promo + Brand,lm_cust_s2)
summary(lm8)
anova(lm8)

result$var[8] <- "lm(formula = Order_Qty ~ Qt + promo + Brand, data = lm_cust_s2)"
result$pval[8] <- "Adjusted R-squared:  0.04555 F-statistic:  1.34 on 70 and 429 DF,  p-value: 0.04416"
result$comment[8] <- "not much sig." 


lm9 <- lm(formula = Order_Qty ~ Qt + Brand + promo + month, data = lm_cust_s2)	
summary(lm9)
anova(lm9)
result$var[9] <- "lm(formula = Order_Qty ~ Qt + Brand + promo + month, data = lm_cust_s2)"
result$pval[9] <- "Adjusted R-squared:  0.05115 F-statistic: 1.345 on 78 and 421 DF,  p-value: 0.03633"
result$comment[9] <- "Pvalue looks less than  to .05"

lm10 <- lm(formula = Order_Qty ~ Qt + promo + month, data = lm_cust_s2)
summary(lm10)
anova(lm10)
result$var[10] <- "lm(formula = Order_Qty ~ Qt + promo + month, data = lm_cust_s2)"
result$pval[10] <- "Adjusted R-squared:  0.0448 
F-statistic: 1.498 on 47 and 452 DF,  p-value: 0.02147"
result$comment[10] <- "not much sig."


# p-value: 0.02331
lm_pbm <- lm(formula = Order_Qty ~  promo + Brand + month, data = lm_cust_s2)

summary(lm(formula = Order_Qty ~  promo + Brand + month, data = lm_cust_s2))
anova(lm(formula = Order_Qty ~  promo + Brand + month, data = lm_cust_s2))

# p-value: 0.0177
lm_pd <- lm(formula = Order_Qty ~  promo  + `Order Date`, data = lm_cust_s2)
anova(lm(formula = Order_Qty ~  promo  + `Order Date`, data = lm_cust_s2))
summary(lm(formula = Order_Qty ~  promo  + `Order Date`, data = lm_cust_s2))

#p-value: 0.0169
lm_py <- lm(formula = Order_Qty ~  promo  + year(`Order Date`), data = lm_cust_s2)
summary(lm(formula = Order_Qty ~  promo  + year(`Order Date`), data = lm_cust_s2))
anova(lm(formula = Order_Qty ~  promo  + year(`Order Date`), data = lm_cust_s2))

#p-value: 0.4186  Not considering 
summary(lm(formula = Order_Qty ~  month + Brand + Qt , data = lm_cust_s2))

#p-value: 0.0569
lm_pbms <- lm(formula = Order_Qty ~  Brand + promo + month + state,   data = lm_cust_s2)

summary(lm(formula = Order_Qty ~  Brand + promo + month + state,   data = lm_cust_s2))


# p-value: 0.08366 
lmdata<-na.omit(lm_cust_s2) 
lm_qbpms <-(lm(formula = Order_Qty ~  Qt + Brand + promo +  `Order Date`,   data = lm_cust_s2))
summary(lm_qbpms)

```



#### Assess the linear model

From above analyis I would use differnt model to valdiate the result. Lets take model lm_pbm which is model for promotion + brand + month would predict order Quanity.

 To assess whether the linear model is reliable, we need to check for 
 (1) linearity, 
 (2) nearly normal residuals, and 
 (3) constant variability. 
 (4) Residuals are independent
  Residual = Observed value - Predicted value 

```{r message=FALSE}


# From above analyis I would use differnt model to valdiate the result. Lets take model lm_pbm which is model for promotion + brand + month would predict order Quanity .

# To assess whether the linear model is reliable, we need to check for 
#(1) linearity, 
#(2) nearly normal residuals, and 
#(3) constant variability. 
#(4) Residuals are independent
# Residual = Observed value - Predicted value 

library(DATA606)
plot_ss(x = lm_cust_s2$Order_Qty, y = lm_pbm$residuals,showSquares = TRUE)

# # (1) Linear association: The residuals plot shows a random scatter.
#Based on the plot we can clearly say that there is apparent pattern in the distribution as the numbers appear to be group and outlier are close to the regression line, so it can be treated as strong corelation and can be considered as a linear relationship.

# # (2) Nearly normal residuals: To check this condition, we can look at a histogram
hist(lm_pbm$residuals)

# or a normal probability plot of the residuals.
#It seems the plot is slightly skewed left, 
qqnorm(lm_pbm$residuals)
qqline(lm_pbm$residuals)  # adds diagonal line to the normal prob plot
# (3) we can say that its also  Nearly normal residuals even though its right skewed with few outliers . 
# (4)Residuals can be treated as independent as sample is drawn from independent .


# plot on sample
ggplot(data = lm_cust_s2,mapping = aes(y=lm_cust_s2$Order_Qty ,x= lm_cust_s2$promo))+ geom_point() + geom_smooth(method = "lm",se=FALSE) + geom_abline(slope = lm_pbm$coefficients[8], intercept = lm_pbm$coefficients[1], color="red")


# regression Line population data of 2 year.
(ggplot(data = mkt_Data,mapping = aes(y=mkt_Data$`Order Quantity` ,x= mkt_Data$`External Description`))+ geom_point() + geom_smooth(method = "lm",se=FALSE) + geom_abline(slope = lm_pbm$coefficients[8], intercept = lm_pbm$coefficients[1], color="red") )


# Lets plot on by Order Date 
# To assess whether the linear model is reliable, we need to check for 
#(1) linearity, 
#(2) nearly normal residuals, and 
#(3) constant variability. 
#(4) Residuals are independent
# 
plot_ss(x = lm_cust_s2$Order_Qty, y = lm_pd$residuals,showSquares = TRUE) #  Linear association
hist(lm_pd$residuals) # Right skewed 
qqnorm(lm_pd$residuals)
qqline(lm_pd$residuals) # very much on the regression line, Nearly normal residuals even though its right skewed


# On Sample

ggplot(data = lm_cust_s2,mapping = aes(y=lm_cust_s2$Order_Qty ,x= lm_cust_s2$`Order Date`))+ geom_point() + geom_smooth(method = "lm",se=FALSE) +
 geom_abline(slope = lm_pd$coefficients[8], intercept = lm_pd$coefficients[1], color="green")+
  geom_abline(slope = lm_pbm$coefficients[8], intercept = lm_pbm$coefficients[1], color="red")

# On Population 
ggplot(data = mkt_Data,mapping = aes(y = mkt_Data$`Order Quantity`,x= mkt_Data$Brand))+ geom_point() + geom_smooth(method = "lm",se=FALSE) +  geom_abline(slope = lm_pd$coefficients[8], intercept = lm_pd$coefficients[1], color="red") +
  geom_abline(slope = lm_pbm$coefficients[8], intercept = lm_pbm$coefficients[1], color="green")



plot(lm_pbm)
plot(lm_pd)


```


<div class = "row">
  
<div class = "col-md-12">
 Residuals: We can see that the multiple regression model1 has a smaller range for the residuals as compared with Model 2: (Model 1) -59 to 339 vs.(model 2) -62.02 to 356.44.  Secondly the median of the multiple regression model 1 is much closer to 0 than the model 2 regression model.

 Coefficients:
 (Intercept): The intercept is the left over when you average the independent and dependent variable.  In the simple regression Model 1 we see that the intercept is 20.02172 which is close tp ZERO, and Model 2 has much larger intercept ie. 266.9  (format( 2.669e+02, scientific = FALSE))  meaning there's a fair amount left over.  Model 1 looks close fit with nearrest to ZERO intercept. 

 promo: Both multiple regression model shows that when we add promo variable it's multiplying this variable times  the numeric (ordinal) value of the Promotion code.So for every promocode in the year, you add an additional estimated column unit value in sales.  For example : promoRSD will add 130 Unit each MOnth.promoWild Card will add 53 Unit 

 Brand : So far every brand addition would add resepctive value in the sales unit by multiplying the brand intercept with its ordinal vlaue. FOr example  addition of brand AX would add 3 unit each month. 

 Month: When we add in the Month variable it's multiplying this variable times the numeric (ordinal) value of the month.  For example July and August 
</div>
  
<div class = "col-md-6">
```{r, message=FALSE, echo=FALSE}
#Lets take two model and undersatnd its summary
##--------------------------------------------Model 1 
summary(lm_pbm)

lm_pbm$xlevels
# Residuals:
#    Min     1Q Median     3Q    Max 
# -59.94  -8.49  -2.41   4.08 339.09 
# # Equation of Regression line :
# y = b+ mx 
# y = B0 + B1 X Promo  + B2 X Month + B3 X Brand
#  e <-  20.02172 
cc <- lm_pbm$coefficients
(eqn_pbm <- paste("Regression Formula Y =", paste(round(cc[1],2), paste(round(cc[-1],2), names(cc[-1]), sep=" * ", collapse=" + "), sep=" + "), "+ e"))

```
</div>
  


<div class = "col-md-6">
```{r, message=FALSE, echo=FALSE}
##--------------------------------------------Model 2
summary(lm_pd)
anova(lm_pd)
lm_pd$xlevels
#Residuals:
#    Min     1Q Median     3Q    Max 
# -62.02  -9.56  -4.12   2.91 356.44 

```
</div>

<div class = "col-md-12">

Analyis of Variance table 

```{r}
anova(lm_pbm,lm_pd)


```
 Function perfroms an analysis of variance of the two models using an F-test to assess the significanxe of the differences.
 
 We can see Model has decreased the Sum of the Squared error, and the value of 0.1977 says that we can be 80% confidence in saying that they models are different. 



</div>
</div>

#### Evaluation of the Model

Using test and sample data we will see how good our Model is.
We wil luse Absolute Mean error of the model and decide which regression models works well for the sample.

```{r}

# lm_cust_s2 
# lm_cust_s2 <- lm_cust[sample(nrow(lm_cust),500),]
#  lm_cust_s2$month <- month(lm_cust_s2$`Order Date`)
#  lm_cust_s2$month <- as.factor(lm_cust_s2$month )
#  names(lm_cust_s2)
#  

# Uisng same sample to test lm_cust_s2 , creating sample of 500 more to test.

lm_cust_t1  <- lm_cust_s2
lm_cust_t2 <- lm_cust[sample(nrow(lm_cust),500),]
 lm_cust_t2$month <- month(lm_cust_t2$`Order Date`)
 lm_cust_t2$month <- as.factor(lm_cust_t2$month )
 names(lm_cust_t2)
 
 lm_pbm1 <-  update(lm_pbm,lm_cust_s2)
 
 
 # Predict main train data
lm_pred_pbm <- predict(lm_pbm,lm_cust_t1)
lm_pred_pd <- predict(lm_pd,lm_cust_t1)
lm_pred_qbpms <- predict(lm_qbpms,lm_cust_t1)


mean(abs(lm_cust_t1$Order_Qty- lm_cust_t1$Order_Qty))
mean(abs(lm_pred_pbm - lm_cust_t1$Order_Qty))
mean(abs(lm_pred_pd - lm_cust_t1$Order_Qty))
mean(abs(lm_pred_qbpms - lm_cust_t1$Order_Qty))

# Using test data of new set
# lm_pbm <- lm(formula = Order_Qty ~ promo + Brand + month, data = lm_cust_t2)

update
lm_pred_pbm <- predict(lm(formula = Order_Qty ~ promo + Brand + month, data = lm_cust_t2),lm_cust_t2)
lm_pred_pd <- predict(lm(formula = Order_Qty ~ promo + `Order Date` , data = lm_cust_t2),lm_cust_t2)
lm_pred_qbpms <- predict(lm(formula = Order_Qty ~ Qt + Brand+promo +month+ state , data = lm_cust_t2),lm_cust_t2)


mean(abs(lm_cust_t2$Order_Qty- lm_cust_t2$Order_Qty))
mean(abs(lm_pred_pbm - lm_cust_t2$Order_Qty))
mean(abs(lm_pred_pd - lm_cust_t2$Order_Qty))
mean(abs(lm_pred_qbpms - lm_cust_t2$Order_Qty))




```

From above mean errors we can see that model 1 with regession model with formula `lm(formula = Order_Qty ~ promo + Brand + month, data = lm_cust_t2)` is well close to zero in terms of mean absulate error.

</div>


```{r eval=FALSE, include=FALSE}



#### Using Repeated Measures ANOVA in R || lme4
#Started for testing...<Not applicable for analyis now>
# install.packages("lmerTest")

head(lm_cust_s2)
# lm(formula = Order_Qty ~ Qt + promo + month, data = lm_cust_s2)
model1 <- lmer(Order_Qty ~ Qt + (1|KUNNR_NEW),data = lm_cust_s2)
anova(model1)

model2 <- lmer(Order_Qty ~ promo + (1|KUNNR_NEW),data = lm_cust_s2)
anova(model2)
summary(model2)
## 0.02895 *

model3 <- lmer(Order_Qty ~  city + (1|KUNNR_NEW),data = lm_cust_s2)
anova(model3)
summary(model3)



```


```{r eval=FALSE, include=FALSE}
# install.packages("plotly")
# install.packages("Rcpp")
library(plotly)
library(Rcpp)
# install_github('ramnathv/rCharts', force= TRUE)
library(rCharts)

library(rCharts)

nPlot(Order_Qty ~   promo , type = 'multiBarChart',data=lm_cust_s2, type = "point") 

nPlot(Order_Qty ~  Qt , group= 'promo', data=lm_cust_s2, type = "Bar") 



mk <- mkt_Datalean[which(mkt_Data$Brand=="CH"),] %>% group_by(`Order Date`,Qt)%>% summarise(ordQty = sum(`Order Quantity`)) %>% spread(Qt,ordQty)
  
#   
# mPlot(x = day(mk$`Order Date`),y=c("Q1_17","Q2_17"), type = "Line", data= mk)
# library(leaflet)
# mp3 <- Leaflet$new()
# mp3$setView(c(51,505, -0.09), zoom = 12)
# mp3$marker(c(51,505, -0.09), bindPopup = "TEST HELLP")
# mp3



```



```{r eval=FALSE, include=FALSE}

#------------------------------------------SAMPLE ----------------------


#Create sample 

# mkt_s <- sample_n(mkt_Data, size = 5000, replace = FALSE)
 mkt_s <-    bind_rows(
   Q1_17[sample(nrow(Q1_17),200),],
   Q2_17[sample(nrow(Q2_17),200),],
   Q3_17[sample(nrow(Q3_17),200),],
   Q4_17[sample(nrow(Q4_17),200),],
   Q1_18[sample(nrow(Q1_18),200),],
   Q2_18[sample(nrow(Q2_18),200),],
   Q3_18[sample(nrow(Q3_18),200),],
   Q4_18[sample(nrow(Q4_18),200),])

 
#mkt_s <- mkt_Data %>% sample_frac(0.05)

head(mkt_s[which(is.na(mkt_s$Customer)),])
unique(mkt_s$Qt)
mkt_s <- mkt_s[which(!is.na(mkt_s$Customer)),]
unique(mkt_s$Qt)
# mkt_s2 <- dplyr:: left_join(mkt_s, custmap, by = c("Customer" = "KUNNR_OLD"))

unique(mkt_s$Qt)
head(mkt_s[which(is.na(mkt_s$KUNNR_NEW)),])
install.packages("xlsx")
library("xlsx")
write.xlsx(mkt_s,"ap.xlsx",sheetName = "Sheet1", 
  col.names = TRUE, row.names = TRUE, append = FALSE)
write.xlsx(mkt_s2,"ap2.xlsx",sheetName = "Sheet1", 
  col.names = TRUE, row.names = TRUE, append = FALSE)


ggplot(mkt_s, mapping = aes(x= as.character(mkt_s$`Promotion Order Doll`), y= mkt_s$`Order Quantity`, color= mkt_s$`External Description`))+
  geom_point()+theme(axis.text.x = element_text(angle = 70, hjust = 1)) + facet_wrap(~mkt_s$Qt)

 # (nms <- names(read_excel(path=paste0(filePath,"/","2017_Q3.xlsx"), sheet="DATA",n_max = 0)))

# rm(Q1_17)
# rm(Q2_17)
# rm(Q3_17)
# rm(Q4_17)
# rm(Q1_18)
# rm(Q2_18)
# rm(Q3_18)
# rm(Q4_18)


#Dropping some Text values from sample 
mkt_s <- mkt_s[,-c(1,2,4,5,6,7,8,7)]

# mkt_s getting data in from of customer and Qt speding

cust_qt <- mkt_s %>% group_by(Customer,Qt) %>% summarise(order_unit = sum(`Order Quantity`))

spread(cust_qt,Qt,order_unit)
```



### Part 5 - Conclusion
Write a brief summary of your findings without repeating your statements from earlier. Also include a discussion of what you have learned about your research question and the data you collected. You may also want to include ideas for possible future research.
# 

### References
https://stats.stackexchange.com/questions/405243/compare-sales-data-over-time-in-sequence

https://www.machinelearningplus.com/machine-learning/complete-introduction-linear-regression-r/
 
https://university.business-science.io/courses/541056/lectures/9826285
https://www.youtube.com/watch?v=SvKv375sacA

 
### Appendix (optional)

Remove this section if you don't have an appendix









